{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import pickle as pkl\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "teacher_forcing_ratio = 0\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_ind(arr):\n",
    "    arr = arr.cpu().numpy()\n",
    "    batch_size = arr.shape[1]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if 1 in arr[:,i]:\n",
    "            ind = np.where(arr[:,i]== 1)[0][0]\n",
    "        \n",
    "            arr[:,i][:ind+1]=1\n",
    "            arr[:,i][ind+1:]=0\n",
    "        else:\n",
    "            arr[:,i]=1\n",
    "        \n",
    "    \n",
    "    return arr, np.count_nonzero(arr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_IDX = 2\n",
    "UNK_IDX = 3\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\", 3:\"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "#     s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"&apos;m\", r\"am\", s)\n",
    "    s = re.sub(r\"&apos;s\", r\"is\", s)\n",
    "    s = re.sub(r\"&apos;re\", r\"are\", s)\n",
    "    s = re.sub(r\"&apos;\", r\"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingLangs(sourcelang, targetlang, setname):\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    print('Reading lines...')\n",
    "    # Read the file \n",
    "    with open('../../iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,sourcelang)) as f:\n",
    "        for line in f.readlines():\n",
    "            input_ls.append([normalizeString(word) for word in line.split()])\n",
    "    with open('../../iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,targetlang)) as f:\n",
    "        for line in f.readlines():\n",
    "            output_ls.append([normalizeString(word) for word in line.split()])\n",
    "    pairs = list(zip(input_ls, output_ls))\n",
    "    print('Read %s sentence pairs'%(len(input_ls)))\n",
    "    input_lang = Lang(sourcelang)\n",
    "    output_lang = Lang(targetlang)\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213377 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88918\n",
      "en 69063\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6133\n",
      "en 4015\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5215\n",
      "en 3518\n"
     ]
    }
   ],
   "source": [
    "source_tra, target_tra, pairs_tra = loadingLangs('zh', 'en', 'train')\n",
    "source_val, target_val, pairs_val = loadingLangs('zh', 'en', 'dev')\n",
    "source_tes, target_tes, pairs_tes = loadingLangs('zh', 'en', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% of chinese sentences length = 44.0\n",
      "95% of english sentences length = 48.0\n",
      "(['我们', '的', '战争', '法律', '和', '责任', '也', '有', '了', '新', '的', '变化', '我们', '该', '怎么', '处理', '像', '无人', '无人机', '人机', '屠杀', '这种', '事情'], ['We', 'have', 'new', 'wrinkles', 'in', 'the', 'laws', 'of', 'war', 'and', 'accountability', ' .', 'What', 'do', 'we', 'do', 'with', 'things', 'like', 'unmanned', 'slaughter', ' ?'])\n"
     ]
    }
   ],
   "source": [
    "print(\"95% of chinese sentences length = {0}\".format(np.percentile([len(x[0]) for x in pairs_tra], 95)))\n",
    "print(\"95% of english sentences length = {0}\".format(np.percentile([len(x[1]) for x in pairs_tra], 95)))\n",
    "print(random.choice(pairs_tra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 38\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair,source,target):\n",
    "    input_lang = source\n",
    "    output_lang = target\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0]).reshape((-1))\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1]).reshape((-1))\n",
    "    return (input_tensor, input_tensor.shape[0], target_tensor, target_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, source, target, pairs):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        inp_ten, inp_len, tar_ten, tar_len = tensorsFromPair(self.pairs[key], self.source, self.target)\n",
    "        item = {}\n",
    "        item['inputtensor'] = inp_ten[:MAX_SENT_LEN]\n",
    "        item['inputlen'] = min(inp_len, MAX_SENT_LEN)\n",
    "        item['targettensor'] = tar_ten[:MAX_SENT_LEN]\n",
    "        item['targetlen'] = min(tar_len, MAX_SENT_LEN)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NMTDataset(source_tra, target_tra, pairs_tra)\n",
    "val_data = NMTDataset(source_tra, target_tra, pairs_val)\n",
    "test_data = NMTDataset(source_tra, target_tra, pairs_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputtensor': tensor([  16, 1222, 1223,  110, 1224, 1038, 1088, 1093,  112, 1225, 1226, 1227,\n",
       "          405, 1034, 1035, 1036, 1037, 1228, 1229,  213,   16,  885,  213,   16,\n",
       "            6, 1206,  213,   16,    6, 1230,   35, 1195,    1],\n",
       "        device='cuda:0'),\n",
       " 'inputlen': 33,\n",
       " 'targettensor': tensor([ 61, 645, 161, 831, 833,   5,   6, 286,  52,  89,  30, 834,  21, 913,\n",
       "         792,  92,   6, 914,  16,  92, 915,  16,  92, 273, 916,  16,  92, 273,\n",
       "         917,  30,  92, 273, 901,  44,   1], device='cuda:0'),\n",
       " 'targetlen': 35}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(231)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate function\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_data, tar_data, src_len, tar_len = [], [], [], []\n",
    "    for datum in batch:        \n",
    "        src_datum = np.pad(np.array(datum['inputtensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['inputlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        tar_datum = np.pad(np.array(datum['targettensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['targetlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        src_data.append(src_datum)\n",
    "        tar_data.append(tar_datum)\n",
    "        src_len.append(datum['inputlen'])\n",
    "        tar_len.append(datum['targetlen'])\n",
    "    return [torch.from_numpy(np.array(src_data)).to(device),torch.from_numpy(np.array(tar_data)).to(device),\n",
    "               torch.from_numpy(np.array(src_len)).to(device),torch.from_numpy(np.array(tar_len)).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence batch: \n",
      "tensor([[ 1330,   277,     1,  ...,     2,     2,     2],\n",
      "        [  409,  3943,   185,  ...,     2,     2,     2],\n",
      "        [   49,   308,   416,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [  110,  2959,  4465,  ...,     2,     2,     2],\n",
      "        [  493,   168, 15436,  ...,     2,     2,     2],\n",
      "        [ 2712, 20887,  1920,  ...,  2712, 20887,  1920]], device='cuda:0')\n",
      "input batch dimension: torch.Size([64, 38])\n",
      "target sentence batch: \n",
      "tensor([[1197,  113,  220,  ...,    2,    2,    2],\n",
      "        [ 539,    6,  283,  ...,    2,    2,    2],\n",
      "        [1005,   16,   48,  ...,  529,   44,    1],\n",
      "        ...,\n",
      "        [2973,   22, 2360,  ...,    2,    2,    2],\n",
      "        [  51,  311, 1671,  ...,    2,    2,    2],\n",
      "        [ 503, 4978,  359,  ...,   16,  467,  495]], device='cuda:0')\n",
      "target batch dimension: torch.Size([64, 38])\n",
      "input sentence len: \n",
      "tensor([ 3,  8, 29, 10, 18,  9, 38,  6, 26,  8, 11,  6,  6, 25, 20, 10, 12, 16,\n",
      "        38, 20, 14,  7, 10,  6, 16, 30, 20, 17, 11,  8, 21,  6, 38,  7,  8, 17,\n",
      "        26, 11,  9,  8,  9, 21, 17, 14, 12,  4, 25, 38, 17, 38, 16,  7, 12, 27,\n",
      "        21,  7, 14, 13, 20, 26, 21, 35,  4, 38], device='cuda:0')\n",
      "target sentence len: \n",
      "tensor([ 4, 10, 38, 11, 15, 15, 38,  8, 24,  9, 20,  8,  9, 30, 26, 10, 12, 17,\n",
      "        38, 26, 19, 15, 11,  8, 14, 35, 25, 18, 14, 12, 23,  8, 38,  9,  7, 20,\n",
      "        31,  9, 12,  9, 14, 26, 19,  8, 14,  4, 27, 38, 17, 38, 15,  8, 15, 35,\n",
      "        23,  7, 12, 17, 29, 32, 23, 28,  9, 38], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# sample data loader\n",
    "count = 0\n",
    "for data in train_loader:\n",
    "    count+=1\n",
    "    print('input sentence batch: ')\n",
    "    print(data[0])\n",
    "    print('input batch dimension: {}'.format(data[0].size()))\n",
    "    print('target sentence batch: ')\n",
    "    print(data[1])\n",
    "    print('target batch dimension: {}'.format(data[1].size()))\n",
    "    print('input sentence len: ')\n",
    "    print(data[2])\n",
    "    print('target sentence len: ')\n",
    "    print(data[3])\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers = 1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True) \n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device) \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size()[1]\n",
    "        seq_len = input.size()[0]\n",
    "        embedded = self.embedding(input).view(seq_len, batch_size, -1) \n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.fc1(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0, max_length=MAX_SENT_LEN):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.view(1,-1)\n",
    "        batch_size = input.size()[1]\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, batch_size, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  \n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied.transpose(0,1)[0]), 1)\n",
    "        \n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer):\n",
    "    batch_size = input_tensor.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_optimizer.zero_grad()  \n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size()[0] \n",
    "    target_length = target_tensor.size()[0]\n",
    "    encoder_outputs = torch.zeros(target_length, batch_size, encoder.hidden_size, device=device) \n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        torch.cat((encoder_hidden[0].cpu().data,encoder_hidden[1].cpu().data),dim = 1)).to(device).unsqueeze(0)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)  # decoder_input: torch.Size([1, 32])\n",
    "    decoder_hidden = encoder_hidden.to(device)\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        loss = 0 \n",
    "        criterion = nn.NLLLoss(reduce = True, ignore_index = 2, reduction = 'mean') \n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_input = target_tensor[di]                          \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "           \n",
    "            loss += temp_loss\n",
    "            \n",
    "        ave_loss = loss/target_length\n",
    "            \n",
    "    else:\n",
    "        loss = None \n",
    "        criterion = nn.NLLLoss(reduce = False) \n",
    "        prediction = None\n",
    "\n",
    "        for di in range(target_length):            \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            if prediction is None:\n",
    "                prediction = topi.view(1,-1)\n",
    "            else:\n",
    "                prediction = torch.cat((prediction, topi.view(1,-1)), dim=0)            \n",
    "                            \n",
    "            decoder_input = topi.transpose(0,1).detach()  # detach from history as input\n",
    "                        \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            if loss is None:\n",
    "                loss = temp_loss.view(1,-1)\n",
    "            else:\n",
    "                loss = torch.cat((loss, temp_loss.view(1,-1)),dim=0)\n",
    "                \n",
    "        mask, count = mask_ind(prediction)\n",
    "        total_loss = torch.sum(loss * torch.from_numpy(mask).float().to(device))\n",
    "        ave_loss = total_loss/count\n",
    "\n",
    "    ave_loss.backward()    \n",
    "    encoder_optimizer.step()   \n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    \n",
    "    return ave_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh1087/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 41s (- -1m 18s), Epoch: [1/10], Step: [200/3335], Train Loss: 5.619498220682144\n",
      "Time: 1m 22s (- -2m 37s), Epoch: [1/10], Step: [400/3335], Train Loss: 3.784364628791809\n",
      "Time: 2m 4s (- -3m 55s), Epoch: [1/10], Step: [600/3335], Train Loss: 3.5432434237003325\n",
      "Time: 2m 46s (- -3m 13s), Epoch: [1/10], Step: [800/3335], Train Loss: 3.5158208918571474\n",
      "Time: 3m 28s (- -4m 31s), Epoch: [1/10], Step: [1000/3335], Train Loss: 3.4835895121097566\n",
      "Time: 4m 9s (- -5m 50s), Epoch: [1/10], Step: [1200/3335], Train Loss: 3.42646985411644\n",
      "Time: 4m 51s (- -5m 9s), Epoch: [1/10], Step: [1400/3335], Train Loss: 3.431347281932831\n",
      "Time: 5m 32s (- -6m 27s), Epoch: [1/10], Step: [1600/3335], Train Loss: 3.4177776539325713\n",
      "Time: 6m 14s (- -7m 46s), Epoch: [1/10], Step: [1800/3335], Train Loss: 3.408577970266342\n",
      "Time: 6m 55s (- -7m 4s), Epoch: [1/10], Step: [2000/3335], Train Loss: 3.441721132993698\n",
      "Time: 7m 37s (- -8m 22s), Epoch: [1/10], Step: [2200/3335], Train Loss: 3.4358979654312134\n",
      "Time: 8m 19s (- -9m 40s), Epoch: [1/10], Step: [2400/3335], Train Loss: 3.5013775300979613\n",
      "Time: 9m 1s (- -10m 58s), Epoch: [1/10], Step: [2600/3335], Train Loss: 3.4979773950576782\n",
      "Time: 9m 43s (- -10m 16s), Epoch: [1/10], Step: [2800/3335], Train Loss: 3.551347794532776\n",
      "Time: 10m 25s (- -11m 35s), Epoch: [1/10], Step: [3000/3335], Train Loss: 3.529348540306091\n",
      "Time: 11m 7s (- -12m 53s), Epoch: [1/10], Step: [3200/3335], Train Loss: 3.512684773206711\n",
      "[5.619498220682144, 3.784364628791809, 3.5432434237003325, 3.5158208918571474, 3.4835895121097566, 3.42646985411644, 3.431347281932831, 3.4177776539325713, 3.408577970266342, 3.441721132993698, 3.4358979654312134, 3.5013775300979613, 3.4979773950576782, 3.551347794532776, 3.529348540306091, 3.512684773206711]\n",
      "Time: 12m 16s (- -13m 46s), Epoch: [2/10], Step: [200/3335], Train Loss: 3.5670440292358396\n",
      "Time: 12m 58s (- -13m 3s), Epoch: [2/10], Step: [400/3335], Train Loss: 3.5294271111488342\n",
      "Time: 13m 39s (- -14m 21s), Epoch: [2/10], Step: [600/3335], Train Loss: 3.4899533939361573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9bc4b9e676a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         loss = train(input_tensor, target_tensor, encoder1,\n\u001b[0;32m---> 25\u001b[0;31m                      attn_decoder1, encoder_optimizer, decoder_optimizer)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-647e6b1032f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6dafa96e61b6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         attn_weights = F.softmax(\n",
      "\u001b[0;32m~/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;31m# Activation functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "learning_rate=0.0001\n",
    "num_epoch = 10\n",
    "print_every=200\n",
    "plot_every=200\n",
    "\n",
    "encoder1 = EncoderRNN(source_tra.n_words,hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, target_tra.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(attn_decoder1.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "#         print(i)\n",
    "        input_tensor = input_sentences.transpose(0,1)   \n",
    "        target_tensor = target_sentences.transpose(0,1)\n",
    "        mask = target_tensor.ge(1)   \n",
    "        loss = train(input_tensor, target_tensor, encoder1,\n",
    "                     attn_decoder1, encoder_optimizer, decoder_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i > 0 and i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}'.format(\n",
    "                timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                len(train_loader),print_loss_avg))\n",
    "\n",
    "        if i > 0 and i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    print(plot_losses)\n",
    "\n",
    "        #if i % print_every == 0:\n",
    "            #corpus, truths = evaluate_rnn(encoder1, attn_decoder1, val_loader, max_length=MAX_SENT_LEN)\n",
    "            #score_ls = bleu(corpus, truths)\n",
    "            #avg_score = np.array(score_ls).mean()\n",
    "            #print_loss_avg = print_loss_total / print_every\n",
    "            #print_loss_total = 0\n",
    "            #print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Average BLEU: {}'.format(\n",
    "                    #timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                    #len(train_loader),print_loss_avg, avg_score))\n",
    "\n",
    "        #if i > 0 and i % plot_every == 0:\n",
    "            #plot_loss_avg = plot_loss_total / plot_every\n",
    "            #plot_losses.append(plot_loss_avg)\n",
    "            #plot_loss_total = 0\n",
    "\n",
    "                \n",
    "    #print(plot_losses)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(encoder, decoder, data_loader, max_length=MAX_SENT_LEN):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    corpus = []\n",
    "    truths = []\n",
    "\n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(data_loader): \n",
    "#         print('v',i)\n",
    "        input_tensor = input_sentences.transpose(0,1).to(device)  \n",
    "        target_tensor = target_sentences.to(device)\n",
    "        truths.append(target_tensor)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        batch_size = input_tensor.size()[1]\n",
    "#         print(batch_size)\n",
    "\n",
    "    # encode the source lanugage\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = Variable(torch.zeros(max_length,batch_size, encoder.hidden_size)).to(device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] =  encoder_output[0]\n",
    "        encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        torch.cat((encoder_hidden[0],encoder_hidden[1]),dim = 1)).unsqueeze(0)\n",
    "        \n",
    "    # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]*batch_size])).to(device) # SOS\n",
    "#         print(decoder_input.size()) #[1,32]\n",
    "        # output of this function\n",
    "        decoded_words = torch.zeros(batch_size, max_length)\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        # unfold\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # hint: print out decoder_output and decoder_attention\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi\n",
    "        \n",
    "            decoded_words[:,di] = ni.squeeze()\n",
    "\n",
    "#             # stop unfolding whenever '<EOS>' token is returned\n",
    "#             if ni == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 decoded_words.append(target_tra.index2word[ni])\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor(ni.transpose(0,1))).to(device)\n",
    "#             print(decoded_words.size())\n",
    "        corpus.append(decoded_words)\n",
    "#             attns.append(decoder_attentions[:di + 1])\n",
    "    #truths = [t.transpose(0,1) for t in truths]\n",
    "    return corpus, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_2_sent(pred_tensor, truth_tensor,lang_obj):\n",
    "    pred_word_list = []\n",
    "    truth_word_list = []\n",
    "    for i in pred_tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            pred_word_list.append(lang_obj.index2word[i.item()])\n",
    "    for j in truth_tensor:\n",
    "        if j.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            truth_word_list.append(lang_obj.index2word[j.item()])\n",
    "    pred_sent = (' ').join(pred_word_list)\n",
    "    truth_sent = (' ').join(truth_word_list)\n",
    "    return pred_sent, truth_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(corpus, truths):\n",
    "    '''\n",
    "    corpus: list, NBs * BATCHSIZE * MAX_LEN\n",
    "    truths: list, NBs * BATCHSIZE * MAX_LEN\n",
    "    \n",
    "    return: array of length NBs, avg blue score for each batch\n",
    "    '''\n",
    "    n = len(corpus)\n",
    "    bleus = [0]*n\n",
    "    for i in range(n):\n",
    "        pred, true = corpus[i], truths[i]\n",
    "        sumbleu = 0.0\n",
    "        for j in range(len(corpus[i])):\n",
    "            pred_tensor, true_tensor = pred[j], true[j]\n",
    "            pred_sent, true_sent = convert_idx_2_sent(pred_tensor, true_tensor, target_tra)\n",
    "            sumbleu += corpus_bleu(true_sent, pred_sent).score\n",
    "        avgbleu = sumbleu / len(corpus[i])\n",
    "#         print(avgbleu)\n",
    "        bleus[i] = avgbleu\n",
    "    return bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
