{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import pickle as pkl\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "teacher_forcing_ratio = 0\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import raw_corpus_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_ind(arr):\n",
    "    arr = arr.cpu().numpy()\n",
    "    batch_size = arr.shape[1]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if 1 in arr[:,i]:\n",
    "            ind = np.where(arr[:,i]== 1)[0][0]\n",
    "        \n",
    "            arr[:,i][:ind+1]=1\n",
    "            arr[:,i][ind+1:]=0\n",
    "        else:\n",
    "            arr[:,i]=1\n",
    "        \n",
    "    \n",
    "    return arr, np.count_nonzero(arr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_2_sent_new(idx_tensor, lang_obj):\n",
    "    word_list = []\n",
    "    #truth_word_list = []\n",
    "    for i in idx_tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "#     for j in truth_tensor:\n",
    "#         if j.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "#             truth_word_list.append(lang_obj.index2word[j.item()])\n",
    "    sent = (' ').join(word_list)\n",
    "    #truth_sent = (' ').join(truth_word_list)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_new(corpus,truths):\n",
    "    n = len(corpus)\n",
    "    #bleu = [0]*n\n",
    "    pred_ls = []\n",
    "    true_ls = []\n",
    "    for i in range(n):\n",
    "        pred, true = corpus[i], truths[i]\n",
    "        pred_ls.append( [convert_idx_2_sent_new(sent, target_tra) for sent in pred])\n",
    "        true_ls.append([convert_idx_2_sent_new(sent, target_tra) for sent in true])\n",
    "    flattened_pred  = [val for sublist in pred_ls for val in sublist]\n",
    "    flattened_true  = [val for sublist in true_ls for val in sublist]\n",
    "    bleu= corpus_bleu(flattened_pred, [flattened_true]).score\n",
    "    return bleu\n",
    "\n",
    "    #n = len(corpus)\n",
    "    #bleu = [0]*n\n",
    "    #for i in range(n):\n",
    "        #pred, true = corpus[i], truths[i]\n",
    "        #pred_ls = [convert_idx_2_sent_new(sent, target_tra) for sent in pred]\n",
    "        #true_ls = [convert_idx_2_sent_new(sent, target_tra) for sent in true]\n",
    "        #bleu[i] = corpus_bleu(pred_ls, [true_ls]).score\n",
    "    #return np.mean(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_IDX = 2\n",
    "UNK_IDX = 3\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\", 3:\"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "#     s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"&apos;m\", r\"am\", s)\n",
    "    s = re.sub(r\"&apos;s\", r\"is\", s)\n",
    "    s = re.sub(r\"&apos;re\", r\"are\", s)\n",
    "    s = re.sub(r\"&apos;\", r\"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingLangs(sourcelang, targetlang, setname):\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    print('Reading lines...')\n",
    "    # Read the file \n",
    "    with open('../iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,sourcelang)) as f:\n",
    "        for line in f.readlines():\n",
    "            input_ls.append([normalizeString(word) for word in line.split()])\n",
    "    with open('../iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,targetlang)) as f:\n",
    "        for line in f.readlines():\n",
    "            output_ls.append([normalizeString(word) for word in line.split()])\n",
    "    pairs = list(zip(input_ls, output_ls))\n",
    "    print('Read %s sentence pairs'%(len(input_ls)))\n",
    "    input_lang = Lang(sourcelang)\n",
    "    output_lang = Lang(targetlang)\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213377 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88918\n",
      "en 69063\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6133\n",
      "en 4015\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5215\n",
      "en 3518\n"
     ]
    }
   ],
   "source": [
    "source_tra, target_tra, pairs_tra = loadingLangs('zh', 'en', 'train')\n",
    "source_val, target_val, pairs_val = loadingLangs('zh', 'en', 'dev')\n",
    "source_tes, target_tes, pairs_tes = loadingLangs('zh', 'en', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 38\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair,source,target):\n",
    "    input_lang = source\n",
    "    output_lang = target\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0]).reshape((-1))\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1]).reshape((-1))\n",
    "    return (input_tensor, input_tensor.shape[0], target_tensor, target_tensor.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, source, target, pairs):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        inp_ten, inp_len, tar_ten, tar_len = tensorsFromPair(self.pairs[key], self.source, self.target)\n",
    "        item = {}\n",
    "        item['inputtensor'] = inp_ten[:MAX_SENT_LEN]\n",
    "        item['inputlen'] = min(inp_len, MAX_SENT_LEN)\n",
    "        item['targettensor'] = tar_ten[:MAX_SENT_LEN]\n",
    "        item['targetlen'] = min(tar_len, MAX_SENT_LEN)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NMTDataset(source_tra, target_tra, pairs_tra)\n",
    "val_data = NMTDataset(source_tra, target_tra, pairs_val)\n",
    "test_data = NMTDataset(source_tra, target_tra, pairs_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputtensor': tensor([  49,  871,   16, 1235,  454, 1112,    6,   84,   85,  322,  398,  310,\n",
       "            6, 1236, 1237,  735,   57, 1238,  391,  621,  611,  612,  613,   84,\n",
       "           85,   16, 1239,   18,  885,    6, 1240,    1], device='cuda:0'),\n",
       " 'inputlen': 32,\n",
       " 'targettensor': tensor([ 48,  89,  52,  53, 577, 206,  77,  30, 113,  25,  54,  21, 210, 831,\n",
       "          21,  22,  23,  56,  77, 206,  52,  53, 921, 915,  44,   1],\n",
       "        device='cuda:0'),\n",
       " 'targetlen': 26}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate function\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_data, tar_data, src_len, tar_len = [], [], [], []\n",
    "    for datum in batch:        \n",
    "        src_datum = np.pad(np.array(datum['inputtensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['inputlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        tar_datum = np.pad(np.array(datum['targettensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['targetlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        src_data.append(src_datum)\n",
    "        tar_data.append(tar_datum)\n",
    "        src_len.append(datum['inputlen'])\n",
    "        tar_len.append(datum['targetlen'])\n",
    "    return [torch.from_numpy(np.array(src_data)).to(device),torch.from_numpy(np.array(tar_data)).to(device),\n",
    "               torch.from_numpy(np.array(src_len)).to(device),torch.from_numpy(np.array(tar_len)).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_func)\n",
    "\n",
    "#Here change to shuffle=True\n",
    "val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence tensor batch: \n",
      "tensor([   16,   240,   404,  1212, 30553,  1095,     1,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2],\n",
      "       device='cuda:0')\n",
      "input sentence token batch: \n",
      "input batch dimension: torch.Size([64, 38])\n",
      "target sentence batch: \n",
      "tensor([ 61, 306, 408, 751,  44,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2], device='cuda:0')\n",
      "We can do everything  .\n",
      "target batch dimension: torch.Size([64, 38])\n",
      "input sentence len: \n",
      "tensor([31, 38, 28,  7,  4, 38, 24, 31,  9, 38,  8,  9,  3,  6, 23, 14,  7,  6,\n",
      "        38, 20, 13, 15, 19, 17, 14, 22, 20, 13, 28, 32,  8,  9, 23, 17,  6, 11,\n",
      "        10, 23, 36, 34,  8, 10,  7, 21, 22, 38, 14, 13, 17, 14, 23, 38,  8, 38,\n",
      "        38, 10, 14,  7, 10, 12, 10, 11, 20, 15], device='cuda:0')\n",
      "target sentence len: \n",
      "tensor([33, 38, 31,  6,  4, 38, 24, 38, 12, 38, 11,  9,  6,  7, 18, 12,  8,  8,\n",
      "        38, 28, 15, 20, 24, 24, 12, 22, 24, 17, 35, 29, 13, 17, 30, 20,  9, 17,\n",
      "         7, 26, 36, 38,  6, 12,  9, 22, 25, 38, 16, 18, 29, 17, 16, 38, 11, 33,\n",
      "        37, 11, 18,  9, 13, 12, 12, 17, 24, 14], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# sample data loader\n",
    "count = 0\n",
    "for data in train_loader:\n",
    "    count+=1\n",
    "    print('input sentence tensor batch: ')\n",
    "    print(data[0][3])\n",
    "    print('input sentence token batch: ')\n",
    "    \n",
    "    print('input batch dimension: {}'.format(data[0].size()))\n",
    "    print('target sentence batch: ')\n",
    "    print(data[1][3])\n",
    "    tokens = convert_idx_2_sent_new(data[1][3], target_tra)\n",
    "    print(tokens)\n",
    "    \n",
    "    print('target batch dimension: {}'.format(data[1].size()))\n",
    "    print('input sentence len: ')\n",
    "    print(data[2])\n",
    "    print('target sentence len: ')\n",
    "    print(data[3])\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx =PAD_IDX)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size()[1]\n",
    "        #print('in Encoder, batch_size is {} \\n'.format(batch_size))\n",
    "        seq_len = input.size()[0]\n",
    "        #print('in Encoder, seq_len is {} \\n'.format(seq_len))\n",
    "        embedded = self.embedding(input).view(seq_len, batch_size, -1)\n",
    "        #print('in Encoder, embedded is {}, dimension is {} \\n'.format(embedded, embedded.size()))\n",
    "        output = embedded\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('in Encoder, output after gru is {}, dimension is {} \\n'.format(output, output.size()))\n",
    "        #print('in Encoder, hidden after gru is {}, dimension is {} \\n'.format(hidden, hidden.size()))\n",
    "        \n",
    "        context = self.fc1(torch.cat((hidden[0],hidden[1]),dim = 1 )).unsqueeze(0)\n",
    "        #print('in encoder, context is {}, dimension is {}'.format(context, context.size()))\n",
    "        #output = self.fc1(output)\n",
    "        return output, context\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        initHidden = torch.zeros(2, batch_size, self.hidden_size, device=device)\n",
    "        #print('in Encoder, initialized hidden dimension is {}'.format(initHidden.size()))\n",
    "        return initHidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx = PAD_IDX)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1,-1)\n",
    "        batch_size = input.size()[1]\n",
    "        #print('in Decoder, batch_size is {}'.format(batch_size))\n",
    "        \n",
    "        #print('in Decoder, input before embedded layer is {}, dimension is {}'.format(input,input.size()))\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "        #print('in Decoder, output after embedded is {}, dimension is {} \\n'.format(output, output.size()))\n",
    "        output = F.relu(output)\n",
    "        #print('in Decoder, output after relu is {}, dimension is {} \\n'.format(output, output.size()))\n",
    "        #print('in Decoder, the initial hidden is {}, dimension is {}'.format(hidden, hidden.size()))\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('in Decoder, output of GRU is {}, dimension is {}'.format(output, output.size()))\n",
    "        #print('in Decoder, hidden of GRU is {}, dimension is {}'.format(hidden, hidden.size()))\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        #print('in Decoder, output after softmax is {}, dimension is {}'.format(output, output.size()))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        initHidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        #print('in Decoder, initHidden is {}, dimension is {} \\n'.format(initHidden, initHidden.size()))\n",
    "        return initHidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer):\n",
    "    \n",
    "    batch_size = input_tensor.size()[1]\n",
    "    #print('in train, batch size is {}'.format(batch_size))\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    #print('in train, initial encoder hidden is {}, dimension is {}'.format(encoder_hidden, encoder_hidden.size()))\n",
    "    \n",
    "    encoder_optimizer.zero_grad()  \n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size()[0]\n",
    "    #print('in train, input_length is {}'.format(input_length))\n",
    "    target_length = target_tensor.size()[0]\n",
    "    #print('in train, target_length is {}'.format(target_length))\n",
    "    #encoder_outputs = torch.zeros(target_length, batch_size, encoder.hidden_size, device=device) \n",
    "\n",
    "  \n",
    "\n",
    "    _, context = encoder(input_tensor, encoder_hidden)\n",
    "    #print('in train encoder_hidden[0] is {}, dimension is {}'.format(encoder_hidden[0],encoder_hidden[0].size()))\n",
    "    #print('in train after concatenating encoder_hidden[0] and [1] is {}, dimension is {}'.format(torch.cat((encoder_hidden[0].cpu().data,encoder_hidden[1].cpu().data),dim = 1), torch.cat((encoder_hidden[0].cpu().data,encoder_hidden[1].cpu().data),dim = 1).size()))\n",
    "    \n",
    "    #decoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        #torch.cat((encoder_hidden[0].cpu().data,encoder_hidden[1].cpu().data),dim = 1)).to(device).unsqueeze(0)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)  # decoder_input: torch.Size([1, 32])\n",
    "    decoder_hidden = context.to(device)\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #print('target_tensor is {}, dimension is {}'.format(target_tensor, target_tensor.size()))\n",
    "    \n",
    "    \n",
    "    #print('target_tensor is {}, dimension is {}'.format(target_tensor, target_tensor.size()))\n",
    "    #print('sentence 3 in this batch is {}, dimension is {}'.format(convert_idx_2_sent_new(target_tensor[:,2], target_tra)))\n",
    "    #print('sentence 3 in this batch is {}'.format(convert_idx_2_sent_new(target_tensor[:,2], target_tra)))\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        loss = 0 \n",
    "        criterion = nn.NLLLoss(reduce = True, ignore_index = 2, reduction = 'mean') \n",
    "\n",
    "        for di in range(target_length):\n",
    "            #print('in teacher_forcing, step {}'.format(di))\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            decoder_input = target_tensor[di]  \n",
    "            #print('in teacher forcing, decoder_output at current timestep is {}, dimension is {}'.format(decoder_output, decoder_output.size()))\n",
    "            #print('predicted target at current timestep is {}, dimension is {}'.format(torch.argmax(decoder_output, dim=1), torch.argmax(decoder_output, dim=1).size()))\n",
    "            #print('true target at current timestep is {}, dimension is {}'.format(target_tensor[i], target_tensor[i].size()))\n",
    "            #print('predicted target at current timestep is {}, dimension is {}'.format(decoder_output, decoder_output.size()))\n",
    "            \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            #print ('in teacher forcing, temp loss at current step is {}'.format(temp_loss))\n",
    "            #print('temp_loss for current batch, current token is {}, dimension is {}'.format(temp_loss, temp_loss.size()))\n",
    "            \n",
    "            loss += temp_loss\n",
    "            #loss += temp_loss * mask[di:di+1].float()  \n",
    "            #print('loss is {}, dimension is {}'.format(loss, loss.size()))\n",
    "            #ave_loss = loss.sum()/batch_size\n",
    "        ave_loss = loss/target_length\n",
    "            \n",
    "    else:\n",
    "        loss = None \n",
    "        criterion = nn.NLLLoss(reduce = False) \n",
    "        prediction = None\n",
    "\n",
    "        for di in range(target_length):\n",
    "            #print('in non_teacher forcing, step {}'.format(di))\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            #print('in non_teacher forcing, topi is {}, dimension is {}'.format(topi, topi.size()))\n",
    "            \n",
    "            if prediction is None:\n",
    "                prediction = topi.view(1,-1)\n",
    "            else:\n",
    "                prediction = torch.cat((prediction, topi.view(1,-1)), dim=0)\n",
    "            \n",
    "            #print('at current step, cumulative prediction is {}, dimension is {}'.format(prediction, prediction.size()))\n",
    "            \n",
    "                            \n",
    "            decoder_input = topi.transpose(0,1).detach()  # detach from history as input\n",
    "            #print('in non_teacher forcing, input of the current step is {}, dimension is {}'.format(topi.transpose(0,1),topi.transpose(0,1).size()))\n",
    "            #print('in non_teacher forcing decoder_output at current timestep is {}, dimension is {}'.format(decoder_output, decoder_output.size()))\n",
    "            \n",
    "            #print('predicted target at current timestep is {}, dimension is {}'.format(torch.argmax(decoder_output, dim=1), torch.argmax(decoder_output, dim=1).size()))\n",
    "\n",
    "            #print('true target at current timestep is {}, dimension is {}'.format(target_tensor[i], target_tensor[i].size()))\n",
    "            \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            if loss is None:\n",
    "                loss = temp_loss.view(1,-1)\n",
    "            else:\n",
    "                loss = torch.cat((loss, temp_loss.view(1,-1)),dim=0)\n",
    "            #print('temp_loss for current batch, current token is {}, dimension is {}'.format(temp_loss, temp_loss.size()))\n",
    "            \n",
    "            \n",
    "    \n",
    "    #print('Final prediction is {}'.format(prediction))\n",
    "        mask, count = mask_ind(prediction)\n",
    "        total_loss = torch.sum(loss * torch.from_numpy(mask).float().to(device))\n",
    "        ave_loss = total_loss/count\n",
    "    #print('total_loss is {}, dimension is{}'.format(total_loss, total_loss.size()))        \n",
    "    ave_loss.backward()\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.step()   \n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #print('total valid predicted token is {}'.format(count))\n",
    "    #print('ave_loss type is {}'.format(type(ave_loss)))\n",
    "    #print('ave_loss.item() type is {}'.format(type(ave_loss.item())))\n",
    "    \n",
    "    return ave_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, data_loader, max_length=MAX_SENT_LEN):\n",
    "    start = time.time()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    inputs = []\n",
    "    corpus = []\n",
    "    truths = []\n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(data_loader):\n",
    "#         if i % 5 == 0:\n",
    "#             print('Time: {}, Step: [{}/{}]'.format(\n",
    "#                 timeSince(start, i + 1/len(train_loader)), i, len(data_loader)))\n",
    "        inputs.append(input_sentences.to(device))#put into inputs: batch*seq: each row is a sentence\n",
    "        input_tensor = input_sentences.transpose(0,1).to(device)\n",
    "        truths.append(target_sentences.to(device))#put into truths: batch*seq: each row is a sentence\n",
    "        target_tensor = target_sentences.transpose(0,1).to(device) \n",
    "        #truths.append(target_tensor)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        batch_size = input_tensor.size()[1]\n",
    "    \n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        #encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size, device=device)\n",
    "        _, context = encoder(input_tensor, encoder_hidden)\n",
    "        \n",
    "        #encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        #torch.cat((encoder_hidden[0].cpu().data,encoder_hidden[1].cpu().data),dim = 1)).to(device).unsqueeze(0)\n",
    "        \n",
    "        decoder_hidden = context.to(device)\n",
    "        decoder_input = torch.tensor([[SOS_token]*batch_size], device=device) \n",
    "        decoded_words = torch.zeros(batch_size, max_length)\n",
    "    \n",
    "    \n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoded_words[:,di] = topi.squeeze()  #put into decoded_words: batch*seq\n",
    "            decoder_input = topi.transpose(0,1).detach()\n",
    "            #print('true target is {}, dimension is {}'.format(target_tensor[:,di],target_tensor[di].size()))\n",
    "            #print('before transpose, topi is {}, dimension is {}'.format(topi, topi.size()))\n",
    "            #print('after transpose, topi is {}, dimension is {}'.format(topi.transpose(0,1),topi.transpose(0,1).size()))\n",
    "        corpus.append(decoded_words)\n",
    "        \n",
    "        #print('last: decoded_words is {}, dimension is {}'.format(decoded_words, decoded_words.size()))\n",
    "        #print('last: inputs is {}, dimension is {}'.format(inputs, len(inputs)))\n",
    "        #print('last: truths is {}, dimension is {}'.format(truths, len(truths)))\n",
    "        #print(inputs[0].size(), corpus[0].size(), truths[0].size())\n",
    "    return inputs, corpus, truths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh1087/nlp_environment/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 5s (- -1m 54s), Epoch: [1/10], Step: [20/3335], Train Loss: 11.516338539123534, BLEU: 0.00020406701288841283\n",
      "\n",
      "Input1:> 如果 想见 我 就 拿 着 UNK 来 花园 吧 这样 我们 就 可以 种 点 东西 啦\n",
      "\n",
      "Target1:= If you want to meet with me , come to the garden with your shovel so we can plant some shit  . \n",
      "Predict1:< defecate defecate defecate defecate defecate\n",
      "\n",
      "Input2:> 但是 要 尝试 试一下 一下\n",
      "\n",
      "Target2:= But I will do a trial  . \n",
      "Predict2:< Corfu defecate defecate defecate defecate\n",
      "Time: 0m 11s (- -1m 49s), Epoch: [1/10], Step: [40/3335], Train Loss: 10.959937858581544, BLEU: 0.0\n",
      "\n",
      "Input1:> 所以 我 希望 分享 这种 主动 且 有意 有意识 意识 去 创造 记忆 的 想法\n",
      "\n",
      "Target1:= So I want to share the idea of taking an active role in consciously creating memories  . \n",
      "Predict1:< \n",
      "\n",
      "Input2:> 因此 我 今天 站 在 这里 流 着 阿富汗 的 血 以及 上帝 授予 我 的 乡土 作风 一个 无神 无神论 无神论者 神论 神论者 论者 和 一个 激进 的 政治 艺术 艺术家 术家 并且 在 过去 九年 一直\n",
      "\n",
      "Target2:= And so I stand here today , Afghan by blood , redneck by the grace of God , an atheist and a radically politicized artist who is been living , working and creating in Afghanistan for the last \n",
      "Predict2:< \n",
      "Time: 0m 16s (- -1m 43s), Epoch: [1/10], Step: [60/3335], Train Loss: 10.933368492126466, BLEU: 0.0\n",
      "\n",
      "Input1:> 为什么 什么 因为 他们 看着 图表 上 的 曲线 说 如果 人口 继续 以 这种 速度 增长 纽约 的 人们 出行 将 需要 六百 六百万 百万 UNK 马 而 六百 六百万 百万 UNK 马 所 排泄 的 粪便\n",
      "\n",
      "Target1:= Why  ? Because they looked at the curve and said , if the population keeps growing at this rate , to move the population of New York around , they would have needed six million horses , and \n",
      "Predict1:< \n",
      "\n",
      "Input2:> 几千 几千人 成为 农奴 餐馆 UNK 家庭 UNK 还 不止 这些\n",
      "\n",
      "Target2:= Tens of hundreds of people are enslaved in agriculture , in restaurants , in domestic servitude , and the list can go on  . \n",
      "Predict2:< \n",
      "Time: 0m 21s (- -1m 38s), Epoch: [1/10], Step: [80/3335], Train Loss: 9.873051500320434, BLEU: 0.0\n",
      "\n",
      "Input1:> 在 这个 物欲 UNK 横流 的 时代 里 努力 坚持 自我 铭记 人生 的 短暂 与 生命 的 脆弱 变得 比 以往 任何 何时 时代 都 更 重要\n",
      "\n",
      "Target1:= In our age of increasing distractions , it is more important than ever to find ways to maintain perspective and remember that life is brief and tender  . \n",
      "Predict1:< \n",
      "\n",
      "Input2:> 最终 在 经历 了 漫长 的 旅行 后 我 和 家人 终于 在 韩国 重聚 但是 获得 自由 只是 打赢 了 战争 的 一半\n",
      "\n",
      "Target2:= Eventually , after our long journey , my family and I were reunited in South Korea , but getting to freedom is only half the battle  . \n",
      "Predict2:< \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 27s (- -1m 33s), Epoch: [1/10], Step: [100/3335], Train Loss: 5.304116988182068, BLEU: 0.019996369928600218\n",
      "\n",
      "Input1:> UNK 会议 主持 总统 他们 没有 创造 这个 个头 头衔 我 的 意思 是 这个 个头 头衔 以前 就 有 但是 它 是 指 一个 会议 的 主持 主持人\n",
      "\n",
      "Target1:= President  . They didn t invent the title  . I mean , it existed before , but it just meant somebody who UNK over a meeting  . \n",
      "Predict1:< And  .  .  .  .  .\n",
      "\n",
      "Input2:> 所以 从今 今天 天开 开始 朝 这个 方向 迈出 第一 第一步 一步 我们 开始 了 UNK s Voice UNK 这个 项目\n",
      "\n",
      "Target2:= So , taking the first steps towards this today , we are launching the Parkinson is Voice Initiative  . \n",
      "Predict2:< And  .  .  .  .  .\n",
      "Time: 0m 32s (- -1m 28s), Epoch: [1/10], Step: [120/3335], Train Loss: 4.272518944740296, BLEU: 0.0035138235591212997\n",
      "\n",
      "Input1:> 我 见到 他 时 他 已 在 矿场 工作 了 14 年 照片 片中 看到 的 这个 腿伤 是 在 一次 凿 矿 意外 中 造成 的 当时 情况 UNK 危急 医生 说 要 截肢\n",
      "\n",
      "Target1:= When I met him , he had been working in the mines for 14 years , and the leg injury that you see here is actually from a mining accident , one so severe doctors say his leg \n",
      "Predict1:< I , , , , , , ,\n",
      "\n",
      "Input2:> 现实 和 历史 赋予 了 这个 个头 头衔 高贵 的 含义\n",
      "\n",
      "Target2:= Reality and history have endowed that title with grandeur  . \n",
      "Predict2:< , , , ,\n",
      "Time: 0m 37s (- -1m 22s), Epoch: [1/10], Step: [140/3335], Train Loss: 3.9876989245414736, BLEU: 0.014327535360174446\n",
      "\n",
      "Input1:> 其他 可能 的 结果 包括 长期 的 跟踪 甚至 施虐 者 再婚 之后 仍 会 跟踪 拒绝 绝经 经济 支持 欺骗 家庭 法庭 来 恐吓 受害 受害者 和 她 的 孩子 孩子 们 通常 常会 被 家庭 问题\n",
      "\n",
      "Target1:= Other outcomes include long-term stalking , even after the UNK UNK ; denial of financial resources ; and manipulation of the family court system to terrify the victim and her children , who are regularly forced by family \n",
      "Predict1:< And I , , , , ,\n",
      "\n",
      "Input2:> 为什么 什么 因为 他们 看着 图表 上 的 曲线 说 如果 人口 继续 以 这种 速度 增长 纽约 的 人们 出行 将 需要 六百 六百万 百万 UNK 马 而 六百 六百万 百万 UNK 马 所 排泄 的 粪便\n",
      "\n",
      "Target2:= Why  ? Because they looked at the curve and said , if the population keeps growing at this rate , to move the population of New York around , they would have needed six million horses , and \n",
      "Predict2:< And I , , , , ,\n",
      "Time: 0m 42s (- -1m 17s), Epoch: [1/10], Step: [160/3335], Train Loss: 3.926387083530426, BLEU: 0.01244702570143398\n",
      "\n",
      "Input1:> 如今 我 22 岁\n",
      "\n",
      "Target1:= Today I am 22  . \n",
      "Predict1:< I I , , , , ,\n",
      "\n",
      "Input2:> 我 听到 周围 有人 说话 的 声音 但 更多 是 咳嗽 UNK 混杂 混杂着 用 简陋 工具 砸碎 碎石 石头 的 噪音\n",
      "\n",
      "Target2:= I hear voices talking , but mostly the shaft is this cacophony of men coughing , and stone being broken with primitive tools  . \n",
      "Predict2:< I I I , , , , ,\n",
      "Time: 0m 47s (- -1m 12s), Epoch: [1/10], Step: [180/3335], Train Loss: 3.8163031578063964, BLEU: 0.014658975494179917\n",
      "\n",
      "Input1:> 你 知道 吗 你 可以 选择 任何 的 职业\n",
      "\n",
      "Target1:= You know  ? You can be anything  . \n",
      "Predict1:< And I , , , ,\n",
      "\n",
      "Input2:> 在 阿富汗 你 可以 通过 艺术 艺术作品 作品 创造 展示 很多 多美 美好 的 东西 不过 我 自己 不怎么 怎么 想 描绘 彩虹 我 想要 创作 作出 能够 干扰 身份 挑战 挑战权 权威 的 作品 曝光 那些 UNK\n",
      "\n",
      "Target2:= Now there are a lot of wonderful things that you could make art about in Afghanistan , but personally I don t want to paint rainbows ; I want to make art that disturbs identity and challenges authority \n",
      "Predict2:< And I , , , ,\n",
      "Time: 0m 52s (- -1m 7s), Epoch: [1/10], Step: [200/3335], Train Loss: 3.848144221305847, BLEU: 0.016478482495148324\n",
      "\n",
      "Input1:> 这 很 怪\n",
      "\n",
      "Target1:= That was awkward  . \n",
      "Predict1:< And is , , , , ,\n",
      "\n",
      "Input2:> 于是 有 了 第二 第二个 二个 办法 利用 稻草 稻草人 草人\n",
      "\n",
      "Target2:= And a second idea I got was to use a scarecrow  . \n",
      "Predict2:< And , , , , ,\n",
      "Time: 0m 57s (- -1m 2s), Epoch: [1/10], Step: [220/3335], Train Loss: 3.8046405792236326, BLEU: 0.017565831932170196\n",
      "\n",
      "Input1:> 实际 实际上 我 是 典型 的 家庭 家庭暴力 暴力 受害 受害者 也 是 典型 的 家庭 家庭暴力 暴力 幸存 幸存者\n",
      "\n",
      "Target1:= Because it turns out that I am actually a very typical domestic violence victim and a typical domestic violence survivor  . \n",
      "Predict1:< And I , , , , , , , ,\n",
      "\n",
      "Input2:> 现在 我 的 方法 已经 被 肯尼 肯尼亚 尼亚 的 所有 所有人 有人 使用 来 驱赶 其他 类似 似的 捕食 动物 像 鬣狗 美洲 美洲豹 等 它们 同样 被 用来 驱赶 大象 远离 离人 人类 的 农庄\n",
      "\n",
      "Target2:= And my idea is also being used now all over Kenya for scaring other predators like hyenas , leopards , and it is also being used to scare elephants away from people is farms  . \n",
      "Predict2:< I I , , , , , , , ,\n",
      "Time: 1m 3s (- -2m 57s), Epoch: [1/10], Step: [240/3335], Train Loss: 3.7657713890075684, BLEU: 0.024368608178325417\n",
      "\n",
      "Input1:> 我 想来 分享 几个 有用 的 方法 告诉 你们 如何 才能 保证 让 我们 也 觉得 你们 的 科学 性感 迷人 你们 的 工程 工程学 魅力 UNK 四射\n",
      "\n",
      "Target1:= I want to share a few keys on how you can do that to make sure that we can see that your science is sexy and that your engineering is engaging  . \n",
      "Predict1:< I I I , , , , , , ,\n",
      "\n",
      "Input2:> 而且 速度 也 快 最多 多只 只要 30 秒\n",
      "\n",
      "Target2:= They are high-speed , take about 30 seconds at most  . \n",
      "Predict2:< And is , , , , ,\n",
      "Time: 1m 8s (- -2m 52s), Epoch: [1/10], Step: [260/3335], Train Loss: 3.7295499205589295, BLEU: 0.01798434330186543\n",
      "\n",
      "Input1:> 我 用 蹩脚 的 英文 再 借助 字典 毫不 毫不犹豫 犹豫 地 向 那个 个人 解释 了 情况 那个 个人 走去 ATM 机 为 我 的 家人 以及 另 两个 朝鲜 朝鲜人 支付 了 出狱 的 剩余 的\n",
      "\n",
      "Target1:= In my broken English , and with a dictionary , I explained the situation , and without UNK , the man went to the ATM and he paid the rest of the money for my family and two \n",
      "Predict1:< I I I , , , , ,\n",
      "\n",
      "Input2:> 但是 今天 阿富汗 接受 受教 教育 的 女孩 已经 超过 300 万\n",
      "\n",
      "Target2:= But today , more than three million girls are in school in Afghanistan  . \n",
      "Predict2:< And is , , , ,\n",
      "Time: 1m 13s (- -2m 46s), Epoch: [1/10], Step: [280/3335], Train Loss: 3.877229368686676, BLEU: 0.017534406380626394\n",
      "\n",
      "Input1:> 最近 纽约 纽约时报 时报 报报 报道 美国 每年 有 10 万 到 30 万 小孩 被 买卖 UNK 成为 性 奴\n",
      "\n",
      "Target1:= Recently , the New York Times reported that between 100,000 and 300,000 American children are sold into sex slavery every year  . \n",
      "Predict1:< And is , , , ,\n",
      "\n",
      "Input2:> 如果 有 任何 异样 我 就 会 入狱 并 被 遣送 送回 回去\n",
      "\n",
      "Target2:= If anything seemed unnatural , I could be imprisoned and UNK  . \n",
      "Predict2:< And I , , , , , ,\n",
      "Time: 1m 18s (- -2m 41s), Epoch: [1/10], Step: [300/3335], Train Loss: 3.722635269165039, BLEU: 0.020531887954566556\n",
      "\n",
      "Input1:> 谁 能 发明 这种 科技 来 实现 绿色 绿色革命 革命 大学 吗 别 想 了\n",
      "\n",
      "Target1:= Who is going to invent the technology for the green revolution  ? Universities  ? Forget about it  ! \n",
      "Predict1:< And is , , , ,\n",
      "\n",
      "Input2:> 当在 后台 时 我们 会 给出 一个 像 这样 的 答案\n",
      "\n",
      "Target2:= And when we are backstage , we give an answer that maybe makes it seem like that  . \n",
      "Predict2:< And is , , , ,\n",
      "Time: 1m 23s (- -2m 36s), Epoch: [1/10], Step: [320/3335], Train Loss: 3.6572103977203367, BLEU: 0.019154514197526066\n",
      "\n",
      "Input1:> 每 20 分钟 我 就 必须 跑 回车 车上 清理 我 的 设备 让 它 在 空调 底下 吹 着 冷气 它 才能 重新 重新启动 启动 我 坐在 车上 想 就 连 摄像 摄像机 受到 的 待遇 都\n",
      "\n",
      "Target1:= Every 20 minutes , I d have to run back to our UNK to clean out my gear and run it under an air conditioner to revive it , and as I sat there , I thought , \n",
      "Predict1:< And I , , , , , , ,\n",
      "\n",
      "Input2:> 如果 听完 这些 她们 还是 说 不 不 Cameron 我 想 当 模特 我会 说 当 我 的 老板 吧\n",
      "\n",
      "Target2:= If , after this amazing list , they still are like , &quot; No , no , Cameron , I want to be a model , &quot; well then I say , &quot; Be my boss  . &quot; \n",
      "Predict2:< And is , , , , , ,\n",
      "Time: 1m 28s (- -2m 31s), Epoch: [1/10], Step: [340/3335], Train Loss: 3.6388954877853394, BLEU: 0.02869302279173671\n",
      "\n",
      "Input1:> 我们 西方 西方人 方人 是 帝国 帝国主义 帝国主义者 国主 主义 主义者 殖民 殖民主义 殖民主义者 民主 主义 主义者 以及 传教 传教士 教士 我们 跟 人们 打交道 交道 的 方式 只有 两种 一 是 居高临下 UNK 二 是 家长 做派\n",
      "\n",
      "Target1:= We Western people are imperialist , UNK missionaries , and there are only two ways we deal with people : We either patronize them , or we are paternalistic  . \n",
      "Predict1:< And is is , , , , ,\n",
      "\n",
      "Input2:> 一些 光 流 了 出来 到 了 桌子 上 所以 你 开始 看到 这些 波纹\n",
      "\n",
      "Target2:= Some of the light leaks , goes on the table , and you start seeing these ripples of waves  . \n",
      "Predict2:< And is is , , ,\n",
      "Time: 1m 34s (- -2m 26s), Epoch: [1/10], Step: [360/3335], Train Loss: 3.663482391834259, BLEU: 0.02898568542031679\n",
      "\n",
      "Input1:> 可以 了\n",
      "\n",
      "Target1:= All right  . \n",
      "Predict1:< And is is\n",
      "\n",
      "Input2:> 在 日本 呆 了 6 个 月 以后 携手 志愿 志愿者 愿者 共有 1 100 名 志愿 志愿者 愿者 加入 帮助 日本 救灾 其中 几百 几百名 百名 帮 我们 手工 清洗 超过 13 5 万张 照片 其中 的\n",
      "\n",
      "Target2:= After six months in Japan , 1,100 volunteers had passed through All Hands , hundreds of whom had helped us UNK over 135,000 photographs , the large majority \n",
      "Predict2:< And , , , , , , , , , , , , , , , , ,\n",
      "Time: 1m 39s (- -2m 20s), Epoch: [1/10], Step: [380/3335], Train Loss: 3.604764401912689, BLEU: 0.03529359586838904\n",
      "\n",
      "Input1:> 我们 用 的 就是 清洁 厨具 用 的 钢丝 丝绒 钢丝 丝绒 可以 UNK 很小 小块 我们 把 这些 些小 小块 混进 沥青\n",
      "\n",
      "Target1:= So what we can do is use steel wool just to clean pans , and the steel wool we can cut in very small pieces , and these very small pieces we can mix to the bitumen  . \n",
      "Predict1:< And is is , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 他们 在内 内罗毕 国家 公园 杀 的 六个 狮子 中 的 其中 一个\n",
      "\n",
      "Target2:= It is one of the six lions which were killed in Nairobi  . \n",
      "Predict2:< And is is is , , , ,\n",
      "Time: 1m 45s (- -2m 15s), Epoch: [1/10], Step: [400/3335], Train Loss: 3.535254168510437, BLEU: 0.036938021924431616\n",
      "\n",
      "Input1:> 如果 有些 女士 对 的 我 出现 感到 惊恐 不必 现在 告诉 我 但 我 马上 就 会 在 twitter 上知 知道 的\n",
      "\n",
      "Target1:= If some of the women were really horrified when I came out , you don t have to tell me now , but I ll find out later on Twitter  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 不仅 因为 它 听起 起来 很 不错\n",
      "\n",
      "Target2:= Just because it sounds so good  . \n",
      "Predict2:< And is is is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1m 50s (- -2m 9s), Epoch: [1/10], Step: [420/3335], Train Loss: 3.513837218284607, BLEU: 0.05556175303154072\n",
      "\n",
      "Input1:> 我们 必须 学会 如何 让 这些 人 给出 他们 的 意见\n",
      "\n",
      "Target1:= You have to learn how to get these people to come and talk to you  . \n",
      "Predict1:< And is is is the  .  .\n",
      "\n",
      "Input2:> 举例 举例来说 来说 一直 直到 UNK 年 英国 英国议会 国议会 议会 才 允许 新闻 闻报 报纸 报道 在 辩论 辩论会 会上 政客 们 说 的 真实 实话 话语\n",
      "\n",
      "Target2:= It wasn t until , for example , UNK that the British Parliament allowed newspapers to report the exact words that were said in the debating chamber  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1m 55s (- -2m 4s), Epoch: [1/10], Step: [440/3335], Train Loss: 3.519381070137024, BLEU: 0.03534524268133123\n",
      "\n",
      "Input1:> 他 告诉 我 两件 两件事\n",
      "\n",
      "Target1:= He told me two things  . \n",
      "Predict1:< And is is , , ,\n",
      "\n",
      "Input2:> 别误会 误会\n",
      "\n",
      "Target2:= Now don t get me wrong  . \n",
      "Predict2:< And is is is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 1s (- -3m 58s), Epoch: [1/10], Step: [460/3335], Train Loss: 3.458109724521637, BLEU: 0.03502972049442539\n",
      "\n",
      "Input1:> 几千 几千人 成为 农奴 餐馆 UNK 家庭 UNK 还 不止 这些\n",
      "\n",
      "Target1:= Tens of hundreds of people are enslaved in agriculture , in restaurants , in domestic servitude , and the list can go on  . \n",
      "Predict1:< And is , , , , , ,\n",
      "\n",
      "Input2:> 但 最难 的 是 去 展现 性别 和 种族 种族歧视 歧视 的 职业 传统 因为 我 自己 本身 是 最大 的 受益 受益者\n",
      "\n",
      "Target2:= But mostly it was difficult to unpack a legacy of gender and racial oppression when I am one of the biggest beneficiaries  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 6s (- -3m 53s), Epoch: [1/10], Step: [480/3335], Train Loss: 3.386720025539398, BLEU: 0.03280991095638069\n",
      "\n",
      "Input1:> 我 养 了 一只 黑色 色拉 拉布 拉布拉多 布拉 拉多 猎犬 开本 UNK 奥 赛德 面包 面包车 包车\n",
      "\n",
      "Target1:= My dog is a black lab , and I drive a UNK Odyssey UNK  . \n",
      "Predict1:< I I I I I , , , , , , , , , ,\n",
      "\n",
      "Input2:> 照片 是 一个 提醒 和 留念 提醒 我们 一些 人 一些 事 某个 地方 某 段 感情 某个 深爱 的 人\n",
      "\n",
      "Target2:= A photo is a reminder of someone or something , a place , a relationship , a loved one  . \n",
      "Predict2:< And is , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 12s (- -3m 47s), Epoch: [1/10], Step: [500/3335], Train Loss: 3.5147376537322996, BLEU: 0.03599590288021382\n",
      "\n",
      "Input1:> 因为 我 必须 每天 天都 要 担心 我 看上 看上去 上去 怎么 怎么样\n",
      "\n",
      "Target1:= And I am insecure because I have to think about what I look like every day  . \n",
      "Predict1:< And I , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 清理 了 大量 的 发臭 腐烂 的 鱼 尸骨 那些 鱼 都 是 当地 工厂 厂里 里加 加工 处理 的 鱼\n",
      "\n",
      "Target2:= We cleared tons and tons of stinking , rotting fish carcasses from the local fish processing plant  . \n",
      "Predict2:< And is , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 17s (- -3m 42s), Epoch: [1/10], Step: [520/3335], Train Loss: 3.558142137527466, BLEU: 0.036720412522991734\n",
      "\n",
      "Input1:> 正 因为 有人 表明 了 立场 影响 了 他 的 人生 他 的 未来 才 可能 有所 改变\n",
      "\n",
      "Target1:= Who will he become because someone took a stand and made a difference in his life  ? \n",
      "Predict1:< And is , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 它们 简直 就是 鸟类 粉碎 粉碎机\n",
      "\n",
      "Target2:= They are UNK machines  . \n",
      "Predict2:< And is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 23s (- -3m 36s), Epoch: [1/10], Step: [540/3335], Train Loss: 3.530163061618805, BLEU: 0.034021590009767164\n",
      "\n",
      "Input1:> 尤其 刚 开始 的 时候 他 让 我 觉得 我 是 他 崇拜 的 偶像\n",
      "\n",
      "Target1:= He did this especially at the beginning by UNK me  . \n",
      "Predict1:< And , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 我 在 修复 那些 照片 的 时候 我 不禁 想到 其中 的 每个 个人 以及 照片 片中 传递 的 那些 故事\n",
      "\n",
      "Target2:= &quot; As I worked , I couldn t help but think about the individuals and the stories represented in the images  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 28s (- -3m 31s), Epoch: [1/10], Step: [560/3335], Train Loss: 3.3662715315818788, BLEU: 0.034651616916572085\n",
      "\n",
      "Input1:> 我们 能 进行 高频 高频率 频率 的 监测 以 获取 客观 数据\n",
      "\n",
      "Target1:= We can do high-frequency monitoring to get objective data  . \n",
      "Predict1:< And is is is , , , ,  .  .\n",
      "\n",
      "Input2:> 不然 他们 不会 会来 找 你 谈 的\n",
      "\n",
      "Target2:= Otherwise they don t come and talk to you  . \n",
      "Predict2:< And is is , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 34s (- -3m 25s), Epoch: [1/10], Step: [580/3335], Train Loss: 3.38189115524292, BLEU: 0.03290633867290619\n",
      "\n",
      "Input1:> 我 看到 轮椅 像 二手 汽车 一样 进行 买卖\n",
      "\n",
      "Target1:= I see wheelchairs bought and sold like used cars  . \n",
      "Predict1:< I I I I , , , ,\n",
      "\n",
      "Input2:> 自己 种 食物 就 好像 自己 印 钱 似的\n",
      "\n",
      "Target2:= Growing your own food is like printing your own money  . \n",
      "Predict2:< And is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 39s (- -3m 20s), Epoch: [1/10], Step: [600/3335], Train Loss: 3.449207103252411, BLEU: 0.037924864386341635\n",
      "\n",
      "Input1:> 你们 自己 去 读读 看\n",
      "\n",
      "Target1:= Just go and read her book  . \n",
      "Predict1:< And is is  .  .  .\n",
      "\n",
      "Input2:> 我 第一 第一次 一次 有 了 自己 的 房子 公寓 有 了 第一 第一张 一张 信用 信用卡 我 还有 一个 非常 大 的 秘密\n",
      "\n",
      "Target2:= I had my first apartment , my first little green American Express card , and I had a very big secret  . \n",
      "Predict2:< I I I I I I I , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 45s (- -3m 15s), Epoch: [1/10], Step: [620/3335], Train Loss: 3.432429480552673, BLEU: 0.037149477868475646\n",
      "\n",
      "Input1:> 因为 国会 国会议员 会议 会议员 议员 们 不能 保持 自然 自然环境 环境 的 清洁\n",
      "\n",
      "Target1:= Because MPs do not keep the environment clean  . \n",
      "Predict1:< And is is , , , , , , , ,\n",
      "\n",
      "Input2:> 当然 当地 当地人 对此 表示 毫无 无兴趣 兴趣 所以 我们 付 工资 请 他们 来 劳作 他们 也 是 一会 一会儿 会儿 来 一会 一会儿 会儿 不来 的\n",
      "\n",
      "Target2:= And of course the local people had absolutely no interest in doing that , so we paid them to come and work , and sometimes they would show up  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 50s (- -3m 9s), Epoch: [1/10], Step: [640/3335], Train Loss: 3.3649975419044496, BLEU: 0.03499923640212195\n",
      "\n",
      "Input1:> 那 是 我 人生 中 最低 低落 的 时刻 之一\n",
      "\n",
      "Target1:= This was one of the lowest points in my life  . \n",
      "Predict1:< And is is , , , , ,  .\n",
      "\n",
      "Input2:> 我们 能 立体 的 看 懂吗\n",
      "\n",
      "Target2:= Can we see it in full 3D  ? \n",
      "Predict2:< We is is is  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 56s (- -3m 4s), Epoch: [1/10], Step: [660/3335], Train Loss: 3.423082971572876, BLEU: 0.03490794182094419\n",
      "\n",
      "Input1:> 我 想要 把 集装 集装箱 装箱 改造 为 健康 的 咖啡 咖啡屋\n",
      "\n",
      "Target1:= I want to take shipping containers and turn them into healthy cafes  . \n",
      "Predict1:< I I I I I I I I  .  .  .  .\n",
      "\n",
      "Input2:> 那 就是 她\n",
      "\n",
      "Target2:= There she is  . \n",
      "Predict2:< And is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 1s (- -4m 58s), Epoch: [1/10], Step: [680/3335], Train Loss: 3.368861639499664, BLEU: 0.03611963956481488\n",
      "\n",
      "Input1:> 此外 我 意识 到 韩国 与 朝鲜 之间 有 巨大 的 差异\n",
      "\n",
      "Target1:= Also , I realized there was a wide gap between North and South  . \n",
      "Predict1:< I I I , , , , , , ,\n",
      "\n",
      "Input2:> 通过 过透 透露 他 不为 不为人知 的 秘密 他 在 我们 之间 营造 了 奇特 的 相互 相互信任 互信 信任 的 氛围 他 告诉 我 他 从 四岁 开始 他 的 继父 就 开始 持续 野蛮 的 在\n",
      "\n",
      "Target2:= And he also created a magical atmosphere of trust between us by confessing his secret , which was that , as a very young boy starting at age four , he had been savagely and repeatedly physically abused \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 7s (- -4m 53s), Epoch: [1/10], Step: [700/3335], Train Loss: 3.370510482788086, BLEU: 0.034464419957658254\n",
      "\n",
      "Input1:> 尽管 这 只是 一个 19 世纪 的 报纸 编辑 给出 的 一个 很 好 的 定义 他 的 原话 UNK 是 一个 这样 的 家伙 他 会 为了 谋求 高位 而 不在 不在乎 在乎 任何 政党 任何 舞台\n",
      "\n",
      "Target1:= Although there was a 19th-century newspaper editor who defined it rather better when he said , &quot; A UNK is a fellow who seeks office regardless of party , platform or principle , and who , when he \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 向 客户 提出 你 的 点子\n",
      "\n",
      "Target2:= You can give somebody an idea  . \n",
      "Predict2:< And is , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 12s (- -4m 47s), Epoch: [1/10], Step: [720/3335], Train Loss: 3.426781952381134, BLEU: 0.0335751114849217\n",
      "\n",
      "Input1:> 我 没想 想着 免费 因为 免费 是 不能 持续 的\n",
      "\n",
      "Target1:= I am not talking about no free shit , because free is not sustainable  . \n",
      "Predict1:< I I I , , , , ,  .\n",
      "\n",
      "Input2:> 如果 有 任何 异样 我 就 会 入狱 并 被 遣送 送回 回去\n",
      "\n",
      "Target2:= If anything seemed unnatural , I could be imprisoned and UNK  . \n",
      "Predict2:< And , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 18s (- -4m 42s), Epoch: [1/10], Step: [740/3335], Train Loss: 3.4762234807014467, BLEU: 0.034604316694353175\n",
      "\n",
      "Input1:> 我 在 印度 拜访 过 一个 村子 全家 全家人 家人 都 是 丝绸 贸易 的 奴隶\n",
      "\n",
      "Target1:= I visited villages in India where entire families were enslaved in the silk trade  . \n",
      "Predict1:< I I I I I , , , , , ,\n",
      "\n",
      "Input2:> 我 只是 一个 有着 亲身 亲身经历 身经 经历 要 讲述 的 女人\n",
      "\n",
      "Target2:= I am just one woman with a story to tell  . \n",
      "Predict2:< I I I I I I to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 23s (- -4m 36s), Epoch: [1/10], Step: [760/3335], Train Loss: 3.372128665447235, BLEU: 0.03479996286419976\n",
      "\n",
      "Input1:> 子弹 是 干什么 什么 的 子弹 能 杀人 也 能 灭 了 你 的 演示 报告\n",
      "\n",
      "Target1:= What do bullets do  ? UNK kill , and they will kill your presentation  . \n",
      "Predict1:< And is is , , , , , , ,\n",
      "\n",
      "Input2:> 我们 做 了 老化 实验 加 了 很多 负载 用 我们 的 感应 UNK 电机 给 它们 加热 重复 加热 和 测试\n",
      "\n",
      "Target2:= So we did aging on the samples , did a lot of loading on it , healed them with our induction machine , and healed them and tested them again  . \n",
      "Predict2:< And we , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 29s (- -4m 31s), Epoch: [1/10], Step: [780/3335], Train Loss: 3.5204004645347595, BLEU: 0.03605799770061705\n",
      "\n",
      "Input1:> 我 看见 她们 的 双亲 尤其 是 她们 的 父亲 就 像 我 父亲 那样 支持 她们 尽管 从 他们 脸上 也 看出 了 畏惧\n",
      "\n",
      "Target1:= And I see their parents and their fathers who , like my own , advocate for them , despite and even in the face of daunting opposition  . \n",
      "Predict1:< I I I I , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 她们 带 着 我 走下 一段 一段段 狭窄 的 楼梯 到 了 一个 肮脏 昏暗 的 地下 地下室\n",
      "\n",
      "Target2:= They ushered me down a narrow set of stairs that led to this dirty , dimly fluorescent lit basement  . \n",
      "Predict2:< And , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 34s (- -4m 25s), Epoch: [1/10], Step: [800/3335], Train Loss: 3.4539692282676695, BLEU: 0.03865296913869324\n",
      "\n",
      "Input1:> 政府 免费 授权 了 一段 高速 高速公路 公路 给 我们 在 UNK 号 上 400 米 的 距离 我们 就 在 那里 做 了 材料 测试\n",
      "\n",
      "Target1:= So they donated to us a piece of highway , 400 meters of the UNK , where we had to make a test track to test this material  . \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当然 它 也 有 缺点 这种 路 的 缺点 是 沥青 可能 会 UNK\n",
      "\n",
      "Target2:= It also has disadvantages , of course , and the disadvantage of this road is that UNK can occur  . \n",
      "Predict2:< And is , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 40s (- -4m 20s), Epoch: [1/10], Step: [820/3335], Train Loss: 3.413427102565765, BLEU: 0.03570436524380234\n",
      "\n",
      "Input1:> 我 拒绝 成为 由 别人 人为 我 打造 的 这种 现实 的 一部 一部分 部分 我 要 自己 创造 自己 的 现实\n",
      "\n",
      "Target1:= And I refuse to be a part of this manufactured reality that was manufactured for me by some other people , and I am manufacturing my own reality  . \n",
      "Predict1:< I I I I I I I I , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 第一 第一件 一件 是 他 也 毕业 于 常春 常春藤 联盟 学校 并且 在 一个 非常 好 的 华尔街 银行 工作\n",
      "\n",
      "Target2:= One was that he , too , had just graduated from an Ivy League school , and that he worked at a very impressive Wall Street bank  . \n",
      "Predict2:< And is is , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 45s (- -4m 14s), Epoch: [1/10], Step: [840/3335], Train Loss: 3.3850318670272825, BLEU: 0.03648440965781752\n",
      "\n",
      "Input1:> 即便 即便如此 如此 你 还是 不想 继续 吗\n",
      "\n",
      "Target1:= So do you still not want to continue  ? &quot; \n",
      "Predict1:< And is is , , ,  .  .  .\n",
      "\n",
      "Input2:> UNK 我 的 下 一项 项发明 发明 是 我 想 做 一个 电力 栅栏 CA 电力 栅栏\n",
      "\n",
      "Target2:= My next invention is , I want to make an electric fence  . Electric fence  ? \n",
      "Predict2:< I I I I I , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 50s (- -4m 9s), Epoch: [1/10], Step: [860/3335], Train Loss: 3.4129316806793213, BLEU: 0.035808511863256315\n",
      "\n",
      "Input1:> 英语 在 韩国 太重 重要 了 所以 我 必须 须要 开始 学习 第三 UNK 语言 英语\n",
      "\n",
      "Target1:= English was so important in South Korea , so I had to start learning my third language  . \n",
      "Predict1:< And , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 再 需要 一个 像 这样 的 机器 一个 感应 UNK 电机 它 可以 用来 做饭\n",
      "\n",
      "Target2:= Then you need a machine , like you see here , that you can use for cooking -- an induction machine  . \n",
      "Predict2:< And is , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3m 56s (- -4m 3s), Epoch: [1/10], Step: [880/3335], Train Loss: 3.3512953400611876, BLEU: 0.04011977875481418\n",
      "\n",
      "Input1:> 音乐 音乐 掌声 现在 我们 在 把 他们 应用 用到 生活 中 还有 一些 路 要 走 但 在 未来 我们 可以 把 这个 技术 放到 车里 这样 样车 就 能 防止 碰撞 因为 他们 能 看到 拐角\n",
      "\n",
      "Target1:= Now we have some ways to go before we take this outside the lab on the road , but in the future , we could create cars that avoid collisions with what is around the bend , or \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 原理 是 我们 可以 把 一些 光 打到 门上\n",
      "\n",
      "Target2:= The idea is that we could shine some light on the door  . \n",
      "Predict2:< And is is is is is is  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 1s (- -5m 58s), Epoch: [1/10], Step: [900/3335], Train Loss: 3.3756153345108033, BLEU: 0.03504306880831757\n",
      "\n",
      "Input1:> 而 可持续性 持续 持续性 的 搞笑 之处 处在 在于 你 必须 维持 它\n",
      "\n",
      "Target1:= The funny thing about sustainability , you have to sustain it  . \n",
      "Predict1:< And is , , , , , , , , ,\n",
      "\n",
      "Input2:> 相反 正如 爱因斯坦 因斯 坦 所说 让 一切 尽可 尽可能 可能 的 简单 而 不是 简化\n",
      "\n",
      "Target2:= Instead , as Einstein said , make everything as simple as possible , but no simpler  . \n",
      "Predict2:< And is , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 7s (- -5m 52s), Epoch: [1/10], Step: [920/3335], Train Loss: 3.5211785316467283, BLEU: 0.03891614096202401\n",
      "\n",
      "Input1:> 第一 第一件 一件 是 他 也 毕业 于 常春 常春藤 联盟 学校 并且 在 一个 非常 好 的 华尔街 银行 工作\n",
      "\n",
      "Target1:= One was that he , too , had just graduated from an Ivy League school , and that he worked at a very impressive Wall Street bank  . \n",
      "Predict1:< And is is , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 和 一个 温柔 UNK 善良 男人 再次 结婚 有 了 三个 孩子\n",
      "\n",
      "Target2:= I remarried a kind and gentle man , and we have those three kids  . \n",
      "Predict2:< I I I I I , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 12s (- -5m 47s), Epoch: [1/10], Step: [940/3335], Train Loss: 3.3465731263160707, BLEU: 0.03929536453570174\n",
      "\n",
      "Input1:> 这里 本身 不是 一间 妓院\n",
      "\n",
      "Target1:= This wasn t a brothel , per se  . \n",
      "Predict1:< And is is  .  .  .  .\n",
      "\n",
      "Input2:> 我们 都 知道 自己 正 冒 着 生命 的 危险 老师 学生 以及 家长 都 很 明白\n",
      "\n",
      "Target2:= We all knew we were risking our lives -- the teacher , the students and our parents  . \n",
      "Predict2:< We we we we , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 18s (- -5m 41s), Epoch: [1/10], Step: [960/3335], Train Loss: 3.5037595510482786, BLEU: 0.03500063647663475\n",
      "\n",
      "Input1:> 照片 是 一个 提醒 和 留念 提醒 我们 一些 人 一些 事 某个 地方 某 段 感情 某个 深爱 的 人\n",
      "\n",
      "Target1:= A photo is a reminder of someone or something , a place , a relationship , a loved one  . \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 初次 尝试 的 一张 照片\n",
      "\n",
      "Target2:= This was the very first  . \n",
      "Predict2:< It is is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 23s (- -5m 36s), Epoch: [1/10], Step: [980/3335], Train Loss: 3.4401856303215026, BLEU: 0.03818914887938435\n",
      "\n",
      "Input1:> 记得 得当 当时 站 在 几乎 是 一片 黑暗 当中 我 刹那 刹那间 那间 感到 一阵 强烈 的 恐惧 那一刻 一刻 我 在 想 被困 在 这个 地狱 里 是 多么 的 恐怖\n",
      "\n",
      "Target1:= Standing in the near darkness , I remember feeling this quick , hot fear , and in that instant , I could only imagine what it must be like to be trapped in that hell  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 超过 85 的 施虐 者 是 男性 并且 家庭 家庭暴力 暴力 只 发生 生在 亲密 的 相互 相互依存 依存 的 长期 的 关系 中 换句 换句话 换句话说 发生 生在 在家 家庭 中 这 是 我们 最 不 愿意\n",
      "\n",
      "Target2:= Over 85 percent of abusers are men , and domestic abuse happens only in intimate , interdependent , long-term relationships , in other words , in families , the last place we would want or expect to find \n",
      "Predict2:< And is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 29s (- -5m 30s), Epoch: [1/10], Step: [1000/3335], Train Loss: 3.447231984138489, BLEU: 0.040116082272281486\n",
      "\n",
      "Input1:> 你们 可以 读读 这本 本书 叫 致命 援助 作者 是 丹比 比萨 莫 约 一位 赞比亚 比亚 女 经济 经济学 经济学家 济学 学家\n",
      "\n",
      "Target1:= You want to read the book , read &quot; Dead Aid , &quot; by Dambisa Moyo , Zambian woman economist  . \n",
      "Predict1:< And is , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 就是 我 的 负担 你 的 呢\n",
      "\n",
      "Target2:= That is my burden  . What is yours  ? \n",
      "Predict2:< And is is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 34s (- -5m 25s), Epoch: [1/10], Step: [1020/3335], Train Loss: 3.4344047784805296, BLEU: 0.03990546154929048\n",
      "\n",
      "Input1:> 所以 我们 可以 建立 一个 社区 在 这个 社区 区里 我们 有 来自 自小 小型 小型企业 企业 背景 的 辅助 商 在 咖啡 咖啡馆 中 酒吧 里 为 你 出谋划策 谋划 划策 我们 能 为 你 提供 这位\n",
      "\n",
      "Target1:= So we can create the community where we have facilitators who come from a small business background sitting in cafes , in bars , and your dedicated buddies who will do to you , what somebody did for \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 谢谢\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< And is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 40s (- -5m 19s), Epoch: [1/10], Step: [1040/3335], Train Loss: 3.4025956869125364, BLEU: 0.04324434799090457\n",
      "\n",
      "Input1:> 不是 的 它 是 出自 出自于 一个 新闻 新闻自由 自由 卫士\n",
      "\n",
      "Target1:= It is not  . It is down to a campaigner for the freedom of the press  . \n",
      "Predict1:< It is is , , , , ,  .  .\n",
      "\n",
      "Input2:> 这 是 个 很 悲惨 的 事实 那 就是 朝鲜 朝鲜人 必须 须要 隐姓埋名 埋名 如此 努力 仅仅 为了 能 生存 存下 下去\n",
      "\n",
      "Target2:= It is tragic that North Koreans have to hide their identities and struggle so hard just to survive  . \n",
      "Predict2:< And is is is is , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 45s (- -5m 14s), Epoch: [1/10], Step: [1060/3335], Train Loss: 3.5422894239425657, BLEU: 0.04519725138308344\n",
      "\n",
      "Input1:> 我 的 第一 第一个 一个 答案 是 我 也 不知 知道 因为 他们 没让 我 来 掌管 这件 事\n",
      "\n",
      "Target1:= And the first answer is , &quot; I don t know , they don t put me in charge of that  . &quot; \n",
      "Predict1:< I I I I I , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 开始 意识 到 狮子 们 会 害怕 移动 的 光\n",
      "\n",
      "Target2:= And I discovered that lions are afraid of a moving light  . \n",
      "Predict2:< I I I I I , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 51s (- -5m 9s), Epoch: [1/10], Step: [1080/3335], Train Loss: 3.416458773612976, BLEU: 0.04576711809589498\n",
      "\n",
      "Input1:> 再加 加上 我 着实 厌倦 了 开 45 分钟 的 来回 去 买 个 没 打过 杀虫 杀虫剂 的 苹果\n",
      "\n",
      "Target1:= Plus I got tired of driving 45 minutes round trip to get an apple that wasn t impregnated with pesticides  . \n",
      "Predict1:< I I , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 而 这个 个头 头衔 叫 UNK\n",
      "\n",
      "Target2:= And that title was &quot; President  . &quot; \n",
      "Predict2:< And is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4m 56s (- -5m 3s), Epoch: [1/10], Step: [1100/3335], Train Loss: 3.407421827316284, BLEU: 0.04126646277079105\n",
      "\n",
      "Input1:> 在 西藏 UNK UNK 文化 中 秃鹫 尤其 显得 重要 UNK 在 那些 类似 西藏 的 地方 既 没有 条件 UNK 也 没有 木材 去 进行 火葬 所以 秃鹫 为 当地 当地人 提供 了 一种 种天然 天然 的\n",
      "\n",
      "Target1:= In Tibetan culture , they are performing very important sky burials  . In places like Tibet , there are no places to bury the dead , or wood to UNK them , so these vultures provide a natural \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 第一 你 要 给予 他们 保密 的 承诺\n",
      "\n",
      "Target2:= First , you have to offer them UNK  . \n",
      "Predict2:< And is , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 2s (- -6m 58s), Epoch: [1/10], Step: [1120/3335], Train Loss: 3.4331465363502502, BLEU: 0.042641680625766315\n",
      "\n",
      "Input1:> 他 骄傲 的 不仅 不仅仅 仅仅 是因为 因为 我 的 大学 学学 学历 也 是因为 因为 在 女性 中 我 是 第一 第一个 一个 在 女性 中 我 是 第一 第一个 一个 开车 车带 他 在 喀布尔 布尔\n",
      "\n",
      "Target1:= He not only UNK about my college degree , but also that I was the first woman , and that I am the first woman to drive him through the streets of Kabul  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 事实 事实上 我 要 先 让 它 冷却 下来 才能 向 你们 展示 加热 修复 是否 有效\n",
      "\n",
      "Target2:= Actually , it still has to cool down first before I can show you that the healing works  . \n",
      "Predict2:< And I , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 7s (- -6m 52s), Epoch: [1/10], Step: [1140/3335], Train Loss: 3.4562111377716063, BLEU: 0.04428793089401271\n",
      "\n",
      "Input1:> 他 告诉 我 两件 两件事\n",
      "\n",
      "Target1:= He told me two things  . \n",
      "Predict1:< I I I to  .  .  .\n",
      "\n",
      "Input2:> 上面 写道 当 你 读到 这 封信 时 我家 的 所有 五个 个人 都 将 离开 人世 因为 我们 已经 连续 两周 没有 吃 东西 了\n",
      "\n",
      "Target2:= It read , &quot; When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks  . \n",
      "Predict2:< And is , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 13s (- -6m 47s), Epoch: [1/10], Step: [1160/3335], Train Loss: 3.2998868942260744, BLEU: 0.04178360515261686\n",
      "\n",
      "Input1:> 我 并 不是 精神 精神病 医生 社会 社会工作 社会工作者 工作 工作者 作者 也 不是 家庭 家庭暴力 暴力 方面 的 专家\n",
      "\n",
      "Target1:= I am not a psychiatrist , a social worker or an expert in domestic violence  . \n",
      "Predict1:< I I I I , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 把 书 放在 食品 杂货 袋中 这样 别人 就 会 以为 我们 只是 要 去 购物\n",
      "\n",
      "Target2:= We would cover our books in grocery bags so it would seem we were just out shopping  . \n",
      "Predict2:< We we we we we we , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 18s (- -6m 41s), Epoch: [1/10], Step: [1180/3335], Train Loss: 3.5391891956329347, BLEU: 0.044899419353460084\n",
      "\n",
      "Input1:> 他们 甚至 并 没有 延续 君主 君主制 他们 的 主意 是 你 可以 以是 选举 的 国王 有 一个 固定 的 任期\n",
      "\n",
      "Target1:= And they weren t even being UNK there , they had the idea that you could be elected King for a fixed term  . \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 空气 污浊 闷热 充斥 着 尘土 让 人 很 难 呼吸\n",
      "\n",
      "Target2:= The air is thick with heat and dust , and it is hard to breathe  . \n",
      "Predict2:< And is , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 24s (- -6m 36s), Epoch: [1/10], Step: [1200/3335], Train Loss: 3.399912881851196, BLEU: 0.043637793147804985\n",
      "\n",
      "Input1:> 讲完 了 谢谢\n",
      "\n",
      "Target1:= Peace  . Thank you  . \n",
      "Predict1:< And is  .  .\n",
      "\n",
      "Input2:> 由于 感应 UNK 电机 太大 了 拿 不 上来 我 在 这里 就 用 微波 微波炉 演示 了\n",
      "\n",
      "Target2:= Today I use a microwave because I cannot take the big induction machine here onstage  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 29s (- -6m 30s), Epoch: [1/10], Step: [1220/3335], Train Loss: 3.501010525226593, BLEU: 0.04699053470476448\n",
      "\n",
      "Input1:> 洛杉矶 所 拥有 的 空地 的 数量 在 美国 国名 名列 名列前茅 前茅\n",
      "\n",
      "Target1:= L .A . leads the United States in vacant lots that the city actually owns  . \n",
      "Predict1:< And is , , , , , , , ,\n",
      "\n",
      "Input2:> 而且 和 芭蕾 一样 这 也 需要 经过 特殊 地 训练\n",
      "\n",
      "Target2:= And like ballet , it takes an extraordinary level of training  . \n",
      "Predict2:< And is is , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 35s (- -6m 25s), Epoch: [1/10], Step: [1240/3335], Train Loss: 3.437377381324768, BLEU: 0.048115585278974804\n",
      "\n",
      "Input1:> 他 只是 压力 太大 了 婚礼 的 筹备 和 我家 家庭 家庭成员 成员 的 到来 让 他 喘 不过 过气\n",
      "\n",
      "Target1:= He had just been really stressed out by the wedding and by becoming a family with me  . \n",
      "Predict1:< And is the , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 是 一个 自由 的 组织 由来 来自 不同 同行 同行业 行业 不同 同城 城市 的 园丁 组成 大家 全都 是 志愿 志愿者 愿者 我们 所 做 的 一切 一切都是 义务 的\n",
      "\n",
      "Target2:= What we do , we are a UNK kind of group , where it is composed of gardeners from all walks of life , from all over the city , and it is completely volunteer , and everything \n",
      "Predict2:< We we we we we , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 40s (- -6m 19s), Epoch: [1/10], Step: [1260/3335], Train Loss: 3.4419359445571898, BLEU: 0.044044723873341914\n",
      "\n",
      "Input1:> 大概 是 和 言语 语词 词汇 相关 的 词汇 我 假设\n",
      "\n",
      "Target1:= Something to do with words , I assume  . \n",
      "Predict1:< And is , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 听起 起来 很 神奇 吧\n",
      "\n",
      "Target2:= Don t they sound incredible  ? \n",
      "Predict2:< And is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 46s (- -6m 14s), Epoch: [1/10], Step: [1280/3335], Train Loss: 3.4496235489845275, BLEU: 0.04762286760618236\n",
      "\n",
      "Input1:> 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定 女孩 上学 是 违法 的\n",
      "\n",
      "Target1:= You see , I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school  . \n",
      "Predict1:< I I I I , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 现在 要 关心 的 是 我们 如何 以 一种 可 持续 的 发展 方式 来 满足 70 亿 人 的 食物 医疗 教育 运输 及 通信 需求\n",
      "\n",
      "Target2:= What we have to look at is at how we feed , cure , educate , transport , communicate for seven billion people in a sustainable way  . \n",
      "Predict2:< We we we we we , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 51s (- -6m 8s), Epoch: [1/10], Step: [1300/3335], Train Loss: 3.3910022139549256, BLEU: 0.046351986797472364\n",
      "\n",
      "Input1:> 我 本 以为 只有 我们 意大利 意大利人 大利 利人 在 非洲 栽 了 跟头 但 当 我 知道 美国 国人 的 情况 英国 英国人 国人 的 情况 法国 法国人 国人 的 情况 当 看 了 他们 的 所作\n",
      "\n",
      "Target1:= I thought it was only us Italians blundering around Africa , but then I saw what the Americans were doing , what the English were doing , what the French were doing , and after seeing what they \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 正在 做 什么 呢 我们 正在 研究 这些 秃鹫 我们 在 它们 身上 安装 定位 定位器\n",
      "\n",
      "Target2:= So what is being done  ? Well , we are conducting research on these birds  . We are putting transmitters on them  . \n",
      "Predict2:< We we we we we we we we , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5m 57s (- -6m 3s), Epoch: [1/10], Step: [1320/3335], Train Loss: 3.345831871032715, BLEU: 0.04833887222026024\n",
      "\n",
      "Input1:> 是 的 他们 会 修 几乎 所有 照片 但 这 只是 工作 的 一小 一小部分 小部 部分\n",
      "\n",
      "Target1:= And yeah , they pretty much retouch all the photos , but that is only a small component of what is happening  . \n",
      "Predict1:< And is is , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 发现 家庭 家庭暴力 暴力 的 预兆 并 认真 真的 干预 减少 发生 的 可能 可能性 给 受害 受害者 提供 安全 的 出路\n",
      "\n",
      "Target2:= UNK the early signs of violence and UNK intervene , UNK it , show victims a safe way out  . \n",
      "Predict2:< And is , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 2s (- -7m 57s), Epoch: [1/10], Step: [1340/3335], Train Loss: 3.5066724896430967, BLEU: 0.04784445173422572\n",
      "\n",
      "Input1:> 秃鹫 是 我们 的 大自然 自然 垃圾 清道 清道夫 道夫\n",
      "\n",
      "Target1:= They are our natural garbage collectors  . \n",
      "Predict1:< And is is , , , , , ,\n",
      "\n",
      "Input2:> 他们 必须 须要 赶快 快逃 逃走 所以 我 开始 计划 帮助 他们 逃脱\n",
      "\n",
      "Target2:= They had to get out quickly , so I started planning how to help them escape  . \n",
      "Predict2:< And , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 8s (- -7m 52s), Epoch: [1/10], Step: [1360/3335], Train Loss: 3.632017958164215, BLEU: 0.048566166954762964\n",
      "\n",
      "Input1:> 我们 没有 自由 他们 说道\n",
      "\n",
      "Target1:= &quot; We have no freedom , &quot; they said  . \n",
      "Predict1:< We we we we  .  .  .\n",
      "\n",
      "Input2:> 他 的 脸颊 像 大 苹果 一样 UNK 的 小麦 色 的 金发 看起 看起来 起来 十分 讨人喜欢 喜欢\n",
      "\n",
      "Target2:= He had these big cheeks , these big apple cheeks and this UNK hair , and he seemed so sweet  . \n",
      "Predict2:< And , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 13s (- -7m 46s), Epoch: [1/10], Step: [1380/3335], Train Loss: 3.350728476047516, BLEU: 0.04834099712772081\n",
      "\n",
      "Input1:> 为什么 什么 因为 西红柿 熟 了 并且 光 在 西红柿 内部 反射 在 几万 万亿 亿分 之一 一秒 后 出来\n",
      "\n",
      "Target1:= Why is that  ? Because the tomato is actually ripe , and the light is bouncing around inside the tomato , and it comes out after several UNK of a second  . \n",
      "Predict1:< And is , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target2:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 19s (- -7m 41s), Epoch: [1/10], Step: [1400/3335], Train Loss: 3.4498838424682616, BLEU: 0.050006030387314875\n",
      "\n",
      "Input1:> 又 一次 我们 坐 的 巴士 被 拦下 了 一个 中国 警官 上 了 车\n",
      "\n",
      "Target1:= One time , our bus was stopped and boarded by a Chinese police officer  . \n",
      "Predict1:< And we we , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 狮子 晚上 走近 的 时候 看到 的 就是 这个 样子\n",
      "\n",
      "Target2:= And that is how it looks to lions when they come at night  . \n",
      "Predict2:< And is , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 24s (- -7m 35s), Epoch: [1/10], Step: [1420/3335], Train Loss: 3.4336562514305116, BLEU: 0.048291006749038766\n",
      "\n",
      "Input1:> 我 朋友 跟 我 在 一起\n",
      "\n",
      "Target1:= My friend got to come with me  . \n",
      "Predict1:< I I I I I  .  .\n",
      "\n",
      "Input2:> 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 29s (- -7m 30s), Epoch: [1/10], Step: [1440/3335], Train Loss: 3.435765016078949, BLEU: 0.05126404049561317\n",
      "\n",
      "Input1:> 因此 我 和 我 的 团队 洛杉矶 绿地 我们 聚在一起 一起 开始 种植 自己 的 食物 森林 果树 还有 整整 九种 蔬菜\n",
      "\n",
      "Target1:= So me and my group , L .A . Green UNK , we got together and we started planting my food forest , fruit trees , you know , the whole nine , vegetables  . \n",
      "Predict1:< So I I I , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 肯尼 肯尼亚 尼亚 全世界 世界 都 将 会 有 很大 的 损失 如果 失去 了 这些 奇妙 的 物种\n",
      "\n",
      "Target2:= Kenya , this world , will be much poorer without these wonderful species  . \n",
      "Predict2:< And is , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 35s (- -7m 24s), Epoch: [1/10], Step: [1460/3335], Train Loss: 3.3971356987953185, BLEU: 0.04992475781147184\n",
      "\n",
      "Input1:> 你 要 告诉 他们 创业 创业精神 精神 的 真谛\n",
      "\n",
      "Target1:= And then you have to tell them the truth about entrepreneurship  . \n",
      "Predict1:< You you you you you you you  .  .\n",
      "\n",
      "Input2:> 我们 能 进行 高频 高频率 频率 的 监测 以 获取 客观 数据\n",
      "\n",
      "Target2:= We can do high-frequency monitoring to get objective data  . \n",
      "Predict2:< We we we we we we we we the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 40s (- -7m 19s), Epoch: [1/10], Step: [1480/3335], Train Loss: 3.5412478923797606, BLEU: 0.04968884074941593\n",
      "\n",
      "Input1:> 这 为 我们 营造 了 一个 反省 和 思考 的 空间 也 提醒 我们 在 不断 断成 成长 改变 的 过程 中 什么 才 是 最为 为重 重要 的\n",
      "\n",
      "Target1:= It is about making space for reflection and contemplation , and remembering what really matters most to us as we grow and change  . \n",
      "Predict1:< And is is is , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 不能 披露 太多 我 离开 朝鲜 时 的 细节 但是 我 只能 说 那 是 在 饥荒 中 最 艰难 的 几年 我 被 送到 中国 和 远亲 一起 住\n",
      "\n",
      "Target2:= I can t reveal many details &#91; about &#93; how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives \n",
      "Predict2:< I I I I I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 46s (- -7m 13s), Epoch: [1/10], Step: [1500/3335], Train Loss: 3.4585770845413206, BLEU: 0.0504839141966294\n",
      "\n",
      "Input1:> 现在 超人 能 飞\n",
      "\n",
      "Target1:= So , Superman can fly  . \n",
      "Predict1:< And is is  .  .  .\n",
      "\n",
      "Input2:> 当地 有 一位 女士 指导 我们 清洗\n",
      "\n",
      "Target2:= We had an amazing local woman who guided us  . \n",
      "Predict2:< And is is , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 51s (- -7m 8s), Epoch: [1/10], Step: [1520/3335], Train Loss: 3.4089304924011232, BLEU: 0.07173073052766872\n",
      "\n",
      "Input1:> 实际 实际上 我 是 典型 的 家庭 家庭暴力 暴力 受害 受害者 也 是 典型 的 家庭 家庭暴力 暴力 幸存 幸存者\n",
      "\n",
      "Target1:= Because it turns out that I am actually a very typical domestic violence victim and a typical domestic violence survivor  . \n",
      "Predict1:< And I , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 能 想象 一个 400 000 人 的 社区 凝聚 了 多少 智力 和 热情 吗\n",
      "\n",
      "Target2:= Can you imagine a community of 400,000 people , the intelligence and the passion  ? \n",
      "Predict2:< You you you you you you you you you , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6m 57s (- -7m 2s), Epoch: [1/10], Step: [1540/3335], Train Loss: 3.5430948734283447, BLEU: 0.07128354679959926\n",
      "\n",
      "Input1:> 我们 种 的 西红柿 漂亮 极了 在意 意大利 大利 一个 西红柿 长 这么 大 在 赞比亚 比亚 能 长 这么 大\n",
      "\n",
      "Target1:= We had these magnificent tomatoes  . In Italy , a tomato would grow to this size  . In Zambia , to this size  . \n",
      "Predict1:< We we we we , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 有 一天 我 最糟 糟糕 的 恶梦 成真 了 我 被 中国 警察 抓住 并 带去 警察 警察局 接受 受审 审问\n",
      "\n",
      "Target2:= One day , my worst nightmare came true , when I was caught by the Chinese police and brought to the police station for interrogation  . \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 2s (- -8m 57s), Epoch: [1/10], Step: [1560/3335], Train Loss: 3.486210525035858, BLEU: 0.05338254801349204\n",
      "\n",
      "Input1:> 现在 我们 仍然 会 问 她 为什么 什么 不 逃走\n",
      "\n",
      "Target1:= And still we ask , why doesn t she just leave  ? \n",
      "Predict1:< And is we we we , , , ,  .\n",
      "\n",
      "Input2:> 怎么 会 有人 人愿 愿意 和 一个 一个打 她 的 男人 继续 住在一起 一起\n",
      "\n",
      "Target2:= Why would anyone stay with a man who beats her  ? \n",
      "Predict2:< And is is the , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 8s (- -8m 51s), Epoch: [1/10], Step: [1580/3335], Train Loss: 3.5187360525131224, BLEU: 0.05151716252440909\n",
      "\n",
      "Input1:> 你们 为什么 什么 不早 说 你 又 没 问 过\n",
      "\n",
      "Target1:= &quot; Why didn t you tell us  ? &quot; &quot; You never asked  . &quot; \n",
      "Predict1:< And you , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我 记得 他们 布满 红血 UNK 血丝 的 眼睛 里 充满 疲惫 其中 很多 人 已 在 井下 下工 工作 72 个 小时\n",
      "\n",
      "Target2:= I remember looking into their tired , bloodshot eyes , for many of them had been underground for 72 hours  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 13s (- -8m 46s), Epoch: [1/10], Step: [1600/3335], Train Loss: 3.5551347017288206, BLEU: 0.13393089696774602\n",
      "\n",
      "Input1:> 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 这 就是 我们 做 的 你 看到 的 就是 做 的 实验 路段 当然 这段 路 近几年 几年 不会 坏 我们 在实践中 实践 中 确认 了 这点\n",
      "\n",
      "Target2:= So that is what we did here  . You see where we were making the test road , and then of course this road will last several years without any damage  . That is what we know from \n",
      "Predict2:< And is is is is , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 19s (- -8m 40s), Epoch: [1/10], Step: [1620/3335], Train Loss: 3.5710527062416078, BLEU: 0.15922450360569035\n",
      "\n",
      "Input1:> 但是 如果 我们 不 改变 土壤 的 成分 我们 永远 不会 这样 做\n",
      "\n",
      "Target1:= but if we don t change the composition of the soil , we will never do this  . \n",
      "Predict1:< But we we we we we we we , , , ,\n",
      "\n",
      "Input2:> 但是 当 这些 都 没有 呈现 现在 孩子 们 眼前 如果 他们 并 没有 体会 到 食物 是 如何 影响 人 的 想法 和 身体 的 他们 只是 盲目 目的 吃 着 任何 摆在 他们 面前 的 食物\n",
      "\n",
      "Target2:= But when none of this is presented to them , if they are not shown how food affects the mind and the body , they blindly eat whatever the hell you put in front of them  . \n",
      "Predict2:< But the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 24s (- -8m 35s), Epoch: [1/10], Step: [1640/3335], Train Loss: 3.551608312129974, BLEU: 0.050181148647983786\n",
      "\n",
      "Input1:> 而 你 唯一 能够 守护 护住 的 东西 就是 这里 如果 我们 不得 不得不 不靠 卖 血 来 支付 你 的 学费 我们 会 毫不 毫不犹豫 犹豫\n",
      "\n",
      "Target1:= But the one thing that will always remain with you is what is here , and if we have to sell our blood to pay your school fees , we will  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 我 想 说 的 是 创业 创业精神 精神 是 当今 当今社会 社会 的 潮流\n",
      "\n",
      "Target2:= So what I am saying is that entrepreneurship is where it is at  . \n",
      "Predict2:< So I I , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 30s (- -8m 29s), Epoch: [1/10], Step: [1660/3335], Train Loss: 3.4482064127922056, BLEU: 0.10134931147761424\n",
      "\n",
      "Input1:> 现在 我 的 方法 已经 被 肯尼 肯尼亚 尼亚 的 所有 所有人 有人 使用 来 驱赶 其他 类似 似的 捕食 动物 像 鬣狗 美洲 美洲豹 等 它们 同样 被 用来 驱赶 大象 远离 离人 人类 的 农庄\n",
      "\n",
      "Target1:= And my idea is also being used now all over Kenya for scaring other predators like hyenas , leopards , and it is also being used to scare elephants away from people is farms  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 第一 第一个 一个 问题 是 你 是 怎么 成为 模特 的\n",
      "\n",
      "Target2:= So the first question is , how do you become a model  ? \n",
      "Predict2:< And is is , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 35s (- -8m 24s), Epoch: [1/10], Step: [1680/3335], Train Loss: 3.3928887128829954, BLEU: 0.13114106152793592\n",
      "\n",
      "Input1:> 我 得到 这些 是因为 因为 我 的 外表 而 不是 我 是 身份 有些 人为 为了 他们 的 外表 而 不是 身份 付出 了 代价\n",
      "\n",
      "Target1:= And I got these free things because of how I look , not who I am , and there are people paying a cost for how they look and not who they are  . \n",
      "Predict1:< I I I I , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 这 之后 不久 当 我 走过 一个 火车 火车站 车站 时 我 看到 一件 非常 痛心 的 事情 让 我 难以 难以忘怀 忘怀\n",
      "\n",
      "Target2:= Soon after , when I was walking past a train station , I saw something terrible that I can t erase from my memory  . \n",
      "Predict2:< And I I , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 41s (- -8m 18s), Epoch: [1/10], Step: [1700/3335], Train Loss: 3.5635721921920775, BLEU: 0.10034134952526007\n",
      "\n",
      "Input1:> 但 还有 一些 朝鲜 朝鲜人 就 没 那么 幸运 了\n",
      "\n",
      "Target1:= These North Koreans were not so lucky  . \n",
      "Predict1:< But is is the the the  .  .  .\n",
      "\n",
      "Input2:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target2:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict2:< And you , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 46s (- -8m 13s), Epoch: [1/10], Step: [1720/3335], Train Loss: 3.454908215999603, BLEU: 0.05199175671251661\n",
      "\n",
      "Input1:> 否则 小女 小女孩 女孩 的 脸 就 不会 看上 看上去 上去 像 她 的 脸 了 那 多 悲剧 啊 就 跟 毁 了 照片 一样\n",
      "\n",
      "Target1:= Otherwise , that little girl isn t going to look like that little girl anymore , and surely that is as tragic as having the photo damaged  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 不仅 因为 它 听起 起来 很 不错\n",
      "\n",
      "Target2:= Just because it sounds so good  . \n",
      "Predict2:< And is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 52s (- -8m 7s), Epoch: [1/10], Step: [1740/3335], Train Loss: 3.536852943897247, BLEU: 0.10504496230272665\n",
      "\n",
      "Input1:> 你 知道 的 这样 大概 会 可行\n",
      "\n",
      "Target1:= And , you know , it could have worked  . \n",
      "Predict1:< You you you you you you  .  .\n",
      "\n",
      "Input2:> 绿地 组织 又 开始 安置 也许 是 20 个 花园\n",
      "\n",
      "Target2:= So Green UNK has gone on to plant maybe 20 gardens  . \n",
      "Predict2:< And is , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7m 57s (- -8m 2s), Epoch: [1/10], Step: [1760/3335], Train Loss: 3.4182421565055847, BLEU: 0.14206341316187454\n",
      "\n",
      "Input1:> 我们 马赛 族 的 人 相信 我们 是 带 着 我们 的 动物 和 家园 从 天堂 而来 然后 后放 放牧 生活 这 就是 我们 为什么 什么 如此 重视 它们\n",
      "\n",
      "Target1:= My community , the Maasai , we believe that we came from heaven with all our animals and all the land for herding them , and that is why we value them so much  . \n",
      "Predict1:< We we we we we , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 社区 会议 存在 一个 问题\n",
      "\n",
      "Target2:= There is a problem with community meetings  . \n",
      "Predict2:< It is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 3s (- -9m 56s), Epoch: [1/10], Step: [1780/3335], Train Loss: 3.431700146198273, BLEU: 0.14617124962480973\n",
      "\n",
      "Input1:> 我 想 这 就是 内罗毕 公园 的 狮子 UNK 子会 那么 少 的 原因\n",
      "\n",
      "Target1:= And I think this is why the Nairobi National Park lions are few  . \n",
      "Predict1:< I I I , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 我们 第一 第一个 一个 项目 给 了 我 写 第一 第一本 一本 本书 的 灵感 书名 名叫 UNK 比西 西河 的 涟漪 在 这个 项目 目中 我们 意大利 意大利人 大利 利人 决定 教 赞比亚 比亚 人种 种粮 粮食\n",
      "\n",
      "Target2:= Our first project , the one that has inspired my first book , &quot; UNK from the UNK , &quot; was a project where we Italians decided to teach Zambian people how to grow food  . \n",
      "Predict2:< We we we , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 8s (- -9m 51s), Epoch: [1/10], Step: [1800/3335], Train Loss: 3.5069312930107115, BLEU: 0.07762134733605075\n",
      "\n",
      "Input1:> 我 真心 心地 感谢 他 并 问道 你 为什么 什么 要 帮助 我\n",
      "\n",
      "Target1:= I thanked him with all my heart , and I asked him , &quot; Why are you helping me  ? &quot; \n",
      "Predict1:< I I I I I , , , , I I ,  .  .\n",
      "\n",
      "Input2:> 有着 如此 肥沃 的 山谷 当地 当地人 人居 居然 不 耕种 我们 对此 感到 十分 惊奇\n",
      "\n",
      "Target2:= And we were amazed that the local people , in such a fertile valley , would not have any agriculture  . \n",
      "Predict2:< And , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 14s (- -9m 45s), Epoch: [1/10], Step: [1820/3335], Train Loss: 3.359319305419922, BLEU: 0.16263792816539485\n",
      "\n",
      "Input1:> 他 经常 尖叫\n",
      "\n",
      "Target1:= He screamed a lot  . \n",
      "Predict1:< And is  .  .  .\n",
      "\n",
      "Input2:> 众议 众议院 议院 不想 让 华盛 华盛顿 沉醉 沉醉于 权力\n",
      "\n",
      "Target2:= The House of Representatives didn t want Washington to get drunk on power  . \n",
      "Predict2:< And , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 19s (- -9m 40s), Epoch: [1/10], Step: [1840/3335], Train Loss: 3.634914994239807, BLEU: 0.07604584879511211\n",
      "\n",
      "Input1:> 他们 不断 地 发现 UNK 照片 相册 相机 和 SD 储存 UNK\n",
      "\n",
      "Target1:= They d been finding photos and photo albums and cameras and SD cards  . \n",
      "Predict1:< They the , , , , , , , , ,\n",
      "\n",
      "Input2:> 而且 和 芭蕾 一样 这 也 需要 经过 特殊 地 训练\n",
      "\n",
      "Target2:= And like ballet , it takes an extraordinary level of training  . \n",
      "Predict2:< And is is , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 25s (- -9m 34s), Epoch: [1/10], Step: [1860/3335], Train Loss: 3.592742908000946, BLEU: 0.10812336855910253\n",
      "\n",
      "Input1:> 这 就是 为什么 什么 在 隐姓埋名 埋名 10 年后 我 决定 冒险 去 韩国 就 这样 我 又 一次 开始 了 新 的 生活\n",
      "\n",
      "Target1:= That is why , after 10 years of hiding my identity , I decided to risk going to South Korea , and I started a new life yet again  . \n",
      "Predict1:< And is is is , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 即便 即便如此 如此 你 还是 不想 继续 吗\n",
      "\n",
      "Target2:= So do you still not want to continue  ? &quot; \n",
      "Predict2:< And you you you you ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 30s (- -9m 29s), Epoch: [1/10], Step: [1880/3335], Train Loss: 3.469939112663269, BLEU: 0.16909624049405778\n",
      "\n",
      "Input1:> 我 猜 他们 期待 的 答案 是 如果 你 再 瘦 一点 一点儿 点儿 你 的 头发 再有 光泽 一些 你 就 会 很 开心 很 难以 难以置信 置信\n",
      "\n",
      "Target1:= And I think the answer that they are looking for is , &quot; If you are a little bit skinnier and you have UNK hair , you will be so happy and fabulous  . &quot; \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 那么 这些 以 语音 为 基础 的 测试 他们 会 怎样 与 专业 的 临床 测试 展开 较量 呢 其实 他们 都 是 无 创 技术\n",
      "\n",
      "Target2:= So these UNK tests , how do they stack up against expert clinical tests  ? We ll , they are both non-invasive  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 36s (- -9m 23s), Epoch: [1/10], Step: [1900/3335], Train Loss: 3.53578040599823, BLEU: 0.14667009364318084\n",
      "\n",
      "Input1:> 这 真是 难以 难以置信 置信 我们 跟 赞比亚 比亚 人 说 看 耕种 多 容易 啊\n",
      "\n",
      "Target1:= And we could not believe , and we were telling the UNK , &quot; Look how easy agriculture is  . &quot; \n",
      "Predict1:< And is is is is , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 怎么 能 我 答道 我 做 了 一件 很困 困难 的 事\n",
      "\n",
      "Target2:= How can you do —  ? &quot; And I said , &quot; I do something very , very , very difficult  . \n",
      "Predict2:< I I I you you you you you you you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 41s (- -9m 18s), Epoch: [1/10], Step: [1920/3335], Train Loss: 3.6079674482345583, BLEU: 0.16827149846623754\n",
      "\n",
      "Input1:> 太 不 简单 了\n",
      "\n",
      "Target1:= Extraordinary  . \n",
      "Predict1:< And is is  .  .  .\n",
      "\n",
      "Input2:> 奇怪 奇怪的是 那些 地方 我 都 去过\n",
      "\n",
      "Target2:= Oddly , I had been to many of these places before  . \n",
      "Predict2:< I I I I ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 47s (- -9m 12s), Epoch: [1/10], Step: [1940/3335], Train Loss: 3.381006920337677, BLEU: 0.0782532997218955\n",
      "\n",
      "Input1:> 听起 起来 很 神奇 吧\n",
      "\n",
      "Target1:= Don t they sound incredible  ? \n",
      "Predict1:< And is is  .  .\n",
      "\n",
      "Input2:> 回到 UNK 我 忍不住 不住 想 哭 但是 身旁 的 废 奴 主义 同伴 迅速 地拉 拉住 了 我 他 说 Lisa 别哭 别 在 这里 哭\n",
      "\n",
      "Target2:= Back in the kilns , I wanted to cry , but the UNK next to me quickly grabbed me and he said , &quot; Lisa , don t do that  . Just don t do that here  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 52s (- -9m 7s), Epoch: [1/10], Step: [1960/3335], Train Loss: 3.3917803525924684, BLEU: 0.07831570408092474\n",
      "\n",
      "Input1:> 我们 的 工作 地点 不在 办公 办公室 公室\n",
      "\n",
      "Target1:= We don t work from offices  . \n",
      "Predict1:< We we we we  .  .  .\n",
      "\n",
      "Input2:> 没人 能 独自 自创 创立 一个 企业 没人\n",
      "\n",
      "Target2:= Nobody started a company alone  . No one  . \n",
      "Predict2:< And is is ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8m 58s (- -9m 1s), Epoch: [1/10], Step: [1980/3335], Train Loss: 3.447292077541351, BLEU: 0.1688349354385163\n",
      "\n",
      "Input1:> 我 想 跟 一名 圣战 分子 生活 一天 他 穿着 圣 站 服 去 打败 共产 共产主义 主义 就 跟 流行 流行歌 歌手 的 UNK 珠宝 一样 并且 使用 宗教 威胁 以及 政治 政治腐败 腐败 使 自己 变得\n",
      "\n",
      "Target1:= I want to spend a day in the life of a jihadi gangster who wears his jihad against the communists like UNK bling and uses armed religious intimidation and political corruption to make himself rich  . \n",
      "Predict1:< I I I I , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 因此 我 觉得 太好了 这 是 我 的 责任 我 必须 维护 它 所以 我 想 做 什么 就 可以 做 什么 了\n",
      "\n",
      "Target2:= So I am like , &quot; Cool  . I can do whatever the hell I want , since it is my responsibility and I gotta maintain it  . &quot; \n",
      "Predict2:< So I I I I I I I I I I I I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 3s (- -10m 56s), Epoch: [1/10], Step: [2000/3335], Train Loss: 3.407597279548645, BLEU: 0.17400098029912975\n",
      "\n",
      "Input1:> 还有 一次 当 我 在 洛杉矶 洛杉矶市 市中心 中心 的 一个 流浪 流浪汉 之家 家安 安置 了 一个 花园\n",
      "\n",
      "Target1:= There is another time when I put a garden in this homeless shelter in downtown Los Angeles  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 真的 被 吓坏 了 这些 学生 生发 发达 的 大脑 还有 他们 所 读 的 巨著 及 他们 使用 的 又 长 又 UNK 的 词语 通通 通都 吓到 我 了\n",
      "\n",
      "Target2:= Really scared  . Scared of these students with their big brains and their big books and their big , unfamiliar words  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 9s (- -10m 50s), Epoch: [1/10], Step: [2020/3335], Train Loss: 3.631361413002014, BLEU: 0.17187810872603487\n",
      "\n",
      "Input1:> 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人 挤 在 一个 小 客厅 厅里\n",
      "\n",
      "Target1:= The school was in a house , more than 100 of us packed in one small living room  . \n",
      "Predict1:< And is , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 无需 去 设计 一系 一系列 系列 新 的 医院 来 完成 测试\n",
      "\n",
      "Target2:= You don t have to design a whole new set of hospitals to do it  . \n",
      "Predict2:< You you you you you you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 14s (- -10m 45s), Epoch: [1/10], Step: [2040/3335], Train Loss: 3.5004743933677673, BLEU: 0.1707051476973701\n",
      "\n",
      "Input1:> 众议 众议院 议院 不想 让 华盛 华盛顿 沉醉 沉醉于 权力\n",
      "\n",
      "Target1:= The House of Representatives didn t want Washington to get drunk on power  . \n",
      "Predict1:< And is , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 这些 女孩 是 幸运 的\n",
      "\n",
      "Target2:= These girls were so lucky  . \n",
      "Predict2:< And is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 20s (- -10m 39s), Epoch: [1/10], Step: [2060/3335], Train Loss: 3.62009516954422, BLEU: 0.14941461562866484\n",
      "\n",
      "Input1:> 我 是 在 帮助 朝鲜 的 人们\n",
      "\n",
      "Target1:= &quot; I am helping the North Korean people  . &quot; \n",
      "Predict1:< I I I  .  .  .  .\n",
      "\n",
      "Input2:> 但 不幸 的 是 我 得 告诉 你们 在 2007 年时 一名 纽约 纽约大学 大学 的 博士 在 启发 之下 统计 了 所有 在职 模特 的 人数 在 UNK 名 正式 受雇 模特 中 只有 27 名 也\n",
      "\n",
      "Target2:= But unfortunately I have to inform you that in 2007 , a very inspired NYU Ph .D . student counted all the models on the runway , every single one that was hired , and of the UNK models that \n",
      "Predict2:< But I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 25s (- -10m 34s), Epoch: [1/10], Step: [2080/3335], Train Loss: 3.5110252499580383, BLEU: 0.1715098484205717\n",
      "\n",
      "Input1:> 当个 拿 铲子 的 流氓 怎么 怎么样\n",
      "\n",
      "Target1:= Get gangsta with your shovel , okay  ? \n",
      "Predict1:< And is is ,  .  .  .\n",
      "\n",
      "Input2:> 我 所 做 的 一切 不论 不论是 生活 上 还是 工作 上 我 的 整个 个人 人生 深深 深受 受到 年轻 时 在 非洲 工作 的 那 七年 的 影响\n",
      "\n",
      "Target2:= Everything I do , and everything I do professionally -- my life -- has been shaped by seven years of work as a young man in Africa  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 31s (- -10m 29s), Epoch: [1/10], Step: [2100/3335], Train Loss: 3.4829041957855225, BLEU: 0.15544742597390698\n",
      "\n",
      "Input1:> 就 这样 我们 一路 来到 了 老挝 但是 我 却 要 几乎 花光 全部 积蓄 来 贿赂 老挝 的 边境 警察\n",
      "\n",
      "Target1:= We made it all the way to the border of Laos , but I had to spend almost all my money to bribe the border guards in Laos  . \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 然而 世上 上有 630 万 的 人 患有 这种 疾病 而且 他们 还 得 忍受 无法 法治 治愈 的 虚弱 震颤 僵化 等 其他 伴随 而来 的 症状 的 折磨 所以 我们 需要 客观 的 工具 在 病重\n",
      "\n",
      "Target2:= However , there are 6 .3 million people worldwide who have the disease , and they have to live with incurable weakness , tremor , rigidity and the other symptoms that go along with the disease , so what \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 36s (- -10m 23s), Epoch: [1/10], Step: [2120/3335], Train Loss: 3.462900674343109, BLEU: 0.16798427954247955\n",
      "\n",
      "Input1:> 5 月 13 日 我 来到 日本 大船 渡 县\n",
      "\n",
      "Target1:= On May the 13th , I made my way to the town of UNK  . \n",
      "Predict1:< And I , , , , , , ,\n",
      "\n",
      "Input2:> 否则 照片 里 她 和 她 先生 的 脸部 就 无法 完全 修复 了 我们 也 因此 得以 把 所有 部分 整合 合到 一张 好 的 相片 里 然后 重新 新制 制作 这 张 照片\n",
      "\n",
      "Target2:= Areas like her husband and her face , which otherwise would have been completely impossible to fix , we could just put them together in one good photo , and remake the whole photo  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 42s (- -10m 18s), Epoch: [1/10], Step: [2140/3335], Train Loss: 3.6667950630187987, BLEU: 0.1678210269486148\n",
      "\n",
      "Input1:> 如果 这些 人 要为 自己 争取 取自 自由 奴隶 奴隶主 就 烧掉 他们 所有 的 房子\n",
      "\n",
      "Target1:= When these villagers claimed their freedom , the slaveholders burned down all of their houses  . \n",
      "Predict1:< And , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但 它们 会 损坏 你 的 挡风 挡风玻璃 玻璃 你 肯定 不 高兴\n",
      "\n",
      "Target2:= But they can damage your windshield , so you are not happy with that  . \n",
      "Predict2:< But you you you you you you you you you you you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 47s (- -10m 12s), Epoch: [1/10], Step: [2160/3335], Train Loss: 3.7516698479652404, BLEU: 0.15118899540785852\n",
      "\n",
      "Input1:> 他 无条件 条件 UNK UNK 保留 保留地 分享 他 的 那份 爱\n",
      "\n",
      "Target1:= He shares it unconditionally and he shares it regardless  . \n",
      "Predict1:< And the , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我 根本 不知 知道 身为 一个 朝鲜 的 难民 我 未来 的 生活 活会 怎样 但是 我 很快 快意 意识 到 那 生活 不但 充满 了 艰辛 甚至 无比 危险 因为 朝鲜 难民 在 中国 被 视为 非法\n",
      "\n",
      "Target2:= I had no idea what life was going to be like as a North Korean refugee , but I soon learned it is not only extremely difficult , it is also very dangerous , since North Korean refugees \n",
      "Predict2:< I I I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 53s (- -10m 7s), Epoch: [1/10], Step: [2180/3335], Train Loss: 3.5067422866821287, BLEU: 0.1726786948088906\n",
      "\n",
      "Input1:> 我们 怎么 才能 能干 干预 这些 隐藏 于 舆论 之外 但 却 堪称 普遍 的 恶事\n",
      "\n",
      "Target1:= How can we affect something so insidious , yet so pervasive  ? \n",
      "Predict1:< We we we we we , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 西方 西方人 方人 是 帝国 帝国主义 帝国主义者 国主 主义 主义者 殖民 殖民主义 殖民主义者 民主 主义 主义者 以及 传教 传教士 教士 我们 跟 人们 打交道 交道 的 方式 只有 两种 一 是 居高临下 UNK 二 是 家长 做派\n",
      "\n",
      "Target2:= We Western people are imperialist , UNK missionaries , and there are only two ways we deal with people : We either patronize them , or we are paternalistic  . \n",
      "Predict2:< We we we we we we , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9m 58s (- -10m 1s), Epoch: [1/10], Step: [2200/3335], Train Loss: 3.5842238426208497, BLEU: 0.1769022995081903\n",
      "\n",
      "Input1:> 但是 在 今天 一整 整个 家庭 被 世代 奴役 的 代价 可能 就 只有 18 美金 的 欠债\n",
      "\n",
      "Target1:= Yet today , entire families can be enslaved for generations over a debt as small as $ 18  . \n",
      "Predict1:< But the the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 她 的 两个 个儿 儿子 虽然 已经 不小 了 还 在 在 两个 不同 的 学校 上学\n",
      "\n",
      "Target2:= Her two little boys , not so little anymore , but her two boys were both at school , separate schools  . \n",
      "Predict2:< And the the , , , , , , , , , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 4s (- -11m 56s), Epoch: [1/10], Step: [2220/3335], Train Loss: 3.6678966164588926, BLEU: 0.16949392856965237\n",
      "\n",
      "Input1:> 当 我们 的 船 靠近 的 时候 他 在 发抖 他 怕 我们 的 船 会 撞 上上 他 的 独木 独木舟 木舟\n",
      "\n",
      "Target1:= He was trembling when our boat approached , frightened it would run over his tiny canoe  . \n",
      "Predict1:< And we we we , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定 女孩 上学 是 违法 的\n",
      "\n",
      "Target2:= You see , I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school  . \n",
      "Predict2:< I I I , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 9s (- -11m 50s), Epoch: [1/10], Step: [2240/3335], Train Loss: 3.4331801891326905, BLEU: 0.174206134182187\n",
      "\n",
      "Input1:> 我 也 想 告诉 各位 我 有幸 能够 在 10 秒 内 改变 我 在 你们 心中 的 形象\n",
      "\n",
      "Target1:= I d also note that I am quite privileged to be able to transform what you think of me in a very brief 10 seconds  . \n",
      "Predict1:< I I I I I , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 菜园 在 街上 的 原因\n",
      "\n",
      "Target2:= This is on the street for a reason  . \n",
      "Predict2:< This is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 15s (- -11m 45s), Epoch: [1/10], Step: [2260/3335], Train Loss: 3.6975980758666993, BLEU: 0.1733755790608207\n",
      "\n",
      "Input1:> 在 过去 的 28 年 里 我 一直 在 拍摄 本土 文化 的 纪录 纪录片 走访 了 6 大洲 70 多个 国家 而 在 2009 年 我 非常 荣幸 地 能 以 独立 参展 UNK 展商 的 身份\n",
      "\n",
      "Target1:= For the last 28 years , I ve been documenting indigenous cultures in more than 70 countries on six continents , and in 2009 I had the great honor of being the sole UNK at the Vancouver Peace \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他们 捕杀 了 我们 的 家畜\n",
      "\n",
      "Target2:= They kill our livestock  . \n",
      "Predict2:< They are are to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 20s (- -11m 39s), Epoch: [1/10], Step: [2280/3335], Train Loss: 3.4328919887542724, BLEU: 0.17202478316598271\n",
      "\n",
      "Input1:> 我们 很 害怕 但是 我们 仍然 渴望 望去 学校\n",
      "\n",
      "Target1:= We were scared , but still , school was where we wanted to be  . \n",
      "Predict1:< We we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 所以 他们 决定 杀 了 这些 狮子\n",
      "\n",
      "Target2:= So they kill the lions  . \n",
      "Predict2:< So , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 26s (- -11m 34s), Epoch: [1/10], Step: [2300/3335], Train Loss: 3.707374799251556, BLEU: 0.17548393624467687\n",
      "\n",
      "Input1:> 而 这 张 由 UNK Brown 制作 的 示例 幻灯 幻灯片 效果 就 好多 了 它 向 我们 说明 骨 小 梁 的 特殊 结构 超级 强大 甚至 连 埃菲尔 埃菲尔铁塔 菲尔 铁塔 的 独特 特设 设计 都\n",
      "\n",
      "Target1:= Instead , this example slide by UNK Brown is much more effective  . It is showing that the special structure of UNK are so strong that they actually inspired the unique design of the Eiffel Tower  . \n",
      "Predict1:< And is is is the , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 左边 你 看到 了 我们 的 飞秒 成像 相机\n",
      "\n",
      "Target2:= On the left , you see our UNK  . \n",
      "Predict2:< And , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 31s (- -11m 28s), Epoch: [1/10], Step: [2320/3335], Train Loss: 3.6221840262413023, BLEU: 0.172291476331426\n",
      "\n",
      "Input1:> 在 韩国 国定 定居 比 我 想象 的 更加 困难 并且 充满 了 挑战\n",
      "\n",
      "Target1:= Settling down in South Korea was a lot more challenging than I had expected  . \n",
      "Predict1:< And , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我心 心想 搞 什么 来 真的\n",
      "\n",
      "Target2:= And I am like , &quot; Come on , really  ? \n",
      "Predict2:< And is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 37s (- -11m 23s), Epoch: [1/10], Step: [2340/3335], Train Loss: 3.5756814002990724, BLEU: 0.17586772797105962\n",
      "\n",
      "Input1:> 在 死 之前 我 想 跨过 国际 UNK 日期 变更 线\n",
      "\n",
      "Target1:= &quot; Before I die , I want to straddle the International Date Line  . &quot; \n",
      "Predict1:< And I I , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target2:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict2:< We we we we we , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 42s (- -11m 17s), Epoch: [1/10], Step: [2360/3335], Train Loss: 3.5578932881355287, BLEU: 0.17464946938487896\n",
      "\n",
      "Input1:> 他们 认为 是 个 称号 是 过去 用 过 并经 经过 考验 的\n",
      "\n",
      "Target1:= They thought it was tried and tested  . \n",
      "Predict1:< They they the the , , , , ,  .  .\n",
      "\n",
      "Input2:> 所以 这样 我们 不仅 知道 相机 到 门 的 距离 同时 知道 到 隐藏 物品 的 距离 但 我们 不知 知道 哪个 点 对应 哪个 距离\n",
      "\n",
      "Target2:= And this way , we know the distances , of course to the door , but also to the hidden objects , but we don t know which point corresponds to which distance  . \n",
      "Predict2:< So we we we we we we we , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 48s (- -11m 12s), Epoch: [1/10], Step: [2380/3335], Train Loss: 3.5951579213142395, BLEU: 0.17398999398192305\n",
      "\n",
      "Input1:> 这 正是 我 想要 的\n",
      "\n",
      "Target1:= That is the whole idea  . \n",
      "Predict1:< And is is I  .  .  .\n",
      "\n",
      "Input2:> 彼得 德鲁 德鲁克 史 上 最 伟大 的 管理 咨询 咨询师 之一 几年 年前 前去 去世 了 享年 96 岁\n",
      "\n",
      "Target2:= Peter UNK , one of the greatest management consultants in history , died age 96 , a few years ago  . \n",
      "Predict2:< And , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 53s (- -11m 6s), Epoch: [1/10], Step: [2400/3335], Train Loss: 3.59920734167099, BLEU: 0.1804714065616217\n",
      "\n",
      "Input1:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target1:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict1:< And the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 你 走出 这个 房间 的 时候 你们 就是 秃鹫 达人 了 不但 但要 告诉 你 的 家人 人们 还要 告诉 你们 的 邻居 邻居们 有关 秃鹫 的 故事\n",
      "\n",
      "Target2:= When you walk out of this room , you will be informed about vultures , but speak to your families , to your children , to your neighbors about vultures  . \n",
      "Predict2:< And you you you you you you , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10m 59s (- -11m 1s), Epoch: [1/10], Step: [2420/3335], Train Loss: 3.635907506942749, BLEU: 0.18045949314510804\n",
      "\n",
      "Input1:> 康 纳 第一 第一次 一次 打 我 是 在 我们 婚礼 的 五天 前\n",
      "\n",
      "Target1:= UNK first physically attacked me five days before our wedding  . \n",
      "Predict1:< And I , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 准备 大学 入学 入学考试 考试\n",
      "\n",
      "Target2:= I started studying for the university entrance exam  . \n",
      "Predict2:< And is ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 4s (- -12m 55s), Epoch: [1/10], Step: [2440/3335], Train Loss: 3.3846579909324648, BLEU: 0.17922370625211947\n",
      "\n",
      "Input1:> 所以 我们 可以 建立 一个 社区 在 这个 社区 区里 我们 有 来自 自小 小型 小型企业 企业 背景 的 辅助 商 在 咖啡 咖啡馆 中 酒吧 里 为 你 出谋划策 谋划 划策 我们 能 为 你 提供 这位\n",
      "\n",
      "Target1:= So we can create the community where we have facilitators who come from a small business background sitting in cafes , in bars , and your dedicated buddies who will do to you , what somebody did for \n",
      "Predict1:< So , we we we , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 而是 由 一群 专业 专业人士 人士 制出 的 图象 发型 发型师 化装 UNK 摄影 摄影师 造型 UNK 以及 他们 的 助理 还有 前期 制作 后期 后期制作 制作 他们 制造 造出 了 这些 图象 而 不是 我\n",
      "\n",
      "Target2:= They are constructions , and they are constructions by a group of professionals , by UNK and makeup artists and photographers and stylists and all of their assistants and UNK and post-production , and they build this  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 10s (- -12m 50s), Epoch: [1/10], Step: [2460/3335], Train Loss: 3.5505685448646545, BLEU: 0.18070830474741334\n",
      "\n",
      "Input1:> 如果 孩子 们 种植 甘蓝 孩子 们 就 会 吃 甘蓝\n",
      "\n",
      "Target1:= If kids grow kale , kids eat kale  . \n",
      "Predict1:< And , you , , , , ,  .  .\n",
      "\n",
      "Input2:> 这个 个人 还 得 去 UNK 条约 和 接见 见外 UNK\n",
      "\n",
      "Target2:= This guy has to go and sign treaties and meet foreign UNK  . \n",
      "Predict2:< And is is the , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 15s (- -12m 44s), Epoch: [1/10], Step: [2480/3335], Train Loss: 3.6669039964675902, BLEU: 0.17945998009304248\n",
      "\n",
      "Input1:> 你 能 想象 一个 400 000 人 的 社区 凝聚 了 多少 智力 和 热情 吗\n",
      "\n",
      "Target1:= Can you imagine a community of 400,000 people , the intelligence and the passion  ? \n",
      "Predict1:< You you you , , , , , , , ,\n",
      "\n",
      "Input2:> 太 不 简单 了\n",
      "\n",
      "Target2:= Extraordinary  . \n",
      "Predict2:< And is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 21s (- -12m 39s), Epoch: [1/10], Step: [2500/3335], Train Loss: 3.490741515159607, BLEU: 0.17864864387810578\n",
      "\n",
      "Input1:> 我们 没有 任何 基础 基础设施 设施 的 建设\n",
      "\n",
      "Target1:= We have zero infrastructure  . \n",
      "Predict1:< We we we to to  .  .  .\n",
      "\n",
      "Input2:> 我 的 故事 源 起 于 有 一次 我 获 邀 到 纽约 纽约市 演讲 我 的 妻子 拍 了 张 照片 我 抱 着 我 的 女儿 那天 是 她 的 一岁 生日 我们 站 在 第\n",
      "\n",
      "Target2:= My story begins when I was in New York City for a speaking engagement , and my wife took this picture of me holding my daughter on her first birthday  . We are on the corner of UNK \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 26s (- -12m 33s), Epoch: [1/10], Step: [2520/3335], Train Loss: 3.428479218482971, BLEU: 0.17864002590840944\n",
      "\n",
      "Input1:> 我 最后 爬出 了 矿井 得以 回家 而 这些 被 奴役 的 矿工 可能 永远 没有 回家 的 那 一天\n",
      "\n",
      "Target1:= I got to climb out of that hole , and I got to go home , but they likely never will , because they are trapped in slavery  . \n",
      "Predict1:< I I , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 住 在 这栋 房子 附近 我 就 想 如何 让 它 在 这 片 社区 区里 成为 一个 更好 的 地方 也 思考 了 另 一件 事 这件 事 彻底 彻底改变 改变 了 我 的 人生\n",
      "\n",
      "Target2:= I live near this house , and I thought about how I could make it a nicer space for my neighborhood , and I also thought about something that changed my life forever  . \n",
      "Predict2:< I I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 32s (- -12m 28s), Epoch: [1/10], Step: [2540/3335], Train Loss: 3.534936797618866, BLEU: 0.17331474994926602\n",
      "\n",
      "Input1:> 这个 本来 被 遗忘 的 建筑 变成 了 一个 极具 建设 建设性 的 地方 这些 人 的 希望 和 梦想 让 我 放声 大笑 也 黯然 落泪 也 曾 在 我 经历 困境 的 时候 给 我 安慰\n",
      "\n",
      "Target1:= So this neglected space became a constructive one , and people is hopes and dreams made me laugh out loud , tear up , and they consoled me during my own tough times  . \n",
      "Predict1:< And is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 回到 UNK 我 忍不住 不住 想 哭 但是 身旁 的 废 奴 主义 同伴 迅速 地拉 拉住 了 我 他 说 Lisa 别哭 别 在 这里 哭\n",
      "\n",
      "Target2:= Back in the kilns , I wanted to cry , but the UNK next to me quickly grabbed me and he said , &quot; Lisa , don t do that  . Just don t do that here  . \n",
      "Predict2:< I I I I , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 37s (- -12m 22s), Epoch: [1/10], Step: [2560/3335], Train Loss: 3.5079743385314943, BLEU: 0.1857111694849317\n",
      "\n",
      "Input1:> 还 不算 毁坏 得 太严 严重 但是 被 水浸 浸没 的 地方 导致 照片 片中 中小 小女 小女孩 女孩 的 脸 UNK 了 我们 不得 不得不 精心 修复 做到 十分 精确\n",
      "\n",
      "Target1:= Not terribly damaged , but where the water had caused that UNK on the girl is face had to be repaired with such accuracy and delicacy  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 没人 能 独自 自创 创立 一个 企业 没人\n",
      "\n",
      "Target2:= Nobody started a company alone  . No one  . \n",
      "Predict2:< And is is a  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 43s (- -12m 17s), Epoch: [1/10], Step: [2580/3335], Train Loss: 3.4859880685806273, BLEU: 0.17787944214769752\n",
      "\n",
      "Input1:> 我 在 加纳 地下 50 米 深 的 一个 非法 矿井 里\n",
      "\n",
      "Target1:= I am 150 feet down an illegal mine shaft in Ghana  . \n",
      "Predict1:< I I , , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 我 想 和 大家 聊聊 一种 非常 不 寻常 的 动物\n",
      "\n",
      "Target2:= I would like to talk to you about a very special group of animals  . \n",
      "Predict2:< I I I to , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 48s (- -12m 11s), Epoch: [1/10], Step: [2600/3335], Train Loss: 3.726189911365509, BLEU: 0.17867594884985152\n",
      "\n",
      "Input1:> 当时 我 最 不想 做 的 就是 离开 纽约 离开 我 热爱 的 工作 但是 为了 自己 灵魂 伴侣 我 想 我 需要 做出 牺牲 所以 我 同意 了 辞掉 了 工作 跟 康 纳 一起 离开 了\n",
      "\n",
      "Target1:= Now , the last thing I wanted to do was leave New York , and my dream job , but I thought you made sacrifices for your UNK , so I agreed , and I quit my job \n",
      "Predict1:< I I I I I I I I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 并不知道 不知 知道 他 是 在 虐待 我\n",
      "\n",
      "Target2:= I didn t know he was abusing me  . \n",
      "Predict2:< I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 54s (- -12m 6s), Epoch: [1/10], Step: [2620/3335], Train Loss: 3.556606376171112, BLEU: 0.17736969510227552\n",
      "\n",
      "Input1:> 所以 1860 年 他们 认为 这种 肮脏 的 科技 将 扼杀 纽约 的 生命\n",
      "\n",
      "Target1:= So 1860 , they are seeing this dirty technology that is going to choke the life out of New York  . \n",
      "Predict1:< So , , , , , , , the the the\n",
      "\n",
      "Input2:> 当 这样 一张 照片 重回 它 的 主人 人身 身边 它 将 带来 巨大 的 不同 为 失主 的 生活 带来 很大 的 不一 一样\n",
      "\n",
      "Target2:= When a photo like this can be returned to someone like this , it makes a huge difference in the lives of the person receiving it  . \n",
      "Predict2:< And is is the , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 11m 59s (- -12m 0s), Epoch: [1/10], Step: [2640/3335], Train Loss: 3.5396712183952332, BLEU: 0.18397326712563236\n",
      "\n",
      "Input1:> 但是 当 我 回到 阿富汗 看见 我 的 学生 以及 支持 她们 并 鼓励 她们 的 父母 时 我 看到 了 未来 的 希望 也 看到 这种 变化 会 延续 延续下去 下去\n",
      "\n",
      "Target1:= But when I am back in Afghanistan , when I see the students in my school and their parents who advocate for them , who encourage them , I see a promising future and lasting change  . \n",
      "Predict1:< But I I I I , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 因为 它 有 很多 空洞 相互 抵消 了 噪音 所以 路 就 很 安静\n",
      "\n",
      "Target2:= Because it is very hollow , all the noise will disappear , so it is a very silent road  . \n",
      "Predict2:< And is is is , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 5s (- -13m 55s), Epoch: [1/10], Step: [2660/3335], Train Loss: 3.479815125465393, BLEU: 0.17806248093471613\n",
      "\n",
      "Input1:> 当时 我 在 一家 意大利 大利 非政府 政府 组织 织工 工作 我们 在 非洲 建立 的 每 一个 项目 都 失败 了\n",
      "\n",
      "Target1:= I worked for an Italian NGO , and every single project that we set up in Africa failed  . \n",
      "Predict1:< And I , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 说 你 长大 后 想 当 模特 就 像 你 说 你 长大 后 想 赢 彩票 一样\n",
      "\n",
      "Target2:= Saying that you want to be a model when you grow up is akin to saying that you want to win the Powerball when you grow up  . \n",
      "Predict2:< You you you you you you you you , you , , you , , you , , you , , you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 10s (- -13m 49s), Epoch: [1/10], Step: [2680/3335], Train Loss: 3.594862401485443, BLEU: 0.17883045502112901\n",
      "\n",
      "Input1:> 这些 照片 很 容易 再次 受损\n",
      "\n",
      "Target1:= It is very easy to do more damage to those damaged photos  . \n",
      "Predict1:< And is , ,  .  .  .\n",
      "\n",
      "Input2:> 我们 可以 多次 多次重 多次重复 重复\n",
      "\n",
      "Target2:= Several times we can repeat that  . \n",
      "Predict2:< We we we we to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 16s (- -13m 44s), Epoch: [1/10], Step: [2700/3335], Train Loss: 3.5665628910064697, BLEU: 0.18421865401986137\n",
      "\n",
      "Input1:> 但是 很快 就 有人 抱怨\n",
      "\n",
      "Target1:= And then somebody complained  . \n",
      "Predict1:< But is is  .  .  .\n",
      "\n",
      "Input2:> 我 无法 否认 家里 的确 确有 过 艰难 的 时候 我 多 想 他们 跟 我 一样\n",
      "\n",
      "Target2:= Now , I cannot deny that there have been some challenging moments in my family , moments where I ve wished that they were just like me  . \n",
      "Predict2:< I I I I I I I I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 21s (- -13m 38s), Epoch: [1/10], Step: [2720/3335], Train Loss: 3.534755539894104, BLEU: 0.18457655982326154\n",
      "\n",
      "Input1:> 她家 有 一大 一大堆 大堆 的 家禽 被 狮子 捕食 了 她 问 我 是否 可以 帮 她 在家 家里 也 装 一个 这样 的 灯\n",
      "\n",
      "Target1:= She had a lot of her animals being killed by lions , and she asked me if I could put the lights for her  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所有 这些 受 海啸 影响 的 照片 在 盐水 咸咸的 海水 里 浸泡 受 细菌 侵蚀 又 埋没 在 垃圾 里 有时 甚至 被 油污 浸染 所有 这些 经过 一段 一段时间 段时间 时间 都 会 持续 破坏 这些\n",
      "\n",
      "Target2:= Now , all these photos had been damaged by water , submerged in salt water , covered in bacteria , in sewage , sometimes even in oil , all of which over time is going to continue to \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 27s (- -13m 33s), Epoch: [1/10], Step: [2740/3335], Train Loss: 3.5335161447525025, BLEU: 0.18800376077900577\n",
      "\n",
      "Input1:> 因为 我 的 家人 不 懂 中文 我 害怕 他们 会 被 抓住\n",
      "\n",
      "Target1:= Since my family couldn t understand Chinese , I thought my family was going to be arrested  . \n",
      "Predict1:< And I I I , , , , , I I I  .  .\n",
      "\n",
      "Input2:> 这个 本来 被 遗忘 的 建筑 变成 了 一个 极具 建设 建设性 的 地方 这些 人 的 希望 和 梦想 让 我 放声 大笑 也 黯然 落泪 也 曾 在 我 经历 困境 的 时候 给 我 安慰\n",
      "\n",
      "Target2:= So this neglected space became a constructive one , and people is hopes and dreams made me laugh out loud , tear up , and they consoled me during my own tough times  . \n",
      "Predict2:< And is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 32s (- -13m 27s), Epoch: [1/10], Step: [2760/3335], Train Loss: 3.46214382648468, BLEU: 0.18088669972206067\n",
      "\n",
      "Input1:> 那个 善良 的 陌生 陌生人 生人 象征 着 我 以及 朝鲜 朝鲜人 最 需要 的 希望 他 使 我 明白 陌生 陌生人 生人 的 善意 和 国际 社会 的 支持 真是 我们 朝鲜 朝鲜人 需要 的 希望 之\n",
      "\n",
      "Target1:= The kind stranger symbolized new hope for me and the North Korean people when we needed it most , and he showed me the kindness of strangers and the support of the international community are truly the rays \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢\n",
      "\n",
      "Target2:= Thank you very much  . \n",
      "Predict2:< Thank you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 38s (- -13m 22s), Epoch: [1/10], Step: [2780/3335], Train Loss: 3.5738810658454896, BLEU: 0.1813641579496359\n",
      "\n",
      "Input1:> 我 在 加纳 地下 50 米 深 的 一个 非法 矿井 里\n",
      "\n",
      "Target1:= I am 150 feet down an illegal mine shaft in Ghana  . \n",
      "Predict1:< I I , , , , , , ,  .\n",
      "\n",
      "Input2:> 这些 迹象 表明 如果 我们 能 有 一个 表达 自己 的 意愿 并 与 他人 分享 的 机会 那么 公共 空间 将 发挥 巨大 的 作用\n",
      "\n",
      "Target2:= Together , we ve shown how powerful our public spaces can be if we are given the opportunity to have a voice and share more with one another  . \n",
      "Predict2:< And we we , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 43s (- -13m 16s), Epoch: [1/10], Step: [2800/3335], Train Loss: 3.5715791463851927, BLEU: 0.18779156663318822\n",
      "\n",
      "Input1:> 一天 一周 实际 实际上 是 一年\n",
      "\n",
      "Target1:= A day , a week  ? Actually , a whole year  . \n",
      "Predict1:< And is is is  .  .  .\n",
      "\n",
      "Input2:> 纺织 纺织业 也 是 我们 听到 奴役 劳动 时经 经常 联想 想到 的\n",
      "\n",
      "Target2:= The textile industry is another one we often think of when we hear about slave labor  . \n",
      "Predict2:< And we we we , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 49s (- -13m 11s), Epoch: [1/10], Step: [2820/3335], Train Loss: 3.6761234641075133, BLEU: 0.18888254495847248\n",
      "\n",
      "Input1:> 我 根本 不知 知道 身为 一个 朝鲜 的 难民 我 未来 的 生活 活会 怎样 但是 我 很快 快意 意识 到 那 生活 不但 充满 了 艰辛 甚至 无比 危险 因为 朝鲜 难民 在 中国 被 视为 非法\n",
      "\n",
      "Target1:= I had no idea what life was going to be like as a North Korean refugee , but I soon learned it is not only extremely difficult , it is also very dangerous , since North Korean refugees \n",
      "Predict1:< I I I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 就是 这个 项目 目的 全部 为了 恢复 人性 中 的 一点 小 碎片 挽回 UNK 人们 与 过去 的 连接 连接点 接点\n",
      "\n",
      "Target2:= That is all this project was about , about restoring those little bits of humanity , giving someone that connection back  . \n",
      "Predict2:< This is is a the , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12m 54s (- -13m 5s), Epoch: [1/10], Step: [2840/3335], Train Loss: 3.4386656641960145, BLEU: 0.18739281786154327\n",
      "\n",
      "Input1:> 讲话 会 变得 小声 而且 之后 伴随 更多 气息 音 这些 都 是 症状 的 例子\n",
      "\n",
      "Target1:= The speech actually becomes quieter and more UNK after a while , and that is one of the example symptoms of it  . \n",
      "Predict1:< And the , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 出身 阿富汗 边远 边远地区 地区 有着 与 他人 不同 的 见解 他 坚持 让 他 的 女儿 我 的 母亲 去 上学 并 因此 被迫 与 他 的 父亲 断绝 父子 父子关系 子关 关系\n",
      "\n",
      "Target2:= A total maverick from a remote province of Afghanistan , he insisted that his daughter , my mom , go to school , and for that he was UNK by his father  . \n",
      "Predict2:< He he he , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 0s (- -13m 0s), Epoch: [1/10], Step: [2860/3335], Train Loss: 3.6777557611465452, BLEU: 0.1912014419723966\n",
      "\n",
      "Input1:> 但 当然 由于 各种 组织 和 血液 的 干扰 这回 相当 困难 所以 我们 呼吁 科学 科学家 学家 来 真正 重视 飞秒 成像 把 这个 技术 作为 一种 解决 下一代 一代 健康 问题 的 新 模型\n",
      "\n",
      "Target1:= But of course , because of tissue and blood , this is quite challenging , so this is really a call for scientists to start thinking about UNK as really a new imaging modality to solve the next \n",
      "Predict1:< But , the , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 新奥尔良 奥尔良 拥有 世界 上 很多 最 漂亮 的 建筑 但 同时 她 也 是 全美 拥有 最多 废弃 建筑 的 城市\n",
      "\n",
      "Target2:= The city has some of the most beautiful architecture in the world , but it also has one of the highest amounts of abandoned properties in America  . \n",
      "Predict2:< And is is , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 5s (- -14m 54s), Epoch: [1/10], Step: [2880/3335], Train Loss: 3.3813613414764405, BLEU: 0.18986371968846671\n",
      "\n",
      "Input1:> 其中 特别 是 一张 照 了 各个 年龄 年龄阶段 阶段 的 女士 上至 老奶奶 奶奶 下至 小女 小女孩 女孩 围着 一个 小 婴儿 这 张 照片 颇为 触动 UNK 心弦 因为 我家 也 有 一张 类似 似的 照片\n",
      "\n",
      "Target1:= One in particular , a photo of women of all ages , from grandmother to little girl , gathered around a baby , struck a chord , because a similar photo from my family , my grandmother and \n",
      "Predict1:< And is is a , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 是 一个 非常 典型 的 受害 受害者 还 因为 我家 家庭 家庭暴力 暴力 的 预兆 和 发展 模式 一无所知 无所 所知\n",
      "\n",
      "Target2:= I was also a very typical victim because I knew nothing about domestic violence , its warning signs or its patterns  . \n",
      "Predict2:< I I , a a , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 11s (- -14m 49s), Epoch: [1/10], Step: [2900/3335], Train Loss: 3.327942931652069, BLEU: 0.187605206847575\n",
      "\n",
      "Input1:> 多孔 沥青 这种 材料 用 在 荷兰 的 多数 高速 高速公路 公路 路上 它 有些 些小 小毛 毛孔 让 水 渗过 UNK 过去 雨水 会 流向 旁边 路上 驾车 就 容易 了 也 没有 溅 起 的 水花\n",
      "\n",
      "Target1:= Porous asphalt , a material that we use now in most of the highways in the Netherlands , it has pores and water can just rain through it , so all the rainwater will flow away to the \n",
      "Predict1:< And the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 对 是 的 所以 以是 这样 的 在 来电 期间 你 得 告诉 我们 你 是否 患有 这种 疾病 TR 嗯\n",
      "\n",
      "Target2:= Yeah  . Yeah  . So what will happen is that , during the call you have to indicate whether or not you have the disease or not , you see  . Right  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 16s (- -14m 43s), Epoch: [1/10], Step: [2920/3335], Train Loss: 3.5590161085128784, BLEU: 0.1909889344947479\n",
      "\n",
      "Input1:> 除了 经常 经常性 常性 的 发脾气 脾气 沮丧 和 永无 永无休止 无休 无休止 休止 的 多 动 还有 一点 特别 特别之处 之处 就是 他 纯真 UNK 无邪 的 天性 不 带 偏见 地 看 世界 他 是 个\n",
      "\n",
      "Target1:= Yet beyond the tantrums and the frustration and the never-ending hyperactivity was something really unique : a pure and innocent nature , a boy who saw the world without prejudice , a human who had never lied  . \n",
      "Predict1:< And the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 一个 真正 的 学校\n",
      "\n",
      "Target2:= A real school  . \n",
      "Predict2:< It is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 22s (- -14m 38s), Epoch: [1/10], Step: [2940/3335], Train Loss: 3.5883689761161803, BLEU: 0.1878663697019374\n",
      "\n",
      "Input1:> 这 不是 所有 所有人 有人 都 能 做 得到 的\n",
      "\n",
      "Target1:= Not everybody gets to do that  . \n",
      "Predict1:< This is is is is is  .  .  .  .\n",
      "\n",
      "Input2:> 别 光 告诉 我 你们 研究 骨 小 梁 要 告诉 我 你们 研究 骨 小 梁 是因为 因为 这种 骨骼 的 网状 UNK 组织 对 认识 和 治疗 骨质 骨质疏松症 疏松 症 很重 重要\n",
      "\n",
      "Target2:= Don t just tell me that you study UNK , but tell me that you study UNK , which is the UNK structure of our bones because it is important to understanding and treating osteoporosis  . \n",
      "Predict2:< And , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 27s (- -14m 32s), Epoch: [1/10], Step: [2960/3335], Train Loss: 3.5275267124176026, BLEU: 0.18975640622278225\n",
      "\n",
      "Input1:> 蜜月 期间 我 又 被 打 了 两次\n",
      "\n",
      "Target1:= It happened twice more on the honeymoon  . \n",
      "Predict1:< I I I I ,  .  .  .\n",
      "\n",
      "Input2:> 我们 清理 了 大量 的 发臭 腐烂 的 鱼 尸骨 那些 鱼 都 是 当地 工厂 厂里 里加 加工 处理 的 鱼\n",
      "\n",
      "Target2:= We cleared tons and tons of stinking , rotting fish carcasses from the local fish processing plant  . \n",
      "Predict2:< We we the the the , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 33s (- -14m 27s), Epoch: [1/10], Step: [2980/3335], Train Loss: 3.2911027908325194, BLEU: 0.19070805163824744\n",
      "\n",
      "Input1:> 这 就 像 在 水池 池里 扔 一块 块石 石头\n",
      "\n",
      "Target1:= It is like throwing a stone in a pond of water  . \n",
      "Predict1:< This is is a the , ,  .  .\n",
      "\n",
      "Input2:> 我 在 西澳 西澳大利亚 澳大利 澳大利亚 大利 利亚 利亚州 的 埃斯 UNK 兰斯 进行 了 试验\n",
      "\n",
      "Target2:= I started this as a UNK in UNK , in Western Australia  . \n",
      "Predict2:< I I , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 38s (- -14m 21s), Epoch: [1/10], Step: [3000/3335], Train Loss: 3.4498631715774537, BLEU: 0.18876111603624374\n",
      "\n",
      "Input1:> 对 我 来说 阿富汗 充满 了 希望 和 无限 的 可能 可能性 每 一天 在 UNK 读书 的 女孩 女孩儿 孩儿 都 这么 提醒 着 我\n",
      "\n",
      "Target1:= To me , Afghanistan is a country of hope and boundless possibilities , and every single day the girls of UNK remind me of that  . \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 政府 指责 我 给 了 我 张 告 票 通知 我 必须 移除 我 的 花园 之后 告 票 变成 了 警告 函\n",
      "\n",
      "Target2:= The city came down on me , and basically gave me a citation saying that I had to remove my garden , which this citation was turning into a warrant  . \n",
      "Predict2:< And I I I I I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 44s (- -14m 16s), Epoch: [1/10], Step: [3020/3335], Train Loss: 3.5359001040458677, BLEU: 0.1956733084296673\n",
      "\n",
      "Input1:> 开车 车行 行驶 在 加纳 的 一条 路上 与 Free the UNK 的 伙伴 一起 一个 骑 着 UNK 单车 的 同行 同行人 行人 突然 加速 靠近 我们 的 车 敲 着 车窗\n",
      "\n",
      "Target1:= Driving down a road in Ghana with partners of Free the Slaves , a fellow UNK on a UNK suddenly sped up to our UNK and tapped on the window  . \n",
      "Predict1:< And the the the , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 在 为 这次 演讲 做 调查 时 发现 现在 美国 13 岁 女孩 女孩儿 孩儿 群体 中 百分 百分之 53 不 喜欢 她们 的 身体 到 了 17 岁 这 一比 比例 上升 升至 百分 百分之 78\n",
      "\n",
      "Target2:= When I was researching this talk , I found out that of the 13-year-old girls in the United States , 53 percent don t like their bodies , and that number goes to 78 percent by the time \n",
      "Predict2:< I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 49s (- -14m 10s), Epoch: [1/10], Step: [3040/3335], Train Loss: 3.4980521321296694, BLEU: 0.19403726997498175\n",
      "\n",
      "Input1:> 因为 我 每年 们 拍 同样 的 照片 我们 的 视角 并 不一 一样 女儿 她 到达 新 的 里程 里程碑 我 透过 她 的 眼睛 看待 生活 看 她 如何 反应 如何 看待 一切\n",
      "\n",
      "Target1:= Because while we take the same photo , our perspectives change , and she reaches new milestones , and I get to see life through her eyes , and how she interacts with and sees everything  . \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但是 很快 就 有人 抱怨\n",
      "\n",
      "Target2:= And then somebody complained  . \n",
      "Predict2:< But it is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13m 55s (- -14m 5s), Epoch: [1/10], Step: [3060/3335], Train Loss: 3.4550525188446044, BLEU: 0.19750298923648177\n",
      "\n",
      "Input1:> 接近 女儿 三岁 生日 时 妻子 说 嘿 你 为什么 什么 不 带 萨比 娜 回 纽约 来 个 父女 之 旅 继续 这个 小传 传统\n",
      "\n",
      "Target1:= Approaching my daughter is third birthday , my wife said , &quot; Hey , why don t you take UNK back to New York and make it a father-daughter trip , and continue the ritual  ? &quot; \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 看到 这个 电灯 电灯泡 灯泡 是 朝外 的 因为 狮子 是从 那儿 过来 的\n",
      "\n",
      "Target2:= As you can see , the bulbs face outside , because that is where the lions come from  . \n",
      "Predict2:< You you you , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 0s (- -15m 59s), Epoch: [1/10], Step: [3080/3335], Train Loss: 3.5081154227256777, BLEU: 0.1962985035377626\n",
      "\n",
      "Input1:> 对于 阿富汗 我 意识 到 这 是 西方 方国 国家 也 不曾 意识 到 的 即 在 我们 成功 的 背后 是 一个 这样 的 父亲 他 能 认识 到 自己 女儿 的 价值 也 明白 女儿 的\n",
      "\n",
      "Target1:= What I ve come to realize about Afghanistan , and this is something that is often dismissed in the West , that behind most of us who succeed is a father who recognizes the value in his daughter \n",
      "Predict1:< And , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 它 拥有 26 平方 平方公里 方公里 公里 的 空地\n",
      "\n",
      "Target2:= They own 26 square miles of vacant lots  . \n",
      "Predict2:< It is is the , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 6s (- -15m 54s), Epoch: [1/10], Step: [3100/3335], Train Loss: 3.461050236225128, BLEU: 0.19835101681626366\n",
      "\n",
      "Input1:> 那时 他们 有 一个 问题 要 面对 就是 怎样 称号 乔治 华盛 华盛顿 他们 的 领袖\n",
      "\n",
      "Target1:= And they had to face the question of what to call George Washington , their leader  . \n",
      "Predict1:< And , , , , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 在 我 交 了 罚金 金和 贿赂 用 的 钱 后 我 的 家人 一个 一个月 后 被 释放 了 但 不久 后 他们 在 老挝 的 首都 又 被 抓起 起来 了\n",
      "\n",
      "Target2:= After I paid the fine and bribe , my family was released in one month , but soon after , my family was arrested and jailed again in the capital of Laos  . \n",
      "Predict2:< And I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 11s (- -15m 48s), Epoch: [1/10], Step: [3120/3335], Train Loss: 3.4190367698669433, BLEU: 0.19312233404632717\n",
      "\n",
      "Input1:> 尽管 我们 都 是 朝鲜 UNK 族人 但是 我们 的 内在 已经 经变 变得 非常 不同 了 由于 67 年 的 分裂\n",
      "\n",
      "Target1:= We are all Korean , but inside , we have become very different due to 67 years of division  . \n",
      "Predict1:< And we we we we we we we we , we , , , , ,\n",
      "\n",
      "Input2:> 乍看 UNK 之下 砸 石场 的 男人 个个 健壮 但 我们 凑近 了 却 发现 一些 在 边缘 地带 的 工人 人身 有 残疾 还有 一些 些小 小孩\n",
      "\n",
      "Target2:= At first glance , the pounding site seems full of powerful men , but when we look closer , we see some less fortunate working on the fringes , and children too  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 17s (- -15m 43s), Epoch: [1/10], Step: [3140/3335], Train Loss: 3.4879476547241213, BLEU: 0.1912768169848691\n",
      "\n",
      "Input1:> 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 我们 怎么 怎么样 能够 从 邻里 里间 互借 借到 更多 的 东西 同时 避免 在 不合 不合时宜 时宜 的 时候 敲开 对方 的 门\n",
      "\n",
      "Target2:= How can we lend and borrow more things without knocking on each other is doors at a bad time  ? \n",
      "Predict2:< We we we the the the , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 22s (- -15m 37s), Epoch: [1/10], Step: [3160/3335], Train Loss: 3.402695286273956, BLEU: 0.19596917584167697\n",
      "\n",
      "Input1:> 音乐 而且 因为 我们 有 一个 可以 运行 如此 之 快 的 相机 也 就是 我们 的 飞秒 相机 这个 相机 就 有 一些 特别 别的 能力\n",
      "\n",
      "Target1:= And because we have a camera that can run so fast , our UNK , it has some unique abilities  . \n",
      "Predict1:< And we we we we , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 时常 拍照\n",
      "\n",
      "Target2:= We take photos constantly  . \n",
      "Predict2:< We we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 28s (- -15m 32s), Epoch: [1/10], Step: [3180/3335], Train Loss: 3.4492506861686705, BLEU: 0.1974824343383499\n",
      "\n",
      "Input1:> 这个 指示 指示器 又 会 使 光线 闪动\n",
      "\n",
      "Target1:= And the indicator box makes the lights flash  . \n",
      "Predict1:< And is is is is  .  .  .\n",
      "\n",
      "Input2:> 所以 我 今天 天会 给 你们 一些 实例 来 解释 我们 是 干什么 什么 的\n",
      "\n",
      "Target2:= So I am going to give you some -- attempt today to try and explain to you what I do  . \n",
      "Predict2:< So I I to to to to to to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 33s (- -15m 26s), Epoch: [1/10], Step: [3200/3335], Train Loss: 3.463297486305237, BLEU: 0.19714249711016302\n",
      "\n",
      "Input1:> 谢谢 大家\n",
      "\n",
      "Target1:= Thank you very much  . \n",
      "Predict1:< Thank you  .  .\n",
      "\n",
      "Input2:> 他 被 丢到 了 伦敦 塔里 遭到 囚禁 但是 他 还是 很 勇敢 拥有 足够 的 勇气 在 议会 会上 提议 并且 到 最后 他 在 伦敦 拥有 众多 的 支持 声音 而 赢得 了 胜利\n",
      "\n",
      "Target2:= And he was thrown into the Tower of London and imprisoned , but he was brave enough , he was brave enough to take them on , and in the end he had such popular support in London \n",
      "Predict2:< He he he the , , , , , , , , , , , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 39s (- -15m 21s), Epoch: [1/10], Step: [3220/3335], Train Loss: 3.3463218569755555, BLEU: 0.19857396625406734\n",
      "\n",
      "Input1:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target1:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict1:< We we we to , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 并不知道 不知 知道 该 从 这个 实验 里 期待 些 什么 但是 第二 第二天 二天 整个 墙壁 都 被 填满 了 而且 不断 有人 添加 新 的 答案\n",
      "\n",
      "Target2:= I didn t know what to expect from this experiment , but by the next day , the wall was entirely filled out , and it kept growing  . \n",
      "Predict2:< I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 44s (- -15m 15s), Epoch: [1/10], Step: [3240/3335], Train Loss: 3.417613446712494, BLEU: 0.20136184462151685\n",
      "\n",
      "Input1:> 这 是 UNK 作为 为着 中国 和 朝鲜 边界 的 一部 一部分 部分\n",
      "\n",
      "Target1:= This is the UNK River , which serves as a part of the border between North Korea and China  . \n",
      "Predict1:< This is is a the the the the  .  .  .\n",
      "\n",
      "Input2:> 即使 他们 学会 了 中文 找到 了 工作 他们 的 整个 世界 也 会 在 顷刻 顷刻间 颠覆\n",
      "\n",
      "Target2:= Even after learning a new language and getting a job , their whole world can be turned upside down in an instant  . \n",
      "Predict2:< And they they the , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 50s (- -15m 10s), Epoch: [1/10], Step: [3260/3335], Train Loss: 3.4806026816368103, BLEU: 0.20185233679311573\n",
      "\n",
      "Input1:> 你 可以 想象 这个 小镇 肯定 备受 摧残\n",
      "\n",
      "Target1:= As you can imagine , the town had been devastated  . \n",
      "Predict1:< You you you you you ,  .  .  .\n",
      "\n",
      "Input2:> 我 只能 能指 指望 Free the UNK 组织 能够 在 体系 内 发挥 发挥作用 作用 解放 他们 我 相信 他们 可以 做到\n",
      "\n",
      "Target2:= I d have to rely on Free the Slaves to work within the system for their liberation , and I trusted that they would  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14m 55s (- -15m 4s), Epoch: [1/10], Step: [3280/3335], Train Loss: 3.47256543636322, BLEU: 0.19868595902075412\n",
      "\n",
      "Input1:> 我 是 跟着 50 个 去往 UNK 拉拉 拉巴 巴德 战役 的 反叛 反叛者 乘着 卡车 一起 起来 的 当时 我 19 岁 是 一位 住 在 佛罗里 佛罗里达 佛罗里达州 罗里 里达 达州 杰克 杰克逊 克逊 维尔 的\n",
      "\n",
      "Target1:= So I arrived by truck with about 50 rebels to the battle for UNK as a 19-year-old vegetarian surfer from Jacksonville , Florida  . \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 怎么 会 有人 人愿 愿意 和 一个 一个打 她 的 男人 继续 住在一起 一起\n",
      "\n",
      "Target2:= Why would anyone stay with a man who beats her  ? \n",
      "Predict2:< And is , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 1s (- -16m 59s), Epoch: [1/10], Step: [3300/3335], Train Loss: 3.460128629207611, BLEU: 0.20469975071350974\n",
      "\n",
      "Input1:> 我 觉得 十分 自豪\n",
      "\n",
      "Target1:= And I was very proud  . \n",
      "Predict1:< I I I  .  .  .\n",
      "\n",
      "Input2:> 不是 的 它 是 出自 出自于 一个 新闻 新闻自由 自由 卫士\n",
      "\n",
      "Target2:= It is not  . It is down to a campaigner for the freedom of the press  . \n",
      "Predict2:< It is is is is , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 6s (- -16m 53s), Epoch: [1/10], Step: [3320/3335], Train Loss: 3.451749873161316, BLEU: 0.1953281943386683\n",
      "\n",
      "Input1:> 最 坏 的 消息 是 我 要 把 这件 毛衣 从头 头上 UNK 套进 套进去 进去 因为 这时 这时候 时候 你们 会 笑 我 所以 当 我 穿衣 穿衣服 衣服 的 时候 你们 什么 都 不要 做\n",
      "\n",
      "Target1:= The worst part is putting this sweater over my head , because that is when you ll all laugh at me , so don t do anything while it is over my head  . \n",
      "Predict1:< And , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这样 的 能人 尚未 未出 出生\n",
      "\n",
      "Target2:= This person has never been born  . \n",
      "Predict2:< It is is the  .  .  .\n",
      "[11.516338539123534, 10.959937858581544, 10.933368492126466, 9.873051500320434, 5.304116988182068, 4.272518944740296, 3.9876989245414736, 3.926387083530426, 3.8163031578063964, 3.848144221305847, 3.8046405792236326, 3.7657713890075684, 3.7295499205589295, 3.877229368686676, 3.722635269165039, 3.6572103977203367, 3.6388954877853394, 3.663482391834259, 3.604764401912689, 3.535254168510437, 3.513837218284607, 3.519381070137024, 3.458109724521637, 3.386720025539398, 3.5147376537322996, 3.558142137527466, 3.530163061618805, 3.3662715315818788, 3.38189115524292, 3.449207103252411, 3.432429480552673, 3.3649975419044496, 3.423082971572876, 3.368861639499664, 3.370510482788086, 3.426781952381134, 3.4762234807014467, 3.372128665447235, 3.5204004645347595, 3.4539692282676695, 3.413427102565765, 3.3850318670272825, 3.4129316806793213, 3.3512953400611876, 3.3756153345108033, 3.5211785316467283, 3.3465731263160707, 3.5037595510482786, 3.4401856303215026, 3.447231984138489, 3.4344047784805296, 3.4025956869125364, 3.5422894239425657, 3.416458773612976, 3.407421827316284, 3.4331465363502502, 3.4562111377716063, 3.2998868942260744, 3.5391891956329347, 3.399912881851196, 3.501010525226593, 3.437377381324768, 3.4419359445571898, 3.4496235489845275, 3.3910022139549256, 3.345831871032715, 3.5066724896430967, 3.632017958164215, 3.350728476047516, 3.4498838424682616, 3.4336562514305116, 3.435765016078949, 3.3971356987953185, 3.5412478923797606, 3.4585770845413206, 3.4089304924011232, 3.5430948734283447, 3.486210525035858, 3.5187360525131224, 3.5551347017288206, 3.5710527062416078, 3.551608312129974, 3.4482064127922056, 3.3928887128829954, 3.5635721921920775, 3.454908215999603, 3.536852943897247, 3.4182421565055847, 3.431700146198273, 3.5069312930107115, 3.359319305419922, 3.634914994239807, 3.592742908000946, 3.469939112663269, 3.53578040599823, 3.6079674482345583, 3.381006920337677, 3.3917803525924684, 3.447292077541351, 3.407597279548645, 3.631361413002014, 3.5004743933677673, 3.62009516954422, 3.5110252499580383, 3.4829041957855225, 3.462900674343109, 3.6667950630187987, 3.7516698479652404, 3.5067422866821287, 3.5842238426208497, 3.6678966164588926, 3.4331801891326905, 3.6975980758666993, 3.4328919887542724, 3.707374799251556, 3.6221840262413023, 3.5756814002990724, 3.5578932881355287, 3.5951579213142395, 3.59920734167099, 3.635907506942749, 3.3846579909324648, 3.5505685448646545, 3.6669039964675902, 3.490741515159607, 3.428479218482971, 3.534936797618866, 3.5079743385314943, 3.4859880685806273, 3.726189911365509, 3.556606376171112, 3.5396712183952332, 3.479815125465393, 3.594862401485443, 3.5665628910064697, 3.534755539894104, 3.5335161447525025, 3.46214382648468, 3.5738810658454896, 3.5715791463851927, 3.6761234641075133, 3.4386656641960145, 3.6777557611465452, 3.3813613414764405, 3.327942931652069, 3.5590161085128784, 3.5883689761161803, 3.5275267124176026, 3.2911027908325194, 3.4498631715774537, 3.5359001040458677, 3.4980521321296694, 3.4550525188446044, 3.5081154227256777, 3.461050236225128, 3.4190367698669433, 3.4879476547241213, 3.402695286273956, 3.4492506861686705, 3.463297486305237, 3.3463218569755555, 3.417613446712494, 3.4806026816368103, 3.47256543636322, 3.460128629207611, 3.451749873161316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 14s (- -15m 31s), Epoch: [2/10], Step: [20/3335], Train Loss: 3.510052764415741, BLEU: 0.2011461173812573\n",
      "\n",
      "Input1:> 每当 有人 打喷嚏 喷嚏 时 我 都 感觉 新奥尔良 奥尔良 来 了 一只 游行 队伍\n",
      "\n",
      "Target1:= I feel like every time someone UNK , New Orleans has a parade  . \n",
      "Predict1:< And I , , , , , , , , , ,\n",
      "\n",
      "Input2:> 为了 了解 解决 这个 问题 我们 考虑 到 了 自我 修复 材料\n",
      "\n",
      "Target2:= To solve this problem , we thought of UNK materials  . \n",
      "Predict2:< And we we , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 19s (- -15m 3s), Epoch: [2/10], Step: [40/3335], Train Loss: 3.5408966064453127, BLEU: 0.20683948589566714\n",
      "\n",
      "Input1:> 一支 放在 车子 的 置 物 箱 里\n",
      "\n",
      "Target1:= He kept one in the glove compartment of our car  . \n",
      "Predict1:< And the the the the the  .  .  .\n",
      "\n",
      "Input2:> 议员 甚至 前来 拜访 表达 了 他 有 多么 赞赏 和 热爱 我们 所 做 的\n",
      "\n",
      "Target2:= My UNK even called in and said how they endorse and love what we are doing  . \n",
      "Predict2:< And , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 25s (- -16m 50s), Epoch: [2/10], Step: [60/3335], Train Loss: 3.2979426980018616, BLEU: 0.19951228419083972\n",
      "\n",
      "Input1:> 我 确实 实有 太多 从没 机会 穿 的 8 英寸 高跟鞋 但是 免费 的 东西 却是 我 在 现实 现实生活 生活 中 得到 的 而 这 恰恰 是 我们 不 愿意 谈 的\n",
      "\n",
      "Target1:= I do have too many UNK heels which I never get to wear , except for earlier , but the free stuff that I get is the free stuff that I get in real life , and that \n",
      "Predict1:< I I I to , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 已经 远远 远不止 不止 了\n",
      "\n",
      "Target2:= It is much better than that  . \n",
      "Predict2:< And is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 30s (- -16m 41s), Epoch: [2/10], Step: [80/3335], Train Loss: 3.397197902202606, BLEU: 0.19636448946433474\n",
      "\n",
      "Input1:> 一天 一周 实际 实际上 是 一年\n",
      "\n",
      "Target1:= A day , a week  ? Actually , a whole year  . \n",
      "Predict1:< And is is the ,  .  .  .\n",
      "\n",
      "Input2:> 这 就是 洛杉矶 UNK 贩 酒店 快餐 快餐店 闲置 的 土地\n",
      "\n",
      "Target2:= This is South Central : liquor stores , fast food , vacant lots  . \n",
      "Predict2:< This is is the the , , , , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 36s (- -16m 33s), Epoch: [2/10], Step: [100/3335], Train Loss: 3.534105455875397, BLEU: 0.20291125803747803\n",
      "\n",
      "Input1:> 或许 它 正 发生 生在 你 的 身上\n",
      "\n",
      "Target1:= It may even be your story  . \n",
      "Predict1:< And you you you  .  .  .\n",
      "\n",
      "Input2:> 这些 人 被 奴役 在 加纳 的 另 一处 矿井 里\n",
      "\n",
      "Target2:= These miners are enslaved in a mine shaft in another part of Ghana  . \n",
      "Predict2:< And the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 41s (- -16m 26s), Epoch: [2/10], Step: [120/3335], Train Loss: 3.482458567619324, BLEU: 0.20977958132650193\n",
      "\n",
      "Input1:> 所以 我们 要 做 的 就是 闭嘴\n",
      "\n",
      "Target1:= So what you do -- you shut up  . \n",
      "Predict1:< So we we we we we we  .  .\n",
      "\n",
      "Input2:> 顺便 提下 如果 去 神经 神经科 诊所 做 的话 这个 费用 要 300 美金\n",
      "\n",
      "Target2:= Takes about $ 300 , by the way , in the neurologist is clinic to do it  . \n",
      "Predict2:< And , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 47s (- -16m 19s), Epoch: [2/10], Step: [140/3335], Train Loss: 3.5161154866218567, BLEU: 0.20063398338893065\n",
      "\n",
      "Input1:> 事实 实是 就 因为 他们 的 思维 不符 符合 社会 对 正常 正常人 常人 的 定义 他们 经常 被 孤立 被 误解\n",
      "\n",
      "Target1:= And in fact , because their minds don t fit into society is version of normal , they are often bypassed and misunderstood  . \n",
      "Predict1:< And they , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 直至 现在 我 才 意识 到 丢失 了 这些 照片 肯定 是 这些 人 失去 的 一块 重大 大部 大部分 部分\n",
      "\n",
      "Target2:= Now , it wasn t until this point that I realized that these photos were such a huge part of the personal loss these people had felt  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 52s (- -16m 13s), Epoch: [2/10], Step: [160/3335], Train Loss: 3.5318225264549254, BLEU: 0.21329882203301967\n",
      "\n",
      "Input1:> 我 当时 22 岁 而 在 美国 16 至 24 岁 的 女性 性相 相比 其他 年龄 年龄段 的 女性 受到 家庭 家庭暴力 暴力 伤害 的 可能 可能性 要 高出 两倍 以上 同样 在 美国 每年 有 超过\n",
      "\n",
      "Target1:= I was 22 , and in the United States , women ages 16 to 24 are three times as likely to be domestic violence victims as women of other ages , and over 500 women and girls this \n",
      "Predict1:< I I the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 看到 似乎 是 一家 一家人 家人 在 船上 捕鱼 两个 哥哥 几个 小点 的 孩子 挺 像 的 吧\n",
      "\n",
      "Target2:= I saw what seemed to be a family fishing on a boat , two older brothers , some younger kids , makes sense right  ? \n",
      "Predict2:< I I , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15m 58s (- -16m 7s), Epoch: [2/10], Step: [180/3335], Train Loss: 3.398360550403595, BLEU: 0.20737544296930144\n",
      "\n",
      "Input1:> 而是 我 要 被 停 几次 我 什么 时候 会 被 停\n",
      "\n",
      "Target1:= but &quot; How many times will I get stopped  ? When will I get stopped  ? &quot; \n",
      "Predict1:< And I I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 社区 会议 存在 一个 问题\n",
      "\n",
      "Target2:= There is a problem with community meetings  . \n",
      "Predict2:< It is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 3s (- -16m 1s), Epoch: [2/10], Step: [200/3335], Train Loss: 3.456430661678314, BLEU: 0.2079735636482803\n",
      "\n",
      "Input1:> 当 我 在 拍摄 的 途中 我 带 了 很多 蜡烛 在 翻译 的 帮助 下 我 把 蜡烛 分给 了 我 的 拍摄 对象 我 希望 能 点亮 他们 的 故事 和 他们 的 困境 当 我们\n",
      "\n",
      "Target1:= When I was working in the field , I brought lots of candles with me , and with the help of my interpreter , I imparted to the people I was photographing that I wanted to illuminate their \n",
      "Predict1:< And I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 一切 都 准备 好 了\n",
      "\n",
      "Target2:= So I set up everything  . \n",
      "Predict2:< And , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 9s (- -17m 55s), Epoch: [2/10], Step: [220/3335], Train Loss: 3.383474659919739, BLEU: 0.20948398856074157\n",
      "\n",
      "Input1:> 我 都 快 抓狂 了\n",
      "\n",
      "Target1:= And I was distraught  . \n",
      "Predict1:< I I I to  .  .  .\n",
      "\n",
      "Input2:> 这 是 我 为何 要 维护 它 的 原因\n",
      "\n",
      "Target2:= And this is how I decided to maintain it  . \n",
      "Predict2:< This is is a a a  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 14s (- -17m 49s), Epoch: [2/10], Step: [240/3335], Train Loss: 3.470285475254059, BLEU: 0.21308867368510695\n",
      "\n",
      "Input1:> 你 怎么 能 我 答道 我 做 了 一件 很困 困难 的 事\n",
      "\n",
      "Target1:= How can you do —  ? &quot; And I said , &quot; I do something very , very , very difficult  . \n",
      "Predict1:< You I I to , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 我 在 早上 五点 时 看到 这些 男孩 男孩子 孩子 们 在 收 最后 一个 渔网 可 他们 从 凌晨 1 点 就 开始 工作 了\n",
      "\n",
      "Target2:= I met these boys at five in the morning , when they were hauling in the last of their nets , but they had been working since 1 a .m . \n",
      "Predict2:< I I , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 20s (- -17m 43s), Epoch: [2/10], Step: [260/3335], Train Loss: 3.6210897326469422, BLEU: 0.20724892463121222\n",
      "\n",
      "Input1:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target1:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict1:< And , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 这样 一张 照片 重回 它 的 主人 人身 身边 它 将 带来 巨大 的 不同 为 失主 的 生活 带来 很大 的 不一 一样\n",
      "\n",
      "Target2:= When a photo like this can be returned to someone like this , it makes a huge difference in the lives of the person receiving it  . \n",
      "Predict2:< And , is the , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 25s (- -17m 37s), Epoch: [2/10], Step: [280/3335], Train Loss: 3.5140493512153625, BLEU: 0.2068172161458728\n",
      "\n",
      "Input1:> 谁 还 会 认真 认真对待 真对 对待 他 如果 他 只有 一个 这样 愚蠢 的 小头 头衔 比如 美利 美利坚 联邦 联邦政府 邦政府 政府 主持 UNK of the United UNK of America\n",
      "\n",
      "Target1:= And who is going to take him seriously if he is got a silly little title like President of the United States of America  ? \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 呃 希望 不会 像 图片 上 这样 怪\n",
      "\n",
      "Target2:= Well , hopefully not as awkward as that picture  . \n",
      "Predict2:< And is , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 31s (- -17m 32s), Epoch: [2/10], Step: [300/3335], Train Loss: 3.5426509499549867, BLEU: 0.21442982076022027\n",
      "\n",
      "Input1:> 我 是 跟着 50 个 去往 UNK 拉拉 拉巴 巴德 战役 的 反叛 反叛者 乘着 卡车 一起 起来 的 当时 我 19 岁 是 一位 住 在 佛罗里 佛罗里达 佛罗里达州 罗里 里达 达州 杰克 杰克逊 克逊 维尔 的\n",
      "\n",
      "Target1:= So I arrived by truck with about 50 rebels to the battle for UNK as a 19-year-old vegetarian surfer from Jacksonville , Florida  . \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 每当 你 看到 秃鹫 就 像 这 只 首先 浮现 现在 你 脑海 脑海中 海中 一定 是 这 是 一些 令人 令人厌恶 厌恶 贪得 贪得无厌 并且 追逐 腐肉 的 可怕 怕生 生物 让 人 禁不住 不住 联想 想到\n",
      "\n",
      "Target2:= When you see a vulture like this , the first thing that comes to your mind is , these are disgusting , ugly , greedy creatures that are just after your flesh , associated with politicians  . \n",
      "Predict2:< And you you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 36s (- -17m 26s), Epoch: [2/10], Step: [320/3335], Train Loss: 3.427303409576416, BLEU: 0.2105398283342437\n",
      "\n",
      "Input1:> 这 是 一家 一家人 家人 的 肖像\n",
      "\n",
      "Target1:= This is a family portrait  . \n",
      "Predict1:< This is is a  .  .  .\n",
      "\n",
      "Input2:> 我 并 不是 精神 精神病 医生 社会 社会工作 社会工作者 工作 工作者 作者 也 不是 家庭 家庭暴力 暴力 方面 的 专家\n",
      "\n",
      "Target2:= I am not a psychiatrist , a social worker or an expert in domestic violence  . \n",
      "Predict2:< I I , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 42s (- -17m 20s), Epoch: [2/10], Step: [340/3335], Train Loss: 3.4726403951644897, BLEU: 0.20990262469968055\n",
      "\n",
      "Input1:> 那么 秃鹫 都 面临 着 哪些 问题 呢\n",
      "\n",
      "Target1:= So what is the problem with vultures  ? \n",
      "Predict1:< So , is , ,  .  .\n",
      "\n",
      "Input2:> 你 知道 的 这样 大概 会 可行\n",
      "\n",
      "Target2:= And , you know , it could have worked  . \n",
      "Predict2:< You you you you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 47s (- -17m 15s), Epoch: [2/10], Step: [360/3335], Train Loss: 3.520396375656128, BLEU: 0.20813113460976443\n",
      "\n",
      "Input1:> 我 尝试 试用 选举 来 解密 黑手 黑手党 的 成员 这些 化妆 成为 国家 英雄 的 人\n",
      "\n",
      "Target1:= And try and use this campaign to expose these UNK who are masquerading as national heroes  . \n",
      "Predict1:< I I I to the , , , , , , the the\n",
      "\n",
      "Input2:> 这 是 我 帮助 其他 的 受害 受害者 的 方式 同时 也 是 我 对 你们 最后 的 请求\n",
      "\n",
      "Target2:= It is my way of helping other victims , and it is my final request of you  . \n",
      "Predict2:< This is is I I I I I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 53s (- -17m 9s), Epoch: [2/10], Step: [380/3335], Train Loss: 3.5963674068450926, BLEU: 0.21075860177273273\n",
      "\n",
      "Input1:> 每当 有人 打喷嚏 喷嚏 时 我 都 感觉 新奥尔良 奥尔良 来 了 一只 游行 队伍\n",
      "\n",
      "Target1:= I feel like every time someone UNK , New Orleans has a parade  . \n",
      "Predict1:< And , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 自从 我 足够 大拿 拿起 相机 以来 我 一直 热衷 摄影 但 我 今天 想 和 你们 分享 15 张 我 最 珍爱 的 照片 它们 没有 一张 是 我 拍 的\n",
      "\n",
      "Target2:= Photography has been my passion ever since I was old enough to pick up a camera , but today I want to share with you the 15 most treasured photos of mine , and I didn t take \n",
      "Predict2:< I I I I , I , I , I , I , , I , I , I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16m 58s (- -17m 3s), Epoch: [2/10], Step: [400/3335], Train Loss: 3.6712921142578123, BLEU: 0.21370325016295325\n",
      "\n",
      "Input1:> 我 曾经 误以为 以为 只有 我 一个 个人 有 这样 的 遭遇\n",
      "\n",
      "Target1:= I was mistaken in thinking that I was unique and alone in this situation  . \n",
      "Predict1:< I I I I I I I I  .  .  .  .\n",
      "\n",
      "Input2:> 她们 的 出现 使 我 感觉 非常 糟糕 我 告诉 他们 你们 不用 像 这样 来 拿 食物\n",
      "\n",
      "Target2:= So I am like , man , it made me feel bad that they were there , and I told them , you know , you don t have to do this like this  . \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 4s (- -18m 58s), Epoch: [2/10], Step: [420/3335], Train Loss: 3.4518476128578186, BLEU: 0.21266277505720838\n",
      "\n",
      "Input1:> 不仅 因为 它 听起 起来 很 不错\n",
      "\n",
      "Target1:= Just because it sounds so good  . \n",
      "Predict1:< It is is is is  .  .  .\n",
      "\n",
      "Input2:> 赞比亚 比亚 人 说 没错 这 就是 我们 不在 这里 耕种 的 原因\n",
      "\n",
      "Target2:= And the UNK said , &quot; Yes , that is why we have no agriculture here  . &quot; \n",
      "Predict2:< And , , , , , , , , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 9s (- -18m 52s), Epoch: [2/10], Step: [440/3335], Train Loss: 3.4884423136711122, BLEU: 0.20372048981167604\n",
      "\n",
      "Input1:> 大家 都 想 问 的 另 一个 问题 是 为什么 什么 她 不离 离开\n",
      "\n",
      "Target1:= The other question everybody asks is , why doesn t she just leave  ? \n",
      "Predict1:< And , , , , , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 我 看到 似乎 是 一家 一家人 家人 在 船上 捕鱼 两个 哥哥 几个 小点 的 孩子 挺 像 的 吧\n",
      "\n",
      "Target2:= I saw what seemed to be a family fishing on a boat , two older brothers , some younger kids , makes sense right  ? \n",
      "Predict2:< I I , the the , the , the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 15s (- -18m 46s), Epoch: [2/10], Step: [460/3335], Train Loss: 3.461902391910553, BLEU: 0.21030781176649935\n",
      "\n",
      "Input1:> 这个 小男孩 男孩 8 岁 大\n",
      "\n",
      "Target1:= This young child is eight years old  . \n",
      "Predict1:< And is is the  .  .  .\n",
      "\n",
      "Input2:> 但 第二 第二个 二个 答案 也 是 我 想 对 问问 问题 的 那些 些小 小姑 小姑娘 姑娘 说 的 是 为什么 什么\n",
      "\n",
      "Target2:= But the second answer , and what I really want to say to these little girls is , &quot; Why  ? \n",
      "Predict2:< But , is is , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 20s (- -18m 41s), Epoch: [2/10], Step: [480/3335], Train Loss: 3.489122951030731, BLEU: 0.20958432269433347\n",
      "\n",
      "Input1:> 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定 女孩 上学 是 违法 的\n",
      "\n",
      "Target1:= You see , I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school  . \n",
      "Predict1:< I I I to , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 他 经常 尖叫\n",
      "\n",
      "Target2:= He screamed a lot  . \n",
      "Predict2:< He he &quot;  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 26s (- -18m 35s), Epoch: [2/10], Step: [500/3335], Train Loss: 3.355366384983063, BLEU: 0.21464000428940935\n",
      "\n",
      "Input1:> 我们 怎么 怎么样 能够 从 邻里 里间 互借 借到 更多 的 东西 同时 避免 在 不合 不合时宜 时宜 的 时候 敲开 对方 的 门\n",
      "\n",
      "Target1:= How can we lend and borrow more things without knocking on each other is doors at a bad time  ? \n",
      "Predict1:< We we we to the the , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 举 个例 例子 我 所 住 的 街区 的 肥胖 肥胖率 要 比 相距 我们 10 英里 不到 的 比 佛利 山庄 高出 5 倍 也许 是 8 倍\n",
      "\n",
      "Target2:= For instance , the obesity rate in my neighborhood is five times higher than , say , Beverly Hills , which is probably eight , 10 miles away  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 31s (- -18m 30s), Epoch: [2/10], Step: [520/3335], Train Loss: 3.366362953186035, BLEU: 0.2124212494762625\n",
      "\n",
      "Input1:> 我 看到 透析 中心 像 星巴克 巴克 一样 不断 不断涌现 涌现\n",
      "\n",
      "Target1:= I see dialysis centers popping up like Starbucks  . \n",
      "Predict1:< I I I to to , , , ,  .  .\n",
      "\n",
      "Input2:> 那 就是 她\n",
      "\n",
      "Target2:= There she is  . \n",
      "Predict2:< And is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 37s (- -18m 24s), Epoch: [2/10], Step: [540/3335], Train Loss: 3.543223524093628, BLEU: 0.21804590697700388\n",
      "\n",
      "Input1:> 不过 随着 对话 的 展开 我 的 经历 就 如 爱丽丝 一样 顺着 那个 兔子 洞 下去 看到 了 那 扇 通往 一个 全新 新世界 世界 的 大门\n",
      "\n",
      "Target1:= But as these conversations unfolded , I experienced what Alice must have when she went down that rabbit hole and saw that door to a whole new world  . \n",
      "Predict1:< But the the , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 世界 上 所有 有成 成功 的 企业 只有 一个 共同 共同点 同点 只有 一个 没有 一个 企业 是 靠 一 人 之 力 创建 的\n",
      "\n",
      "Target2:= There is only one thing that all the successful companies in the world have in common , only one : None were started by one person  . \n",
      "Predict2:< It is a the , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 42s (- -18m 19s), Epoch: [2/10], Step: [560/3335], Train Loss: 3.4657609701156615, BLEU: 0.21182174530607217\n",
      "\n",
      "Input1:> 即便 即便如此 如此 你 还是 不想 继续 吗\n",
      "\n",
      "Target1:= So do you still not want to continue  ? &quot; \n",
      "Predict1:< And you you you you you you  .  .  .\n",
      "\n",
      "Input2:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target2:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict2:< And you you you you , you , you , , you , you ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 48s (- -18m 13s), Epoch: [2/10], Step: [580/3335], Train Loss: 3.456070053577423, BLEU: 0.2077636738966588\n",
      "\n",
      "Input1:> 在 这 张 照片 里 我 事实 事实上 在 现实 现实生活 生活 中 根本 没有 男朋友 朋友\n",
      "\n",
      "Target1:= And in this picture , I had actually never had a boyfriend in real life  . \n",
      "Predict1:< And , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 在 他 叔叔 死 后 UNK 要 继续 背负 他 叔叔 的 欠债 他 被迫 继续 在 矿场 工作\n",
      "\n",
      "Target2:= When his uncle died , UNK inherited his uncle is debt , which further forced him into being enslaved in the mines  . \n",
      "Predict2:< He he , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 53s (- -18m 7s), Epoch: [2/10], Step: [600/3335], Train Loss: 3.4908087849617004, BLEU: 0.21107113288260032\n",
      "\n",
      "Input1:> 花园 非常 的 美丽\n",
      "\n",
      "Target1:= And the garden , it was beautiful  . \n",
      "Predict1:< The is  .  .  .\n",
      "\n",
      "Input2:> 朝鲜 朝鲜人 通往 自由 的 道路 是 如此 遥不可及 不可 及\n",
      "\n",
      "Target2:= North Koreans have to travel incredible distances on the path to freedom  . \n",
      "Predict2:< The is is , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 17m 59s (- -18m 2s), Epoch: [2/10], Step: [620/3335], Train Loss: 3.4379523396492004, BLEU: 0.21396066635841038\n",
      "\n",
      "Input1:> 我 回答 说 当然 不 啦 我 当然 不怕 怕人 人们 会 偷 它\n",
      "\n",
      "Target1:= And I am like , &quot; Hell no , I ain t afraid they are gonna steal it  . \n",
      "Predict1:< I I I , , , , , , , , , , , , , I  .\n",
      "\n",
      "Input2:> 所以 很多 很多年 多年 年前 我 有 个 想法 我们 为何 何不 不能 至少 少有 一次 做到 聆听 他们 的 想法 而 不是 把 我们 的 想法 强加 强加于 加于 他们 但 这 不能 以 社区 会议 的\n",
      "\n",
      "Target2:= So years and years ago , I had this idea : Why don t we , for once , instead of arriving in the community to tell people what to do , why don t , for once \n",
      "Predict2:< So , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 4s (- -19m 56s), Epoch: [2/10], Step: [640/3335], Train Loss: 3.498739171028137, BLEU: 0.21538497959961442\n",
      "\n",
      "Input1:> 但 如果 你 在 看 一下 这个 西红柿 你 就 会 注意 到 当 光 在 西红柿 上 走过 过时 它 一直 在 闪耀 它 并 没有 有变 变暗\n",
      "\n",
      "Target1:= But if you look at this tomato one more time , you will notice , as the light washes over the tomato , it continues to glow  . It doesn t become dark  . \n",
      "Predict1:< But if you you you , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 内燃 内燃机 燃机 的 燃料 总 有 耗尽 的 一天\n",
      "\n",
      "Target2:= The internal combustion engine is not sustainable  . \n",
      "Predict2:< And is is the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 10s (- -19m 51s), Epoch: [2/10], Step: [660/3335], Train Loss: 3.3968768954277038, BLEU: 0.22066065331521073\n",
      "\n",
      "Input1:> 掌声 我 的 家人 很 信任 我\n",
      "\n",
      "Target1:= My family believes in me  . \n",
      "Predict1:< And I I I  .  .  .\n",
      "\n",
      "Input2:> 然后 我们 帮助 他们 搜集 相关 知识 和 信息 因为 这 世上 UNK UNK 不成 线 独木 UNK UNK 不成 成林\n",
      "\n",
      "Target2:= And then we help them to go and find the knowledge , because nobody in the world can succeed alone  . \n",
      "Predict2:< And we we to , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 15s (- -19m 45s), Epoch: [2/10], Step: [680/3335], Train Loss: 3.586659252643585, BLEU: 0.21179411993041689\n",
      "\n",
      "Input1:> 我 在 埃斯 UNK 兰斯 的 第一 第一年 一年 只是 出去 遛 街 在 那些 日子 里 我 有 了 第一 第一位 一位 客户 我 帮助 了 这 第一 第一个 一个 个人 他 是 UNK UNK 利人 在\n",
      "\n",
      "Target1:= And so what I did in UNK that first year was to just walk the streets , and in three days I had my first client , and I helped this first guy who was smoking fish from \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 这个 年龄 的 女性 中 高中 高中毕业 毕业 的 人数 数不到 不到 6 如果 不是 我家 家人 的 坚持 我 也 会 像 绝大 绝大多数 大多 大多数 多数 数人 一样\n",
      "\n",
      "Target2:= Fewer than six percent of women my age have made it beyond high school , and had my family not been so committed to my education , I would be one of them  . \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 21s (- -19m 40s), Epoch: [2/10], Step: [700/3335], Train Loss: 3.470386064052582, BLEU: 0.21184446988163758\n",
      "\n",
      "Input1:> 我们 发现 它们 会 在 不同 的 国家 之间 旅行 所以 如果 只是 关注 一个 地区 你 很 难 取得 什么 进展\n",
      "\n",
      "Target1:= We can see that they travel different countries , so if you focus on a problem locally , it is not going to help you  . \n",
      "Predict1:< We can you to , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 学校 我们 用 很多 时间 来 学习 UNK UNK 的 历史 但是 我们 从来 从来没 没有 对外 外面 的 世界 有 过 太多 了解 除了 知道 美国 韩国 和 日本 是 敌人 外\n",
      "\n",
      "Target2:= In school , we spent a lot of time studying the history of Kim UNK , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies  . \n",
      "Predict2:< In we we , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 26s (- -19m 34s), Epoch: [2/10], Step: [720/3335], Train Loss: 3.519069766998291, BLEU: 0.21690437561654322\n",
      "\n",
      "Input1:> 目睹 这样 UNK 的 场景 是 一件 极其 痛苦 的 事\n",
      "\n",
      "Target1:= It is difficult to witness something so overwhelming  . \n",
      "Predict1:< And is is is is is is  .  .  .\n",
      "\n",
      "Input2:> 用 UNK 冷藏 食物 也 非 长久 长久之计 久之 计\n",
      "\n",
      "Target2:= UNK way of maintaining things is not sustainable  . \n",
      "Predict2:< The , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 32s (- -19m 29s), Epoch: [2/10], Step: [740/3335], Train Loss: 3.5215264678001406, BLEU: 0.2160341058088767\n",
      "\n",
      "Input1:> 在 我 的 身后 是 我 父亲 的 奶牛 在 奶牛 的 后边 就是 内罗毕 国家 公园\n",
      "\n",
      "Target1:= Those are my dad is cows at the back , and behind the cows , that is the Nairobi National Park  . \n",
      "Predict1:< And I I the , , , , , the the the the the\n",
      "\n",
      "Input2:> 人们 把 收集 到 的 照片 送到 那个 公社 温泉 当时 我 十分 有幸 幸得 得到 了 他们 的 信任 让 我 开始 清洗 还原 那些 照片\n",
      "\n",
      "Target2:= This is where people were handing them in , and I was honored that day that they actually trusted me to help them start UNK them  . \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 37s (- -19m 23s), Epoch: [2/10], Step: [760/3335], Train Loss: 3.4380592703819275, BLEU: 0.2076023892153481\n",
      "\n",
      "Input1:> 在 这样 寒冷 刮风 的 晚上\n",
      "\n",
      "Target1:= in the cold , windy night  . \n",
      "Predict1:< And is the the the  .\n",
      "\n",
      "Input2:> 停电 也 变得 越来 越来越 频繁 到 了 晚上 我 周围 的 一切 都 变得 漆黑 漆黑一片 一片 除了 中国 的 灯火 灯火通明 通明 只要 穿过 我家 旁 的 河 就是\n",
      "\n",
      "Target2:= Power outages also became more and more frequent , so everything around me was completely dark at night except for the sea of lights in China , just across the river from my home  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 43s (- -19m 18s), Epoch: [2/10], Step: [780/3335], Train Loss: 3.4574529767036437, BLEU: 0.21517863660682623\n",
      "\n",
      "Input1:> 所以 我 想 向 你们 提出 一个 非常 非常规 常规 的 的 方法 一个 我们 能 尝试 去 达到 目的 的 方法 因为 你 也 知道 在 某种 某种意义 意义 上 至少 我们 都 是 像 我 朋友\n",
      "\n",
      "Target1:= So what I want to propose to you as an unconventional way in which we can try to achieve this , because , you see , in one sense , at least , we are all virtuosos like \n",
      "Predict1:< So I I I to to to to to , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 我 在 修复 那些 照片 的 时候 我 不禁 想到 其中 的 每个 个人 以及 照片 片中 传递 的 那些 故事\n",
      "\n",
      "Target2:= &quot; As I worked , I couldn t help but think about the individuals and the stories represented in the images  . \n",
      "Predict2:< And I I I , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 48s (- -19m 12s), Epoch: [2/10], Step: [800/3335], Train Loss: 3.5381688833236695, BLEU: 0.20985440500506344\n",
      "\n",
      "Input1:> 今天 我 站 在 这里 是因为 因为 我 是 个 模特\n",
      "\n",
      "Target1:= And I am on this stage because I am a model  . \n",
      "Predict1:< I I I I , I I I I  .  .  .\n",
      "\n",
      "Input2:> 所有 这些 受 海啸 影响 的 照片 在 盐水 咸咸的 海水 里 浸泡 受 细菌 侵蚀 又 埋没 在 垃圾 里 有时 甚至 被 油污 浸染 所有 这些 经过 一段 一段时间 段时间 时间 都 会 持续 破坏 这些\n",
      "\n",
      "Target2:= Now , all these photos had been damaged by water , submerged in salt water , covered in bacteria , in sewage , sometimes even in oil , all of which over time is going to continue to \n",
      "Predict2:< And the the , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 54s (- -19m 7s), Epoch: [2/10], Step: [820/3335], Train Loss: 3.5082306385040285, BLEU: 0.21460276318429625\n",
      "\n",
      "Input1:> 第一 第一个 一个 问题 是 你 是 怎么 成为 模特 的\n",
      "\n",
      "Target1:= So the first question is , how do you become a model  ? \n",
      "Predict1:< It is is is you you , you you  .  .\n",
      "\n",
      "Input2:> 但是 如果 我们 不 改变 土壤 的 成分 我们 永远 不会 这样 做\n",
      "\n",
      "Target2:= but if we don t change the composition of the soil , we will never do this  . \n",
      "Predict2:< But we we we we we we we we we we we we we\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18m 59s (- -19m 1s), Epoch: [2/10], Step: [840/3335], Train Loss: 3.428694951534271, BLEU: 0.20737469288486213\n",
      "\n",
      "Input1:> 他 很 愿意 了解 我 的 家庭 我 的 童年 我 的 愿望 和 梦想\n",
      "\n",
      "Target1:= He wanted to know everything about my family and my childhood and my hopes and dreams  . \n",
      "Predict1:< He was I , , I , I I  .  .  .\n",
      "\n",
      "Input2:> 所以 今天 对 我 来说 变 的 无惧 就 意味 意味着 要 诚实\n",
      "\n",
      "Target2:= So today , for me , being fearless means being honest  . \n",
      "Predict2:< So I I , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 5s (- -20m 55s), Epoch: [2/10], Step: [860/3335], Train Loss: 3.5138978123664857, BLEU: 0.21293048693164404\n",
      "\n",
      "Input1:> 我们 就是 土壤\n",
      "\n",
      "Target1:= We are the soil  . \n",
      "Predict1:< We is we  .  .  .\n",
      "\n",
      "Input2:> 当时 我 以为 我 只是 和 我 的 家人 分开 很 短 一段 一段时间 段时间 时间\n",
      "\n",
      "Target2:= But I only thought that I would be separated from my family for a short time  . \n",
      "Predict2:< I I I I , I , I , I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 10s (- -20m 50s), Epoch: [2/10], Step: [880/3335], Train Loss: 3.720611560344696, BLEU: 0.21845837235087742\n",
      "\n",
      "Input1:> 但是 我 那 受过 教育 的 母亲 成为 为了 一名 名教 教师\n",
      "\n",
      "Target1:= But my educated mother became a teacher  . \n",
      "Predict1:< But I I I to , ,  .  .  .\n",
      "\n",
      "Input2:> CA 会 是 一场 UNK Richard UNK 你 与众不同 不同\n",
      "\n",
      "Target2:= In the trenches  . Richard UNK , you are something else  . \n",
      "Predict2:< It is , , , , you  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 16s (- -20m 44s), Epoch: [2/10], Step: [900/3335], Train Loss: 3.758130359649658, BLEU: 0.2121875651143952\n",
      "\n",
      "Input1:> 所以 现在 的 沥青 有 一些 些小 钢丝 丝绒 在 里面\n",
      "\n",
      "Target1:= So then you have asphalt with very small pieces of steel wool in it  . \n",
      "Predict1:< So , is is the the the the  .  .\n",
      "\n",
      "Input2:> 我 朋友 跟 我 在 一起\n",
      "\n",
      "Target2:= My friend got to come with me  . \n",
      "Predict2:< I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 21s (- -20m 39s), Epoch: [2/10], Step: [920/3335], Train Loss: 3.5878894209861754, BLEU: 0.21045803179139827\n",
      "\n",
      "Input1:> 而是 我 要 被 停 几次 我 什么 时候 会 被 停\n",
      "\n",
      "Target1:= but &quot; How many times will I get stopped  ? When will I get stopped  ? &quot; \n",
      "Predict1:< And I I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 所以 科学 科学家 学家 和 工程 工程师 们 请 跟 我们 说 说 高科技 科技 是 什么 吧\n",
      "\n",
      "Target2:= So scientists and engineers , please , talk nerdy to us  . \n",
      "Predict2:< So , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 27s (- -20m 33s), Epoch: [2/10], Step: [940/3335], Train Loss: 3.426309847831726, BLEU: 0.2153708925520526\n",
      "\n",
      "Input1:> 非法 非法性 性交 性交易 交易 是 我们 在 听到 奴役 时 最常 联想 想到 的 因为 全球 皆知 我 曾 被 警告 说 若 我 坚持 拍摄 这个 行业 人身 人身安全 安全 将 难以 得到 保障\n",
      "\n",
      "Target1:= Sex trafficking is what we often think of when we hear the word slavery , and because of this worldwide awareness , I was warned that it would be difficult for me to work safely within this particular \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他们 不断 地 发现 UNK 照片 相册 相机 和 SD 储存 UNK\n",
      "\n",
      "Target2:= They d been finding photos and photo albums and cameras and SD cards  . \n",
      "Predict2:< They they they , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 32s (- -20m 28s), Epoch: [2/10], Step: [960/3335], Train Loss: 3.3840793490409853, BLEU: 0.22332763472766545\n",
      "\n",
      "Input1:> 因此 我 今天 站 在 这里 流 着 阿富汗 的 血 以及 上帝 授予 我 的 乡土 作风 一个 无神 无神论 无神论者 神论 神论者 论者 和 一个 激进 的 政治 艺术 艺术家 术家 并且 在 过去 九年 一直\n",
      "\n",
      "Target1:= And so I stand here today , Afghan by blood , redneck by the grace of God , an atheist and a radically politicized artist who is been living , working and creating in Afghanistan for the last \n",
      "Predict1:< So I I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 我们 就是 土壤\n",
      "\n",
      "Target2:= We are the soil  . \n",
      "Predict2:< We is we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 38s (- -20m 22s), Epoch: [2/10], Step: [980/3335], Train Loss: 3.5230902314186094, BLEU: 0.2248956039275114\n",
      "\n",
      "Input1:> 所以 他们 给 他 一个 他们 能够 想到 的 最低 低微 的 最 微不足道 不足 不足道 的 最为 可悲 的 头衔\n",
      "\n",
      "Target1:= So they wanted to give him the UNK , UNK , most pathetic title that they could think of  . \n",
      "Predict1:< So they they they to , , , , , , , they , , they  .  .\n",
      "\n",
      "Input2:> 我 正在 用电 电脑 工作 想要 完成 自由 自由职业 职业 撰稿 的 任务 当时 我 有些 烦躁 康 纳 以 我 的 愤怒 为 借口 用 双手 UNK 我 的 脖子 死死 UNK 掐着 让 我 无法 呼吸\n",
      "\n",
      "Target2:= I was working on my computer trying to finish a freelance writing assignment , and I got frustrated , and UNK used my anger as an excuse to put both of his hands around my neck and to \n",
      "Predict2:< I I I I I I , I , , I , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 43s (- -20m 17s), Epoch: [2/10], Step: [1000/3335], Train Loss: 3.5231069207191466, BLEU: 0.21881258744865184\n",
      "\n",
      "Input1:> 如果 这些 人 要为 自己 争取 取自 自由 奴隶 奴隶主 就 烧掉 他们 所有 的 房子\n",
      "\n",
      "Target1:= When these villagers claimed their freedom , the slaveholders burned down all of their houses  . \n",
      "Predict1:< If if if are , , , , , , , , , ,\n",
      "\n",
      "Input2:> 说 你 长大 后 想 当 模特 就 像 你 说 你 长大 后 想 赢 彩票 一样\n",
      "\n",
      "Target2:= Saying that you want to be a model when you grow up is akin to saying that you want to win the Powerball when you grow up  . \n",
      "Predict2:< You you you you you , you , , you , , you , you , you , you you you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 49s (- -20m 11s), Epoch: [2/10], Step: [1020/3335], Train Loss: 3.454880428314209, BLEU: 0.2175306490275659\n",
      "\n",
      "Input1:> 这 是 菜园 在 街上 的 原因\n",
      "\n",
      "Target1:= This is on the street for a reason  . \n",
      "Predict1:< This is is the the  .  .\n",
      "\n",
      "Input2:> 我们 在 纽约 的 地铁 上 碰巧 坐在 一起 他 先 跟 我 聊 了 起来\n",
      "\n",
      "Target2:= He sat next to me on the New York City subway , and he started chatting me up  . \n",
      "Predict2:< We I the , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19m 54s (- -20m 6s), Epoch: [2/10], Step: [1040/3335], Train Loss: 3.6200636863708495, BLEU: 0.21777467886480956\n",
      "\n",
      "Input1:> 啊 这个 我 办不到 不到 那 要 我 替 你 找 人 帮忙 吗\n",
      "\n",
      "Target1:= &quot; Oh , no , I cannot do this  . &quot; &quot; Would you like me to find you somebody  ? &quot; \n",
      "Predict1:< And , I I , , , I , I I I I I  .\n",
      "\n",
      "Input2:> 这些 井 有 差不多 不多 有 90 多米 深 他们 抬出 抬出来 出来 装满 UNK 的 袋子 这些 石头 被 运送 送到 另外 一个 区域 被 捣碎 以便 提取 金子\n",
      "\n",
      "Target2:= The shafts are up to 300 feet deep , and they carry out heavy bags of stone that later will be transported to another area , where the stone will be pounded so that they can extract the \n",
      "Predict2:< And they are , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 0s (- -20m 0s), Epoch: [2/10], Step: [1060/3335], Train Loss: 3.623232400417328, BLEU: 0.2225983604802066\n",
      "\n",
      "Input1:> 大概 是 和 言语 语词 词汇 相关 的 词汇 我 假设\n",
      "\n",
      "Target1:= Something to do with words , I assume  . \n",
      "Predict1:< And is , , , , , ,  .  .  .  .\n",
      "\n",
      "Input2:> 错 他们 都 是 奴隶\n",
      "\n",
      "Target2:= Wrong  . They were all enslaved  . \n",
      "Predict2:< And they they  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 5s (- -21m 55s), Epoch: [2/10], Step: [1080/3335], Train Loss: 3.5490960836410523, BLEU: 0.22710038515272643\n",
      "\n",
      "Input1:> 我们 拨开 开运 运河 和 水沟 沟里 的 瓦砾\n",
      "\n",
      "Target1:= We pulled debris from canals and ditches  . \n",
      "Predict1:< We we we to the the  .  .\n",
      "\n",
      "Input2:> 我 想 跟 大家 分享 一些 人们 在 那 面墙 墙上 写 的 东西\n",
      "\n",
      "Target2:= And I d like to share a few things that people wrote on this wall  . \n",
      "Predict2:< I I to to to to to to to to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 11s (- -21m 49s), Epoch: [2/10], Step: [1100/3335], Train Loss: 3.510421073436737, BLEU: 0.22558371914286904\n",
      "\n",
      "Input1:> 一年 之内 我 做 了 27 个 项目 政府 的 人 跑 来 问 我 你 是 怎么 怎么办 办到 的\n",
      "\n",
      "Target1:= In a year , I had 27 projects going on , and the government came to see me to say , &quot; How can you do that  ? \n",
      "Predict1:< And I I I , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 术语 是 我们 理解 你们 想法 的 一个 障碍\n",
      "\n",
      "Target2:= Jargon is a barrier to our understanding of your ideas  . \n",
      "Predict2:< And is is is is , is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 17s (- -21m 44s), Epoch: [2/10], Step: [1120/3335], Train Loss: 3.4698970317840576, BLEU: 0.2198693150007524\n",
      "\n",
      "Input1:> 我们 拨开 开运 运河 和 水沟 沟里 的 瓦砾\n",
      "\n",
      "Target1:= We pulled debris from canals and ditches  . \n",
      "Predict1:< We we we the the the  .  .\n",
      "\n",
      "Input2:> 我们 把 书 放在 食品 杂货 袋中 这样 别人 就 会 以为 我们 只是 要 去 购物\n",
      "\n",
      "Target2:= We would cover our books in grocery bags so it would seem we were just out shopping  . \n",
      "Predict2:< We we we to to to , , we , , we , we , we ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 22s (- -21m 38s), Epoch: [2/10], Step: [1140/3335], Train Loss: 3.495111036300659, BLEU: 0.20497923995575965\n",
      "\n",
      "Input1:> 在 死 之前 我 想 种 一棵 一棵树\n",
      "\n",
      "Target1:= &quot; Before I die , I want to plant a tree  . &quot; \n",
      "Predict1:< And I I I I I I I  .  .\n",
      "\n",
      "Input2:> 谢谢 你 Tom 掌声\n",
      "\n",
      "Target2:= Thanks , Tom  . \n",
      "Predict2:< Thank you you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 27s (- -21m 33s), Epoch: [2/10], Step: [1160/3335], Train Loss: 3.4560638070106506, BLEU: 0.21734425081743697\n",
      "\n",
      "Input1:> 从 他 记事 开始 就 被迫 迫在 湖上 上工 工作\n",
      "\n",
      "Target1:= For as long as he can recall , he is been forced to work on the lake  . \n",
      "Predict1:< And he , , , , , ,  .  .\n",
      "\n",
      "Input2:> 赞比亚 比亚 人 说 没错 这 就是 我们 不在 这里 耕种 的 原因\n",
      "\n",
      "Target2:= And the UNK said , &quot; Yes , that is why we have no agriculture here  . &quot; \n",
      "Predict2:< And , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 33s (- -21m 27s), Epoch: [2/10], Step: [1180/3335], Train Loss: 3.3580992221832275, BLEU: 0.21929150496323563\n",
      "\n",
      "Input1:> 我们 做 过 调查 对 世界 上 100 家 标杆 企业 进行 研究 卡内 卡内基 培训 公司 西屋 公司 爱迪生 公司 福特 福特汽车公司 汽车 公司 以及 所有 这些 新兴 新兴产业 产业 的 公司 如 谷 歌 雅虎\n",
      "\n",
      "Target1:= We ve done the research , and we have looked at the 100 iconic companies of the world -- Carnegie , Westinghouse , Edison , Ford , all the new companies , Google , Yahoo  . \n",
      "Predict1:< We we we to the , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 除了 经常 经常性 常性 的 发脾气 脾气 沮丧 和 永无 永无休止 无休 无休止 休止 的 多 动 还有 一点 特别 特别之处 之处 就是 他 纯真 UNK 无邪 的 天性 不 带 偏见 地 看 世界 他 是 个\n",
      "\n",
      "Target2:= Yet beyond the tantrums and the frustration and the never-ending hyperactivity was something really unique : a pure and innocent nature , a boy who saw the world without prejudice , a human who had never lied  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 39s (- -21m 21s), Epoch: [2/10], Step: [1200/3335], Train Loss: 3.4656935334205627, BLEU: 0.21984663882863328\n",
      "\n",
      "Input1:> 我 三岁 的 时候 弟弟 出生 了 我 非常 常兴 兴奋 我 有 了 新 的 家人\n",
      "\n",
      "Target1:= I was just three years old when my brother came along , and I was so excited that I had a new being in my life  . \n",
      "Predict1:< I I I I I , I I I I I I  .  .\n",
      "\n",
      "Input2:> 公共 空间 可以 更好 的 体现 到底 什么 对 我们 是 真正 重要 的 无论 无论是 对 个人 来说 或者 对于 整个 社区 来说 有 了 更多 的 方式 来 分享 我们 的 希望 恐惧 和 经历 我们\n",
      "\n",
      "Target2:= Our shared spaces can better reflect what matters to us as individuals and as a community , and with more ways to share our hopes , fears and stories , the people around us can not only help \n",
      "Predict2:< And we we we , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 44s (- -21m 16s), Epoch: [2/10], Step: [1220/3335], Train Loss: 3.4209280133247377, BLEU: 0.2280413615840514\n",
      "\n",
      "Input1:> 事实 事实上 这些 肌肉 强健 的 男人 若干 若干年 年后 很 有 可能 会 跟 这个 个人 一样 被 汞 中毒 和 肺结核 结核 苦苦 折磨\n",
      "\n",
      "Target1:= In fact , it is very likely that this muscular person will end up like this one here , racked with tuberculosis and mercury poisoning in just a few years  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 照片 上 他 正在 UNK 洗澡 把 一桶 一桶桶 水 往 头上 倒 好消息 消息 是 此时 此时此刻 此刻 UNK 已经 和 家人 团聚 了 更让人 欣喜 的 是 他们 一家 已 能够 自己 谋生 并 确保 孩子\n",
      "\n",
      "Target2:= Here he is seen taking a bath at the well , pouring big buckets of water over his head , and the wonderful news is , as you and I are sitting here talking today , Kofi has \n",
      "Predict2:< And he , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 50s (- -21m 10s), Epoch: [2/10], Step: [1240/3335], Train Loss: 3.552421677112579, BLEU: 0.2259586339607893\n",
      "\n",
      "Input1:> 子弹 是 干什么 什么 的 子弹 能 杀人 也 能 灭 了 你 的 演示 报告\n",
      "\n",
      "Target1:= What do bullets do  ? UNK kill , and they will kill your presentation  . \n",
      "Predict1:< And is is is you , you , you you you you\n",
      "\n",
      "Input2:> 今天 我 做到 了\n",
      "\n",
      "Target2:= And here I am today  . \n",
      "Predict2:< I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20m 55s (- -21m 5s), Epoch: [2/10], Step: [1260/3335], Train Loss: 3.603846085071564, BLEU: 0.2204901936310942\n",
      "\n",
      "Input1:> 在 我 交 了 罚金 金和 贿赂 用 的 钱 后 我 的 家人 一个 一个月 后 被 释放 了 但 不久 后 他们 在 老挝 的 首都 又 被 抓起 起来 了\n",
      "\n",
      "Target1:= After I paid the fine and bribe , my family was released in one month , but soon after , my family was arrested and jailed again in the capital of Laos  . \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 现在 从 这个 故事 里 你 能够 了解 到 三件 有趣 的 事情\n",
      "\n",
      "Target2:= Now you can learn three interesting things from this  . \n",
      "Predict2:< Now , is , you , you , you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 1s (- -22m 59s), Epoch: [2/10], Step: [1280/3335], Train Loss: 3.5228471994400024, BLEU: 0.2180896530119801\n",
      "\n",
      "Input1:> 我 记 不 起 第一 第一次 一次 知道 孤独 孤独症 这个 词 的 情形 我 也 记 不 起 有 哪一天 一天 我 是 对 孤独 孤独症 一无所知 无所 所知 的\n",
      "\n",
      "Target1:= And I cannot remember the first moment I encountered autism , but I cannot recall a day without it  . \n",
      "Predict1:< I I I I , I , I , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 事实 事实上 这些 肌肉 强健 的 男人 若干 若干年 年后 很 有 可能 会 跟 这个 个人 一样 被 汞 中毒 和 肺结核 结核 苦苦 折磨\n",
      "\n",
      "Target2:= In fact , it is very likely that this muscular person will end up like this one here , racked with tuberculosis and mercury poisoning in just a few years  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 6s (- -22m 54s), Epoch: [2/10], Step: [1300/3335], Train Loss: 3.5175101161003113, BLEU: 0.22190520831066707\n",
      "\n",
      "Input1:> 让 人 震惊 的 是 奴隶 在 全球 产生 的 利润 每年 超过 130 亿美金 美金\n",
      "\n",
      "Target1:= Astonishingly , slavery generates profits of more than $ 13 billion worldwide each year  . \n",
      "Predict1:< The is is the the the the the the the the the\n",
      "\n",
      "Input2:> 上面 写道 当 你 读到 这 封信 时 我家 的 所有 五个 个人 都 将 离开 人世 因为 我们 已经 连续 两周 没有 吃 东西 了\n",
      "\n",
      "Target2:= It read , &quot; When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 12s (- -22m 48s), Epoch: [2/10], Step: [1320/3335], Train Loss: 3.4965953707695006, BLEU: 0.22122069997208588\n",
      "\n",
      "Input1:> 我 这里 有 一个 关于 声 UNK 颤动 的 视频\n",
      "\n",
      "Target1:= So here we have a video of the vibrating vocal folds  . \n",
      "Predict1:< I I I a a a a  .  .  .\n",
      "\n",
      "Input2:> 从 他 记事 开始 就 被迫 迫在 湖上 上工 工作\n",
      "\n",
      "Target2:= For as long as he can recall , he is been forced to work on the lake  . \n",
      "Predict2:< And he , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 17s (- -22m 43s), Epoch: [2/10], Step: [1340/3335], Train Loss: 3.5607233881950378, BLEU: 0.22627658006265117\n",
      "\n",
      "Input1:> 他 到 家 时 电话 电话响 了 一个 声音 警告 他 如果 他 再 将 女儿 送去 学校 刚才 的 事情 就 会 再次 再次发生 发生\n",
      "\n",
      "Target1:= As he arrived home , the phone rang , a voice warning him that if he sent his daughter back to school , they would try again  . \n",
      "Predict1:< He he he , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 现在 的 我\n",
      "\n",
      "Target2:= And here is me today  . \n",
      "Predict2:< This is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 23s (- -22m 37s), Epoch: [2/10], Step: [1360/3335], Train Loss: 3.4644684314727785, BLEU: 0.22358447507467707\n",
      "\n",
      "Input1:> 但 我 转而 又 想到 他们 教 我 的 那些 事 关于 个性 交流 和 爱 我 意识 到 这些 东西 我 可不 愿意 用 正常 去 换\n",
      "\n",
      "Target1:= But I cast my mind back to the things that they ve taught me about individuality and communication and love , and I realize that these are things that I wouldn t want to change with normality  . \n",
      "Predict1:< But I I I I , , I , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target2:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict2:< And , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 28s (- -22m 32s), Epoch: [2/10], Step: [1380/3335], Train Loss: 3.487239670753479, BLEU: 0.23173738854437662\n",
      "\n",
      "Input1:> 一支 放在 床 的 枕头 下面 第三 第三支 三支 则 一直 放在 口袋 里\n",
      "\n",
      "Target1:= He kept one under the pillows on our bed , and the third one he kept in his pocket at all times  . \n",
      "Predict1:< And is , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 我们 需要 去 和 整个 地区 的 政府 们 打交道 交道\n",
      "\n",
      "Target2:= We need to work with governments in regional levels  . \n",
      "Predict2:< We we to to to the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 34s (- -22m 26s), Epoch: [2/10], Step: [1400/3335], Train Loss: 3.6136677980422975, BLEU: 0.22721186552498407\n",
      "\n",
      "Input1:> 我 厌倦 了 这些 现实\n",
      "\n",
      "Target1:= I got tired of seeing this happening  . \n",
      "Predict1:< I I I to  .  .  .\n",
      "\n",
      "Input2:> 因此 我 做 的 就是 在 我 的 房子 前 种 上 食物 森林\n",
      "\n",
      "Target2:= So what I did , I planted a food forest in front of my house  . \n",
      "Predict2:< So I I I I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 39s (- -22m 21s), Epoch: [2/10], Step: [1420/3335], Train Loss: 3.668859434127808, BLEU: 0.21967404022085546\n",
      "\n",
      "Input1:> 她 也 开始 始自 自己 亲手 清洗 UNK 照片 直到 她 意识 到 她 自己 清洗 其实 对照 照片 造成 了 更多 损害\n",
      "\n",
      "Target1:= She had started UNK them herself and stopped when she realized she was doing more damage  . \n",
      "Predict1:< She she , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 一点 一点儿 点儿 也 不明 明白 UNK 是 什么 意思\n",
      "\n",
      "Target2:= Now I have no idea what &quot; UNK &quot; is  . \n",
      "Predict2:< I I t to to , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 45s (- -22m 15s), Epoch: [2/10], Step: [1440/3335], Train Loss: 3.3869709253311155, BLEU: 0.2310701628510621\n",
      "\n",
      "Input1:> 她 的 名字 叫 琼 对 我 来讲 她 就 像 我 的 母亲 一样 她 死 得 很 突然 没有 有人 预料 预料到 料到\n",
      "\n",
      "Target1:= Her name was Joan , and she was a mother to me , and her death was sudden and unexpected  . \n",
      "Predict1:< She she , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 猜到 事情 是 怎么 发展 的\n",
      "\n",
      "Target2:= Well you can see where this is going  . \n",
      "Predict2:< You can you you you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 50s (- -22m 10s), Epoch: [2/10], Step: [1460/3335], Train Loss: 3.4813894867897033, BLEU: 0.22731497193878988\n",
      "\n",
      "Input1:> 我们 有志 志愿 志愿者 愿者 团队 帮助 创业 辅助 商 为 客户 集结 资源 及 人力 我们 发现 当地 当地人 人群 的 智慧 能 创造 造出 出奇 奇迹 能 改变 社区 的 文化 和 经济 只要 你 能\n",
      "\n",
      "Target1:= We have groups of volunteers supporting the Enterprise UNK to help you to find resources and people and we have discovered that the miracle of the intelligence of local people is such that you can change the culture \n",
      "Predict1:< We we we to , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 在 西澳 西澳大利亚 澳大利 澳大利亚 大利 利亚 利亚州 的 埃斯 UNK 兰斯 进行 了 试验\n",
      "\n",
      "Target2:= I started this as a UNK in UNK , in Western Australia  . \n",
      "Predict2:< I I I , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 21m 56s (- -22m 4s), Epoch: [2/10], Step: [1480/3335], Train Loss: 3.591507375240326, BLEU: 0.22850351567537083\n",
      "\n",
      "Input1:> 突然 间 我 似乎 无法 骄傲 骄傲地 称 任何 一个 国家 为 我 自己 的 国家\n",
      "\n",
      "Target1:= Suddenly , there was no country I could proudly call my own  . \n",
      "Predict1:< And I I I , , , I , , I , I I  .\n",
      "\n",
      "Input2:> 我 的 父亲 在 用 他 的 灰色 小 收音 收音机 听 BBC 新闻\n",
      "\n",
      "Target2:= My father was listening to BBC News on his small , gray radio  . \n",
      "Predict2:< I I , , , , , , the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 1s (- -23m 59s), Epoch: [2/10], Step: [1500/3335], Train Loss: 3.5478954911231995, BLEU: 0.22095777540610825\n",
      "\n",
      "Input1:> 我们 是 一个 自由 的 组织 由来 来自 不同 同行 同行业 行业 不同 同城 城市 的 园丁 组成 大家 全都 是 志愿 志愿者 愿者 我们 所 做 的 一切 一切都是 义务 的\n",
      "\n",
      "Target1:= What we do , we are a UNK kind of group , where it is composed of gardeners from all walks of life , from all over the city , and it is completely volunteer , and everything \n",
      "Predict1:< We is a a a , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但是 如果 我们 不 改变 土壤 的 成分 我们 永远 不会 这样 做\n",
      "\n",
      "Target2:= but if we don t change the composition of the soil , we will never do this  . \n",
      "Predict2:< But we we we we we , we we we we we we\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 7s (- -23m 53s), Epoch: [2/10], Step: [1520/3335], Train Loss: 3.518847572803497, BLEU: 0.22298646519336335\n",
      "\n",
      "Input1:> 我 不知 知道 你们 的 情况 但 我 个人 而言 除了 这 15 张 照片 我 不会 经常 经常出现 出现 现在 在家 家庭 照片 里面\n",
      "\n",
      "Target1:= I don t know about you , but aside from these 15 shots , I am not in many of the family photos  . \n",
      "Predict1:< I I I to to , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 甚至 将 我 的 朋友 带回 我 的 社区 帮助 他们 在 没有 赶 狮 灯 的 人家 家里 里安 安装 这个 我 教 他们 怎么 安装\n",
      "\n",
      "Target2:= I even took my friends back to my community , and we are installing the lights to the homes which don t have &#91; any &#93; , and I am teaching them how to put them  . \n",
      "Predict2:< I I I to to , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 12s (- -23m 48s), Epoch: [2/10], Step: [1540/3335], Train Loss: 3.6335194945335387, BLEU: 0.22407311591509174\n",
      "\n",
      "Input1:> 十几 十几岁 几岁 时 我 跟 朋友 一起 起开 开车 她 是 个 糟糕 的 司机 闯 了 红灯 我们 被 停车 我 只 说 了 句 长官 抱歉 我们 就 继续 上路 了\n",
      "\n",
      "Target1:= When I was a teenager , I was driving with my friend who was an awful driver and she ran a red and of course , we got pulled over , and all it took was a &quot; \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 记得 得当 当时 站 在 几乎 是 一片 黑暗 当中 我 刹那 刹那间 那间 感到 一阵 强烈 的 恐惧 那一刻 一刻 我 在 想 被困 在 这个 地狱 里 是 多么 的 恐怖\n",
      "\n",
      "Target2:= Standing in the near darkness , I remember feeling this quick , hot fear , and in that instant , I could only imagine what it must be like to be trapped in that hell  . \n",
      "Predict2:< And was , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 18s (- -23m 42s), Epoch: [2/10], Step: [1560/3335], Train Loss: 3.563645601272583, BLEU: 0.22572373345769012\n",
      "\n",
      "Input1:> 那些 大石 UNK 石板 比 搬运 石板 的 孩子 还要 重 孩子 们 用 树枝 绳子 破烂 的 衣服 制成 背带 把 石板 绑 在 自己 己身 身上\n",
      "\n",
      "Target1:= The big sheets of slate were heavier than the children carrying them , and the kids hoisted them from their heads using these handmade harnesses of sticks and rope and torn cloth  . \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 我 的 信条 我 告诉 人们 种植 你们 自己 的 食物 吧\n",
      "\n",
      "Target2:= It is my gospel , when I am telling people , grow your own food  . \n",
      "Predict2:< And is is I I I I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 23s (- -23m 37s), Epoch: [2/10], Step: [1580/3335], Train Loss: 3.493287992477417, BLEU: 0.237548071452858\n",
      "\n",
      "Input1:> 这些 人 被 奴役 在 加纳 的 另 一处 矿井 里\n",
      "\n",
      "Target1:= These miners are enslaved in a mine shaft in another part of Ghana  . \n",
      "Predict1:< And are the the the the the the  .\n",
      "\n",
      "Input2:> 因此 科学 科学家 学家 和 工程 工程师 们 当 你们 解决 了 这个 方程 方程式 程式 以后 就 想尽 一切 一切办法 办法 跟 我 说 说 高科技 科技 吧\n",
      "\n",
      "Target2:= And so , scientists and engineers , when you ve solved this equation , by all means , talk nerdy to me  . \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 29s (- -23m 31s), Epoch: [2/10], Step: [1600/3335], Train Loss: 3.527575957775116, BLEU: 0.2261061609056384\n",
      "\n",
      "Input1:> 我 当时 就 意识 到 那 是 我 人生 很 有 象征 象征意义 意义 的 一个 时刻\n",
      "\n",
      "Target1:= I realized that this was a symbolic moment in my life  . \n",
      "Predict1:< I I I I , I , I , I I I  .  .  .  .\n",
      "\n",
      "Input2:> 我 以为 我 这辈子 辈子 完 了 尽管 尽管如此 如此 我 还是 努力 隐藏 了 自己 的 恐惧 和 担忧 并 回答 了 他们 的 问题\n",
      "\n",
      "Target2:= I thought my life was over , but I managed to control all the emotions inside me and answer the questions  . \n",
      "Predict2:< I I I I to , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 34s (- -23m 26s), Epoch: [2/10], Step: [1620/3335], Train Loss: 3.4237345337867735, BLEU: 0.220971970446168\n",
      "\n",
      "Input1:> 我 并不知道 不知 知道 该 从 这个 实验 里 期待 些 什么 但是 第二 第二天 二天 整个 墙壁 都 被 填满 了 而且 不断 有人 添加 新 的 答案\n",
      "\n",
      "Target1:= I didn t know what to expect from this experiment , but by the next day , the wall was entirely filled out , and it kept growing  . \n",
      "Predict1:< I I to to to to , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 要 回答 下人 人们 常常 问 我 的 那些 问题 以 一种 诚实 的 方式\n",
      "\n",
      "Target2:= And I am going to answer the questions that people always ask me , but with an honest twist  . \n",
      "Predict2:< I I I to to to to to to to , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 40s (- -23m 20s), Epoch: [2/10], Step: [1640/3335], Train Loss: 3.3108620047569275, BLEU: 0.22353979962687973\n",
      "\n",
      "Input1:> 那时 我们 并不知道 不知 知道 这样 的 旅程 会 深深 改变 我们 的 生活\n",
      "\n",
      "Target1:= Back then , we had no idea how much this trip would change our lives  . \n",
      "Predict1:< And we we we we we we we we we we we we we\n",
      "\n",
      "Input2:> 在 这 张 照片 里 我 事实 事实上 在 现实 现实生活 生活 中 根本 没有 男朋友 朋友\n",
      "\n",
      "Target2:= And in this picture , I had actually never had a boyfriend in real life  . \n",
      "Predict2:< And is , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 45s (- -23m 15s), Epoch: [2/10], Step: [1660/3335], Train Loss: 3.464619541168213, BLEU: 0.2260895606321329\n",
      "\n",
      "Input1:> 现实 实是 当 你 毕业 后 有 了 简历 做 过 了 几份 工作 你 就 不能 再说 其它 的 了 所以 如果 你 说 你 想成 成为 美国 总统 但 你 的 简历 上 写 着 有\n",
      "\n",
      "Target1:= Unfortunately after you ve gone to school , and you have a résumé and you ve done a few jobs , you can t say anything anymore , so if you say you want to be the President \n",
      "Predict1:< And you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 死 之前 我 想 在 上百 上百万 百万 的 观众 面前 唱歌\n",
      "\n",
      "Target2:= &quot; Before I die , I want to sing for millions  . &quot; \n",
      "Predict2:< And I I , I , , I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 51s (- -23m 9s), Epoch: [2/10], Step: [1680/3335], Train Loss: 3.5175366401672363, BLEU: 0.22609688660215227\n",
      "\n",
      "Input1:> Max 靠着 这些 采样 不妨 说 在 一万 人 中 你 能够 辨别 出 谁 是 健康 的 谁 是 患病 的\n",
      "\n",
      "Target1:= So Max , by taking all these samples of , let is say , 10,000 people , you ll be able to tell who is healthy and who is not  ? \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 能够 结束 自己 疯狂 的 爱 的 故事 靠 的 是 打破 沉默\n",
      "\n",
      "Target2:= I was able to end my own crazy love story by breaking the silence  . \n",
      "Predict2:< I I to to to the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22m 56s (- -23m 3s), Epoch: [2/10], Step: [1700/3335], Train Loss: 3.4650726675987245, BLEU: 0.22411856801527552\n",
      "\n",
      "Input1:> 因此 科学 科学家 学家 和 工程 工程师 们 当 你们 解决 了 这个 方程 方程式 程式 以后 就 想尽 一切 一切办法 办法 跟 我 说 说 高科技 科技 吧\n",
      "\n",
      "Target1:= And so , scientists and engineers , when you ve solved this equation , by all means , talk nerdy to me  . \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 否则 照片 里 她 和 她 先生 的 脸部 就 无法 完全 修复 了 我们 也 因此 得以 把 所有 部分 整合 合到 一张 好 的 相片 里 然后 重新 新制 制作 这 张 照片\n",
      "\n",
      "Target2:= Areas like her husband and her face , which otherwise would have been completely impossible to fix , we could just put them together in one good photo , and remake the whole photo  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 2s (- -24m 58s), Epoch: [2/10], Step: [1720/3335], Train Loss: 3.5197460532188414, BLEU: 0.2310904296398356\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 全世界 世界 都 将 会 有 很大 的 损失 如果 失去 了 这些 奇妙 的 物种\n",
      "\n",
      "Target1:= Kenya , this world , will be much poorer without these wonderful species  . \n",
      "Predict1:< And the , the the , , , , , , the the the\n",
      "\n",
      "Input2:> 而 如果 这些 照片 不先 清洗 干净 我们 也 无法 开展 修复 修复 前 它们 还 必须 进行 干燥 认领 完毕\n",
      "\n",
      "Target2:= We couldn t retouch the photo unless it was cleaned , dry and reclaimed  . \n",
      "Predict2:< And if we , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 7s (- -24m 52s), Epoch: [2/10], Step: [1740/3335], Train Loss: 3.6093155264854433, BLEU: 0.2169550362610185\n",
      "\n",
      "Input1:> 我 和 一个 温柔 UNK 善良 男人 再次 结婚 有 了 三个 孩子\n",
      "\n",
      "Target1:= I remarried a kind and gentle man , and we have those three kids  . \n",
      "Predict1:< I I to a a a a a a  .  .  .\n",
      "\n",
      "Input2:> 和 其他 两千 六百 六百五十 百五 百五十 五十 五十万 十万 的 美国 国人 一样 我 生活 在 食物 的 荒漠 洛杉矶 UNK 是 得来 速 快餐 商业 商业模式 模式 顾客 不用 下车 即可 可获 获得 服务 和 飞车\n",
      "\n",
      "Target2:= Just like UNK million other Americans , I live in a food desert , South Central Los Angeles , home of the drive-thru and the UNK  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 13s (- -24m 47s), Epoch: [2/10], Step: [1760/3335], Train Loss: 3.3871394872665403, BLEU: 0.21655152264280436\n",
      "\n",
      "Input1:> 这 就是 我 成为 UNK 的 全球 大使 的 原因 这 是 一个 致力 致力于 女性 教育 的 全球 组织\n",
      "\n",
      "Target1:= That is why I am a global ambassador for UNK , a global campaign to educate women  . \n",
      "Predict1:< This is is a a a , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 这 就是 刚才 我 拿出 出来 的 枪 的 用途\n",
      "\n",
      "Target2:= And here is where those guns come in  . \n",
      "Predict2:< This is is I I I  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 18s (- -24m 41s), Epoch: [2/10], Step: [1780/3335], Train Loss: 3.3616899728775023, BLEU: 0.2234497469035113\n",
      "\n",
      "Input1:> 你 可以 又 一次 看到 波纹 在 桌子 上 展开 背景 是 西红柿 和 墙\n",
      "\n",
      "Target1:= You can watch the ripples again washing over the table , the tomato and the wall in the back  . \n",
      "Predict1:< You can you the the the the the the the the the the the\n",
      "\n",
      "Input2:> 这 让 我们 相信 自己 并 不 孤单\n",
      "\n",
      "Target2:= It is about knowing you are not alone  . \n",
      "Predict2:< And is is we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 24s (- -24m 36s), Epoch: [2/10], Step: [1800/3335], Train Loss: 3.463534212112427, BLEU: 0.2311904864600733\n",
      "\n",
      "Input1:> 如果 你 不是 个 园丁 那 你 就 不是 个 流氓\n",
      "\n",
      "Target1:= If you ain t a gardener , you ain t gangsta  . \n",
      "Predict1:< If you you you you you you you you you you  .\n",
      "\n",
      "Input2:> 而 最近 如果 你 留意 过 肯尼 肯尼亚 尼亚 的 新闻 掌声 欢呼 肯尼 肯尼亚 尼亚 的 国会 国会议员 会议 会议员 议员 们 被 讽刺 为 像 秃鹫 一般 贪婪 和 愚蠢 但是 我 想 UNK 它\n",
      "\n",
      "Target2:= More recently , if you ve been following the Kenyan press — — these are the attributes that they associated the Kenyan MPs with  . But I want to challenge that  . \n",
      "Predict2:< And you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 29s (- -24m 30s), Epoch: [2/10], Step: [1820/3335], Train Loss: 3.4015641808509827, BLEU: 0.22978605039798022\n",
      "\n",
      "Input1:> 这种 人 不存 存在\n",
      "\n",
      "Target1:= It doesn t exist  . \n",
      "Predict1:< It is is  .  .\n",
      "\n",
      "Input2:> 那么 因为 与 我 共事 过 的 工程 工程师 们 教会 我 接触 我 内心 心存 存在 的 学术 狂人 我 想用 一个 方程 方程式 程式 来 总结 一下 我 的 观点\n",
      "\n",
      "Target2:= And because the engineers that I ve worked with have taught me to become really in touch with my inner nerd , I want to summarize with an equation  . \n",
      "Predict2:< So I I I I I I I I I I I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 35s (- -24m 25s), Epoch: [2/10], Step: [1840/3335], Train Loss: 3.3670650124549866, BLEU: 0.2421989931132108\n",
      "\n",
      "Input1:> 秃鹫 还有 非常 多 的 历史 意义\n",
      "\n",
      "Target1:= UNK also have tremendous historical significance  . \n",
      "Predict1:< The is is is  .  .  .\n",
      "\n",
      "Input2:> 现在 我 给 你 展示 一种 新型 照相 技术 叫做 飞秒 成像 这 是 一种 技术 太 快 以至 以至于 至于 它 能 捕捉 捕捉到 捉到 光 的 运动\n",
      "\n",
      "Target2:= I present you a new type of photography , UNK , a new imaging technique so fast that it can create slow motion videos of light in motion  . \n",
      "Predict2:< Now , I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 40s (- -24m 19s), Epoch: [2/10], Step: [1860/3335], Train Loss: 3.466900622844696, BLEU: 0.2233661715459694\n",
      "\n",
      "Input1:> 在 学校 我们 用 很多 时间 来 学习 UNK UNK 的 历史 但是 我们 从来 从来没 没有 对外 外面 的 世界 有 过 太多 了解 除了 知道 美国 韩国 和 日本 是 敌人 外\n",
      "\n",
      "Target1:= In school , we spent a lot of time studying the history of Kim UNK , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies  . \n",
      "Predict1:< In we we we , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 刚 起步 时 并非 单枪 单枪匹马\n",
      "\n",
      "Target2:= He wasn t alone when he started  . \n",
      "Predict2:< He he is a , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 46s (- -24m 14s), Epoch: [2/10], Step: [1880/3335], Train Loss: 3.5745412945747375, BLEU: 0.2268042919937537\n",
      "\n",
      "Input1:> 他们 不想 称 他 为 国王 以防 以防万一 万一 他 或 他 的 继任 继任者 会 产生 什么 想法\n",
      "\n",
      "Target1:= They didn t want to call him King in case that gave him ideas , or his successor ideas  . \n",
      "Predict1:< They said , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 想要 人们 拿走 它 但是 同时 我 也 希望 人们 能够 够重 重归 健康 的 身体\n",
      "\n",
      "Target2:= I want them to take it , but at the same time , I want them to take back their health  . &quot; \n",
      "Predict2:< I I I to to to to to to , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 51s (- -24m 8s), Epoch: [2/10], Step: [1900/3335], Train Loss: 3.4924893021583556, BLEU: 0.23503175251188715\n",
      "\n",
      "Input1:> 湖面 下 浸 着 些 树 的 枝干 经常 常会 挂住 渔网 这些 疲惫 害怕 的 孩子 们 就 被 扔到 水中 去 解开 渔网\n",
      "\n",
      "Target1:= The skeletal tree limbs submerged in Lake Volta often catch the fishing nets , and weary , frightened children are thrown into the water to untether the lines  . \n",
      "Predict1:< The , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 告诉 别人 你 今天 听到 的\n",
      "\n",
      "Target2:= Talk about what you heard here  . \n",
      "Predict2:< You you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23m 57s (- -24m 3s), Epoch: [2/10], Step: [1920/3335], Train Loss: 3.446205401420593, BLEU: 0.22260002938537993\n",
      "\n",
      "Input1:> 事实 事实上 我 要 先 让 它 冷却 下来 才能 向 你们 展示 加热 修复 是否 有效\n",
      "\n",
      "Target1:= Actually , it still has to cool down first before I can show you that the healing works  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 以为 为什么 什么 说 秃鹫 重要 呢 首先 秃鹫 对生 生态 循环 做出 了 至关 至关重要 重要 的 作用 他们 净化 净化环境 环境\n",
      "\n",
      "Target2:= So why are vultures important  ? First of all , they provide vital ecological services  . They clean up  . \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 2s (- -25m 57s), Epoch: [2/10], Step: [1940/3335], Train Loss: 3.4440111756324767, BLEU: 0.23061718737579756\n",
      "\n",
      "Input1:> 所以 我 想 向 你们 提出 一个 非常 非常规 常规 的 的 方法 一个 我们 能 尝试 去 达到 目的 的 方法 因为 你 也 知道 在 某种 某种意义 意义 上 至少 我们 都 是 像 我 朋友\n",
      "\n",
      "Target1:= So what I want to propose to you as an unconventional way in which we can try to achieve this , because , you see , in one sense , at least , we are all virtuosos like \n",
      "Predict1:< So I I I to to to you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 正 因为 有人 表明 了 立场 影响 了 他 的 人生 他 的 未来 才 可能 有所 改变\n",
      "\n",
      "Target2:= Who will he become because someone took a stand and made a difference in his life  ? \n",
      "Predict2:< And is , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 8s (- -25m 52s), Epoch: [2/10], Step: [1960/3335], Train Loss: 3.530017352104187, BLEU: 0.24250603355802075\n",
      "\n",
      "Input1:> 她 不是 朝鲜 朝鲜人\n",
      "\n",
      "Target1:= She is not North Korean  . &quot; \n",
      "Predict1:< She she ,  .  .  .\n",
      "\n",
      "Input2:> 我 看到 轮椅 像 二手 汽车 一样 进行 买卖\n",
      "\n",
      "Target2:= I see wheelchairs bought and sold like used cars  . \n",
      "Predict2:< I I to to to to the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 13s (- -25m 46s), Epoch: [2/10], Step: [1980/3335], Train Loss: 3.5103333830833434, BLEU: 0.22864908943996082\n",
      "\n",
      "Input1:> 所以 掌声 所以 政府 的 人 说 那 接着 做\n",
      "\n",
      "Target1:= So — — So the government says , &quot; Do it again  . &quot; \n",
      "Predict1:< So , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 怎样 更 能 表达 我们 对 空置 的 店面 的 期待 使 我们 的 社区 能 反映 映出 我们 现在 的 需求 求和 梦想\n",
      "\n",
      "Target2:= And how can we share more of our hopes for our vacant storefronts , so our communities can reflect our needs and dreams today  ? \n",
      "Predict2:< And we we we we we we we we we we we we , we we , we ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 19s (- -25m 41s), Epoch: [2/10], Step: [2000/3335], Train Loss: 3.5252883195877076, BLEU: 0.22815701726676113\n",
      "\n",
      "Input1:> 我 的 手 滑 了 一下 立即 想到 数天 前 碰到 的 一名 矿工 他 一手 UNK 就 掉 下 了 无底 深井\n",
      "\n",
      "Target1:= When my hand slips , I suddenly remember a miner I had met days before who had lost his grip and fell countless feet down that shaft  . \n",
      "Predict1:< I I was , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 食物 是 问题 食物 也 是 解决 解决方案 方案\n",
      "\n",
      "Target2:= Food is the problem and food is the solution  . \n",
      "Predict2:< It is is is is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 24s (- -25m 35s), Epoch: [2/10], Step: [2020/3335], Train Loss: 3.5064117431640627, BLEU: 0.23611161260299812\n",
      "\n",
      "Input1:> 这 是 他们 在内 内罗毕 国家 公园 杀 的 六个 狮子 中 的 其中 一个\n",
      "\n",
      "Target1:= It is one of the six lions which were killed in Nairobi  . \n",
      "Predict1:< This is a the the the the the the  .  .  .\n",
      "\n",
      "Input2:> 所以 我 希望 分享 这种 主动 且 有意 有意识 意识 去 创造 记忆 的 想法\n",
      "\n",
      "Target2:= So I want to share the idea of taking an active role in consciously creating memories  . \n",
      "Predict2:< So I I to to to to to to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 30s (- -25m 30s), Epoch: [2/10], Step: [2040/3335], Train Loss: 3.3985719680786133, BLEU: 0.227677375929827\n",
      "\n",
      "Input1:> 然后 他们 放 我 走 了 真是 个 奇迹\n",
      "\n",
      "Target1:= And they let me go  . It was a miracle  . \n",
      "Predict1:< And then I , , , ,  .  .  .\n",
      "\n",
      "Input2:> 现在 从 这个 故事 里 你 能够 了解 到 三件 有趣 的 事情\n",
      "\n",
      "Target2:= Now you can learn three interesting things from this  . \n",
      "Predict2:< Now , you , , , you , you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 35s (- -25m 24s), Epoch: [2/10], Step: [2060/3335], Train Loss: 3.529053580760956, BLEU: 0.2351845023837247\n",
      "\n",
      "Input1:> 所以 1860 年 他们 认为 这种 肮脏 的 科技 将 扼杀 纽约 的 生命\n",
      "\n",
      "Target1:= So 1860 , they are seeing this dirty technology that is going to choke the life out of New York  . \n",
      "Predict1:< So they , , , , , , , ,  .\n",
      "\n",
      "Input2:> 确定 的 是 除了 手术 或 像 两天 前 我 为了 工作 做 的 棕 褐色 我们 基本 基本上 是 没法 改变 外表 的 我们 的 外表 虽然 是 表面 UNK 的 固定 的 还是 极大 大地 影响\n",
      "\n",
      "Target2:= And of course , barring surgery , or the fake tan that I got two days ago for work , there is very little that we can do to transform how we look , and how we look \n",
      "Predict2:< The , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 41s (- -25m 19s), Epoch: [2/10], Step: [2080/3335], Train Loss: 3.4365368127822875, BLEU: 0.23634054461065754\n",
      "\n",
      "Input1:> 我们 已经 有 大约 50 人参 参与 到 我们 的 团队 中 来 他们 全都 是 志愿 志愿者 愿者\n",
      "\n",
      "Target1:= We ve had , like , 50 people come to our UNK and participate , and it is all volunteers  . \n",
      "Predict1:< We we we to to , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 这个 是 健康 状态 下 的 某人 在 发声 我们 把 自己 想 当作 是 声音 的 芭蕾 芭蕾舞 芭蕾舞者 舞者 因为 当 我们 发声 时 我们 得 去 协调 这些 发声 发声器 发声器官 器官 确实 这些 是\n",
      "\n",
      "Target2:= Now , this is healthy and this is somebody making speech sounds , and we can think of ourselves as vocal ballet dancers , because we have to coordinate all of these vocal organs when we make sounds \n",
      "Predict2:< This is is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 46s (- -25m 13s), Epoch: [2/10], Step: [2100/3335], Train Loss: 3.393114244937897, BLEU: 0.23147009559602924\n",
      "\n",
      "Input1:> 用 UNK 冷藏 食物 也 非 长久 长久之计 久之 计\n",
      "\n",
      "Target1:= UNK way of maintaining things is not sustainable  . \n",
      "Predict1:< The : : , , ,  .  .\n",
      "\n",
      "Input2:> 那 是 我 人生 中 最低 低落 的 时刻 之一\n",
      "\n",
      "Target2:= This was one of the lowest points in my life  . \n",
      "Predict2:< And is is the the the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 52s (- -25m 8s), Epoch: [2/10], Step: [2120/3335], Train Loss: 3.4845874667167664, BLEU: 0.23763212253692942\n",
      "\n",
      "Input1:> 在 这样 寒冷 刮风 的 晚上\n",
      "\n",
      "Target1:= in the cold , windy night  . \n",
      "Predict1:< And is the the the  .  .\n",
      "\n",
      "Input2:> 一天 一周 实际 实际上 是 一年\n",
      "\n",
      "Target2:= A day , a week  ? Actually , a whole year  . \n",
      "Predict2:< And is is is  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 24m 57s (- -25m 2s), Epoch: [2/10], Step: [2140/3335], Train Loss: 3.44054913520813, BLEU: 0.22657486009827926\n",
      "\n",
      "Input1:> 于是 有 了 第二 第二个 二个 办法 利用 稻草 稻草人 草人\n",
      "\n",
      "Target1:= And a second idea I got was to use a scarecrow  . \n",
      "Predict1:< So , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我们 走进 一个 社区 时 不会 将 任何 想法 强加 强加于 加于 对方 而是 与 当地 当地人 坐在 一起 交流\n",
      "\n",
      "Target2:= You never arrive in a community with any ideas , and you sit with the local people  . \n",
      "Predict2:< We we a a a , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 3s (- -26m 57s), Epoch: [2/10], Step: [2160/3335], Train Loss: 3.5127314209938048, BLEU: 0.23623452561727115\n",
      "\n",
      "Input1:> 1832 年 查尔斯 尔斯 达尔 达尔文 在 搭乘 贝格 UNK 号 穿越 大西 大西洋 西洋 的 旅途 途中 发现 了 土耳其 秃鹫 他 当时 时说 说道 这些 令人 令人恶心 恶心 的 长 着 猩红 UNK 红色 秃头 的\n",
      "\n",
      "Target1:= When Charles Darwin went across the Atlantic in 1832 on the Beagle , he saw the turkey vulture , and he said , &quot; These are disgusting birds with bald UNK heads that are formed to revel in \n",
      "Predict1:< The the , , , , , , , , , , , , , , , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 在 那里 我 遇到 了 很多 令人 倾 佩 的 人 我 认识 了 一位 Free the UNK 非政府 政府 组织 的 支持 支持者 这个 组织 致力 致力于 根除 现代 社会 的 奴役 现象\n",
      "\n",
      "Target2:= Amongst all the astonishing people I met there , I met a supporter of Free the Slaves , an NGO dedicated to eradicating modern day slavery  . \n",
      "Predict2:< In I I , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 8s (- -26m 51s), Epoch: [2/10], Step: [2180/3335], Train Loss: 3.6206875324249266, BLEU: 0.2366512569587265\n",
      "\n",
      "Input1:> 所以 你 已经 拿到 了 这个 奖学 奖学金 Richard UNK 是 的\n",
      "\n",
      "Target1:= So you got this scholarship  . Yep  . \n",
      "Predict1:< So , you you you the the the the the  .\n",
      "\n",
      "Input2:> 它 拥有 26 平方 平方公里 方公里 公里 的 空地\n",
      "\n",
      "Target2:= They own 26 square miles of vacant lots  . \n",
      "Predict2:< It is is the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 14s (- -26m 46s), Epoch: [2/10], Step: [2200/3335], Train Loss: 3.4641669034957885, BLEU: 0.24462327643469287\n",
      "\n",
      "Input1:> 然而 与 创业 创业者 业者 打交道 交道 有 个 秘诀\n",
      "\n",
      "Target1:= However , there is a secret to work with entrepreneurs  . \n",
      "Predict1:< But , , , , , ,  .  .\n",
      "\n",
      "Input2:> 这 是 现在 的 我\n",
      "\n",
      "Target2:= And here is me today  . \n",
      "Predict2:< This is is  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 19s (- -26m 40s), Epoch: [2/10], Step: [2220/3335], Train Loss: 3.565259349346161, BLEU: 0.23567051134145736\n",
      "\n",
      "Input1:> 康 纳 第一 第一次 一次 打 我 是 在 我们 婚礼 的 五天 前\n",
      "\n",
      "Target1:= UNK first physically attacked me five days before our wedding  . \n",
      "Predict1:< And , , , , , , , , , the  .  .  .\n",
      "\n",
      "Input2:> 他 有着 超凡 的 记忆 记忆力\n",
      "\n",
      "Target2:= He has the most UNK memory  . \n",
      "Predict2:< He he is the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 25s (- -26m 35s), Epoch: [2/10], Step: [2240/3335], Train Loss: 3.444185400009155, BLEU: 0.22962892543676908\n",
      "\n",
      "Input1:> 下 一个 个人 人们 常 问 我 的 问题 是 我 长大 了 能 当 模特 吗\n",
      "\n",
      "Target1:= The next question people always ask me is , &quot; Can I be a model when I grow up  ? &quot; \n",
      "Predict1:< I , I , I I I I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 我们 的 科学 科学家 学家 和 工程 工程师 们 解决 的 都 是 我们 所 面临 的 最大 的 难题 从 能源 到 环境 再 到 卫生 卫生保健 保健 等等 如果 我们 不明 明白 也 不理 理解 的话\n",
      "\n",
      "Target2:= Our scientists and engineers are the ones that are tackling our grandest challenges , from energy to environment to health care , among others , and if we don t know about it and understand it , then \n",
      "Predict2:< We we we to the , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 31s (- -26m 29s), Epoch: [2/10], Step: [2260/3335], Train Loss: 3.5772144556045533, BLEU: 0.2343768511504687\n",
      "\n",
      "Input1:> 而 致使 延期 和 厌倦 的 原因 是 众议 众议院 议院 对 战 参议 参议院 议院\n",
      "\n",
      "Target1:= And the reason for the delay and the boredom was that the House of Representatives were against the Senate  . \n",
      "Predict1:< And the , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 谢谢\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 36s (- -26m 24s), Epoch: [2/10], Step: [2280/3335], Train Loss: 3.447941815853119, BLEU: 0.22713596148878348\n",
      "\n",
      "Input1:> 我 为什么 什么 没有 逃跑 我 有 很多 的 机会\n",
      "\n",
      "Target1:= Why didn t I walk out  ? I could have left any time  . \n",
      "Predict1:< I I I I I I I  .  .\n",
      "\n",
      "Input2:> 我 可以 感觉 到来 来来 来来往往 来往 往往 大汗 大汗淋漓 淋漓 的 人们 从 我 身边 擦 过 但是 在 一片 黑暗 暗中 我 也 看不见 不见 什么 其他 的 东西 了\n",
      "\n",
      "Target2:= I can feel the brush of sweaty bodies passing me in the darkness , but I can t see much else  . \n",
      "Predict2:< I I I to to , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 42s (- -26m 18s), Epoch: [2/10], Step: [2300/3335], Train Loss: 3.682283365726471, BLEU: 0.24770588250090605\n",
      "\n",
      "Input1:> 那么 最后 一个 问题 是 当 模特 是 什么 感觉\n",
      "\n",
      "Target1:= So the last question people ask me is , &quot; What is it like to be a model  ? &quot; \n",
      "Predict1:< So , is is is is is is is is  ?\n",
      "\n",
      "Input2:> 这 是 菜园 在 街上 的 原因\n",
      "\n",
      "Target2:= This is on the street for a reason  . \n",
      "Predict2:< This is is the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 47s (- -26m 13s), Epoch: [2/10], Step: [2320/3335], Train Loss: 3.407685875892639, BLEU: 0.2351283121954362\n",
      "\n",
      "Input1:> 一年 年前 我 只是 一个 热带 带大 草原 上 帮 爸爸 UNK 的 普通 男孩 我 常常 看着 飞机 飞过 并且 告诉 自己 总有一天 一天 我 也 会 在 里面\n",
      "\n",
      "Target1:= So one year ago , I was just a boy in the savanna grassland herding my father is cows , and I used to see planes flying over , and I told myself that one day , I \n",
      "Predict1:< So I I , I , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target2:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict2:< And , I , I , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 53s (- -26m 7s), Epoch: [2/10], Step: [2340/3335], Train Loss: 3.522149133682251, BLEU: 0.2346169000650749\n",
      "\n",
      "Input1:> 他 无所 无所谓 所谓 宗教 宗教信仰 信仰 的 差异 还有 他 从来 从来没 说 过 谎\n",
      "\n",
      "Target1:= He doesn t care about religious differences , and get this : He has never told a lie  . \n",
      "Predict1:< He he he he he , he , he , he , he he  .\n",
      "\n",
      "Input2:> 但是 第一 第一次 一次 见面 给 我 留下 最深 印象 的 是 他 的 智慧 和 幽默 而 他 的 外貌 看起 看起来 起来 像 农场 男孩\n",
      "\n",
      "Target2:= But what made the biggest impression on me that first meeting was that he was smart and funny and he looked like a farm boy  . \n",
      "Predict2:< But the the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25m 58s (- -26m 2s), Epoch: [2/10], Step: [2360/3335], Train Loss: 3.457913064956665, BLEU: 0.23240845266811713\n",
      "\n",
      "Input1:> 在 喜马拉 喜马拉雅 马拉 马拉雅 我 见到 了 做 苦力 的 小孩 背着 石头 走上 几公里 公里 的 山路 到 路边 等 着 的 卡车 旁\n",
      "\n",
      "Target1:= In the Himalayas , I found children carrying stone for miles down UNK terrain to trucks waiting at roads below  . \n",
      "Predict1:< In the , , , , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 我 猜 他们 期待 的 答案 是 如果 你 再 瘦 一点 一点儿 点儿 你 的 头发 再有 光泽 一些 你 就 会 很 开心 很 难以 难以置信 置信\n",
      "\n",
      "Target2:= And I think the answer that they are looking for is , &quot; If you are a little bit skinnier and you have UNK hair , you will be so happy and fabulous  . &quot; \n",
      "Predict2:< I I you you you , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 4s (- -27m 56s), Epoch: [2/10], Step: [2380/3335], Train Loss: 3.548243987560272, BLEU: 0.2419054648079705\n",
      "\n",
      "Input1:> 现在 如果 我 用 一个 激光 光笔 并且 在 万亿 亿分 之一 一秒 内 开关 一次 也 就是 几 飞秒 我 就 会 制造 一些 光子 这些 光子 仅仅 是 几毫米 毫米 宽 并且 这些 光子 像 子弹\n",
      "\n",
      "Target1:= Now if I take a laser pointer and turn it on and off in one UNK of a second -- which is several femtoseconds -- I ll create a packet of photons barely a millimeter wide , and \n",
      "Predict1:< Now , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 想象 这个 小镇 肯定 备受 摧残\n",
      "\n",
      "Target2:= As you can imagine , the town had been devastated  . \n",
      "Predict2:< You can you you the , the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 9s (- -27m 51s), Epoch: [2/10], Step: [2400/3335], Train Loss: 3.5487733125686645, BLEU: 0.23403865938072968\n",
      "\n",
      "Input1:> 这个 项目 也 为 我们 这些 修 图 师 带来 了 很大 改变\n",
      "\n",
      "Target1:= The project is also made a big difference in the lives of the retouchers  . \n",
      "Predict1:< And is is we we , , we , ,  .  .  .\n",
      "\n",
      "Input2:> 回到 我 的 问题 为什么 什么 我 要 留下 留下来 下来 了\n",
      "\n",
      "Target2:= Back to my question : Why did I stay  ? \n",
      "Predict2:< I am I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 15s (- -27m 45s), Epoch: [2/10], Step: [2420/3335], Train Loss: 3.5176759243011473, BLEU: 0.24805115060915864\n",
      "\n",
      "Input1:> 我们 在 被 跟踪 吗\n",
      "\n",
      "Target1:= Were we being followed  ? \n",
      "Predict1:< We we we to  .\n",
      "\n",
      "Input2:> 当 我 在 修复 那些 照片 的 时候 我 不禁 想到 其中 的 每个 个人 以及 照片 片中 传递 的 那些 故事\n",
      "\n",
      "Target2:= &quot; As I worked , I couldn t help but think about the individuals and the stories represented in the images  . \n",
      "Predict2:< When I I I I , , , , , , , , , , , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 20s (- -27m 40s), Epoch: [2/10], Step: [2440/3335], Train Loss: 3.5099847674369813, BLEU: 0.23226745041030103\n",
      "\n",
      "Input1:> 培养 出下 一个 乔治 UNK 华盛 华盛顿 呢\n",
      "\n",
      "Target1:= We might produce the next George Washington Carver  . \n",
      "Predict1:< And : : , , ,  .  .\n",
      "\n",
      "Input2:> 肯尼 肯尼亚 尼亚 全世界 世界 都 将 会 有 很大 的 损失 如果 失去 了 这些 奇妙 的 物种\n",
      "\n",
      "Target2:= Kenya , this world , will be much poorer without these wonderful species  . \n",
      "Predict2:< And the , , , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 26s (- -27m 34s), Epoch: [2/10], Step: [2460/3335], Train Loss: 3.4922006845474245, BLEU: 0.25101365738348236\n",
      "\n",
      "Input1:> 所以 用 这个 技术 我们 可以 制造 能 看到 光路 拐角 的 照相 照相机 相机 能 看到 视野 之外 或者 不用 X 光 就 能 透视 我们 身体 的 照相 照相机 相机 这种 相机 挑战 了 我们 关于\n",
      "\n",
      "Target1:= And with that , we can create cameras that can look around corners , beyond line of sight or see inside our body without an X-ray , and really challenge what we mean by a camera  . \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 很 怪\n",
      "\n",
      "Target2:= That was awkward  . \n",
      "Predict2:< It is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 31s (- -27m 28s), Epoch: [2/10], Step: [2480/3335], Train Loss: 3.4666547417640685, BLEU: 0.2337054822369246\n",
      "\n",
      "Input1:> 我 没有 一天 不想 这些 美丽 但 却 受到 不公 公正 正待 待遇 的 人们 这些 我 UNK UNK UNK 有幸 幸能 能够 遇见 的 人们\n",
      "\n",
      "Target1:= There is not a day that goes by that I don t think of these many beautiful , mistreated people I ve had the tremendous honor of meeting  . \n",
      "Predict1:< I I I to to to , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 为 我们 营造 了 一个 反省 和 思考 的 空间 也 提醒 我们 在 不断 断成 成长 改变 的 过程 中 什么 才 是 最为 为重 重要 的\n",
      "\n",
      "Target2:= It is about making space for reflection and contemplation , and remembering what really matters most to us as we grow and change  . \n",
      "Predict2:< And is is a we we we we , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 37s (- -27m 23s), Epoch: [2/10], Step: [2500/3335], Train Loss: 3.518550789356232, BLEU: 0.23583655406156354\n",
      "\n",
      "Input1:> 90 年代 中期 朝鲜 发生 了 一次 严重 的 饥荒\n",
      "\n",
      "Target1:= A huge famine hit North Korea in the mid-1990s  . \n",
      "Predict1:< And is the , , the  .  .  .\n",
      "\n",
      "Input2:> 他 刚 起步 时 并非 单枪 单枪匹马\n",
      "\n",
      "Target2:= He wasn t alone when he started  . \n",
      "Predict2:< He he is a , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 42s (- -27m 17s), Epoch: [2/10], Step: [2520/3335], Train Loss: 3.6120861768722534, BLEU: 0.2488247022246225\n",
      "\n",
      "Input1:> 我 希望 你们 明白 的 是 这些 不是 我 自己 的 照片\n",
      "\n",
      "Target1:= And I hope what you are seeing is that these pictures are not pictures of me  . \n",
      "Predict1:< I I I you you , , you ,  .  .  .\n",
      "\n",
      "Input2:> 第一 你 要 给予 他们 保密 的 承诺\n",
      "\n",
      "Target2:= First , you have to offer them UNK  . \n",
      "Predict2:< And you you , , you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 48s (- -27m 12s), Epoch: [2/10], Step: [2540/3335], Train Loss: 3.357075548171997, BLEU: 0.238003922838481\n",
      "\n",
      "Input1:> 我 为什么 什么 没有 逃跑 我 有 很多 的 机会\n",
      "\n",
      "Target1:= Why didn t I walk out  ? I could have left any time  . \n",
      "Predict1:< I I I I I I I  .  .\n",
      "\n",
      "Input2:> 战争 和 被 驱逐 的 生活 怎样 制造 了 这个 充满 风格 和 时尚 的 环境 这个 只能 通过 过为 士兵 和 自杀 自杀性 爆炸 袭击 袭击者 设计 一款 时装 来 实现 于是 我 把 当地 阿富汗 狐狸\n",
      "\n",
      "Target2:= The war and the UNK life that comes with it have created this environment of style and fashion that can only be described through creating a fashion line for soldiers and suicide bombers where I take local Afghan \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 53s (- -27m 6s), Epoch: [2/10], Step: [2560/3335], Train Loss: 3.5568914294242857, BLEU: 0.2370789994415666\n",
      "\n",
      "Input1:> 他 刚 起步 时 并非 单枪 单枪匹马\n",
      "\n",
      "Target1:= He wasn t alone when he started  . \n",
      "Predict1:< He he a a a  .  .  .  .\n",
      "\n",
      "Input2:> 其中 中大 大部 大部分 部分 最终 回到 了 失主 手里 这 一点 也 异常 重要\n",
      "\n",
      "Target2:= a large majority of which did actually find their home again , importantly  . \n",
      "Predict2:< And is is , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26m 59s (- -27m 1s), Epoch: [2/10], Step: [2580/3335], Train Loss: 3.4946786761283875, BLEU: 0.23740786332550282\n",
      "\n",
      "Input1:> 今天 今天上午 天上 上午 你们 鼓掌 最多 的 演讲 讲来 来自 谁\n",
      "\n",
      "Target1:= Which presentation have you applauded the most this morning  ? \n",
      "Predict1:< And you , you , , you , the  .  .\n",
      "\n",
      "Input2:> 是 充满 创业 热情 的 当地 当地人 你们 为 他们 鼓掌\n",
      "\n",
      "Target2:= Local , passionate people  . That is who you have applauded  . \n",
      "Predict2:< It is , , , , ,  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 4s (- -28m 55s), Epoch: [2/10], Step: [2600/3335], Train Loss: 3.518175733089447, BLEU: 0.23197832818710115\n",
      "\n",
      "Input1:> 我们 需要 客观 地 测量 进展 最后 想要 知道 何时 有治 治愈 之 方 的 唯一 一方 方法 就是 当 我们 拥有 了 能 准确 回答 那些 问题 的 客观 观测 测量 UNK\n",
      "\n",
      "Target1:= We need to be able to measure progression objectively , and ultimately , the only way we are going to know when we actually have a cure is when we have an objective measure that can answer that \n",
      "Predict1:< We we to to to to to , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 必须 给予 他们 保密 的 承诺 并 尊重 他们 的 隐私 我们 要 擅长 为 他们 提供 帮助 这样 他们 才 会上 上门 才 会 纷至沓来\n",
      "\n",
      "Target2:= You have to offer them UNK , privacy , you have to be fantastic at helping them , and then they will come , and they will come in droves  . \n",
      "Predict2:< We are to to to to , , , , , , , they , , they , , they , , they , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 10s (- -28m 50s), Epoch: [2/10], Step: [2620/3335], Train Loss: 3.5185879111289977, BLEU: 0.2510577126077623\n",
      "\n",
      "Input1:> 秃鹫 还 曾经 被 迪斯尼 看上 像 高飞 狗 一样 被 人格 人格化 变成 了 麻木 的 愚蠢 的 动画 UNK 角色\n",
      "\n",
      "Target1:= They ve also be associated with Disney — — personified as goofy , dumb , stupid characters  . \n",
      "Predict1:< The , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 据估计 估计 有 超过 4000 名儿 儿童 在 沃尔特 尔特 湖水 水库 被 奴役 那里 是 世界 上 最大 的 人工 人工湖\n",
      "\n",
      "Target2:= It is estimated that more than 4,000 children are enslaved on Lake Volta , the largest man-made lake in the world  . \n",
      "Predict2:< And the is , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 15s (- -28m 44s), Epoch: [2/10], Step: [2640/3335], Train Loss: 3.5501160621643066, BLEU: 0.24554861327243516\n",
      "\n",
      "Input1:> 而 致使 延期 和 厌倦 的 原因 是 众议 众议院 议院 对 战 参议 参议院 议院\n",
      "\n",
      "Target1:= And the reason for the delay and the boredom was that the House of Representatives were against the Senate  . \n",
      "Predict1:< And is is the , , , , , , , , ,\n",
      "\n",
      "Input2:> 每当 你 看到 秃鹫 就 像 这 只 首先 浮现 现在 你 脑海 脑海中 海中 一定 是 这 是 一些 令人 令人厌恶 厌恶 贪得 贪得无厌 并且 追逐 腐肉 的 可怕 怕生 生物 让 人 禁不住 不住 联想 想到\n",
      "\n",
      "Target2:= When you see a vulture like this , the first thing that comes to your mind is , these are disgusting , ugly , greedy creatures that are just after your flesh , associated with politicians  . \n",
      "Predict2:< And you you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 21s (- -28m 39s), Epoch: [2/10], Step: [2660/3335], Train Loss: 3.5492988228797913, BLEU: 0.23304658469854297\n",
      "\n",
      "Input1:> 所以 在 那 之后 5 年 我 女扮男装 男装 陪 着 我 姐姐 去 一个 秘密 学校 姐姐 那时 已经 不 被 允许 独自 自外 外出 了\n",
      "\n",
      "Target1:= So for the next five years , I dressed as a boy to escort my older sister , who was no longer allowed to be outside alone , to a secret school  . \n",
      "Predict1:< So , I , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 首先 我 要 表扬 下 你们 对于 模特 界 的 了解 令人 印象 深刻\n",
      "\n",
      "Target2:= And first , I commend you on your model knowledge  . Very impressive  . \n",
      "Predict2:< I I I to you , , , , you , , you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 26s (- -28m 33s), Epoch: [2/10], Step: [2680/3335], Train Loss: 3.494688403606415, BLEU: 0.24040767554990353\n",
      "\n",
      "Input1:> 由于 风化 紫外 紫外线 外线 氧化 等 原因 这种 粘合 粘合剂 合剂 这种 沥青 集料 之间 的 黏胶 会 收缩 如果 它 收缩 就 会 有 小 裂纹 从 集 块 开始 会 出现 分层\n",
      "\n",
      "Target1:= Due to UNK , due to UNK light , due to oxidation , this binder , this bitumen , the glue between the aggregates is going to shrink , and if it shrinks , it gets UNK , \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 一些 其他 英雄 能 隐身 但 想象 一个 未来 的 超人 他 能 看到 拐角 后面 的 东西\n",
      "\n",
      "Target2:= Some other heroes can become invisible , but what about a new power for a future superhero : to see around corners  ? \n",
      "Predict2:< And is a a a , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 32s (- -28m 28s), Epoch: [2/10], Step: [2700/3335], Train Loss: 3.47178772687912, BLEU: 0.23729397550419384\n",
      "\n",
      "Input1:> 看 真的 修好 了\n",
      "\n",
      "Target1:= Let is see  . Yeah , it worked  . \n",
      "Predict1:< And is is  .  .  .\n",
      "\n",
      "Input2:> 我 认为 答案 已经 藏 在 问题 之中\n",
      "\n",
      "Target2:= So I figured that the problem is the solution  . \n",
      "Predict2:< I I I to to the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 37s (- -28m 22s), Epoch: [2/10], Step: [2720/3335], Train Loss: 3.493012750148773, BLEU: 0.23144666958096108\n",
      "\n",
      "Input1:> 下 一个 个人 人们 常 问 我 的 问题 是 我 长大 了 能 当 模特 吗\n",
      "\n",
      "Target1:= The next question people always ask me is , &quot; Can I be a model when I grow up  ? &quot; \n",
      "Predict1:< I , I , I I I I I I I I I I I  .\n",
      "\n",
      "Input2:> 那些 大石 UNK 石板 比 搬运 石板 的 孩子 还要 重 孩子 们 用 树枝 绳子 破烂 的 衣服 制成 背带 把 石板 绑 在 自己 己身 身上\n",
      "\n",
      "Target2:= The big sheets of slate were heavier than the children carrying them , and the kids hoisted them from their heads using these handmade harnesses of sticks and rope and torn cloth  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 43s (- -28m 17s), Epoch: [2/10], Step: [2740/3335], Train Loss: 3.425713002681732, BLEU: 0.24006810276277693\n",
      "\n",
      "Input1:> 要 知道 有些 人 可能 不会 打 给 我们 或者 没接 接通\n",
      "\n",
      "Target1:= You see , some people may not do it  . They may not get through it  . \n",
      "Predict1:< And , , , , , , , we we to  .  .\n",
      "\n",
      "Input2:> 你 能 想象 一个 400 000 人 的 社区 凝聚 了 多少 智力 和 热情 吗\n",
      "\n",
      "Target2:= Can you imagine a community of 400,000 people , the intelligence and the passion  ? \n",
      "Predict2:< You you you to you , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 48s (- -28m 11s), Epoch: [2/10], Step: [2760/3335], Train Loss: 3.556734037399292, BLEU: 0.24769806435300018\n",
      "\n",
      "Input1:> 在 洛杉矶 UNK 人们 死于 可 治愈 的 疾病\n",
      "\n",
      "Target1:= People are dying from curable diseases in South Central Los Angeles  . \n",
      "Predict1:< In the the , , the the  .  .\n",
      "\n",
      "Input2:> 我 装上 了 这些 灯 你们 可以 在 背后 看到 这些 就是 赶 狮 灯\n",
      "\n",
      "Target2:= So I put the lights  . You can see at the back , those are the lion lights  . \n",
      "Predict2:< I I you you you , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 54s (- -28m 6s), Epoch: [2/10], Step: [2780/3335], Train Loss: 3.4490211963653565, BLEU: 0.24674884739861597\n",
      "\n",
      "Input1:> 彼得 德鲁 德鲁克 史 上 最 伟大 的 管理 咨询 咨询师 之一 几年 年前 前去 去世 了 享年 96 岁\n",
      "\n",
      "Target1:= Peter UNK , one of the greatest management consultants in history , died age 96 , a few years ago  . \n",
      "Predict1:< The the , , , , , , , , , , , , , the the\n",
      "\n",
      "Input2:> 我们 的 工作 地点 不在 办公 办公室 公室\n",
      "\n",
      "Target2:= We don t work from offices  . \n",
      "Predict2:< We are we the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27m 59s (- -28m 0s), Epoch: [2/10], Step: [2800/3335], Train Loss: 3.482218933105469, BLEU: 0.2432540015541754\n",
      "\n",
      "Input1:> 我 嫁给 我 的 第二 UNK 丈夫 UNK 差不多 不多 20 年 了 并 育 有 3 个 孩子\n",
      "\n",
      "Target1:= I ve been married for almost 20 years to my second husband and we have three kids together  . \n",
      "Predict1:< I I I , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 因此 我 做 的 就是 在 我 的 房子 前 种 上 食物 森林\n",
      "\n",
      "Target2:= So what I did , I planted a food forest in front of my house  . \n",
      "Predict2:< So I I I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 5s (- -29m 55s), Epoch: [2/10], Step: [2820/3335], Train Loss: 3.585243010520935, BLEU: 0.24232670625734912\n",
      "\n",
      "Input1:> 在 这 之后 不久 当 我 走过 一个 火车 火车站 车站 时 我 看到 一件 非常 痛心 的 事情 让 我 难以 难以忘怀 忘怀\n",
      "\n",
      "Target1:= Soon after , when I was walking past a train station , I saw something terrible that I can t erase from my memory  . \n",
      "Predict1:< And , , , I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 这 是 初次 尝试 的 一张 照片\n",
      "\n",
      "Target2:= This was the very first  . \n",
      "Predict2:< This is is the  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 10s (- -29m 49s), Epoch: [2/10], Step: [2840/3335], Train Loss: 3.5806391716003416, BLEU: 0.25295438187871355\n",
      "\n",
      "Input1:> 在 那里 我 遇到 了 很多 令人 倾 佩 的 人 我 认识 了 一位 Free the UNK 非政府 政府 组织 的 支持 支持者 这个 组织 致力 致力于 根除 现代 社会 的 奴役 现象\n",
      "\n",
      "Target1:= Amongst all the astonishing people I met there , I met a supporter of Free the Slaves , an NGO dedicated to eradicating modern day slavery  . \n",
      "Predict1:< And I I , I , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 并不知道 不知 知道 他 是 在 虐待 我\n",
      "\n",
      "Target2:= I didn t know he was abusing me  . \n",
      "Predict2:< I I I , I , I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 16s (- -29m 44s), Epoch: [2/10], Step: [2860/3335], Train Loss: 3.621969020366669, BLEU: 0.23349944456872426\n",
      "\n",
      "Input1:> 她 也 开始 始自 自己 亲手 清洗 UNK 照片 直到 她 意识 到 她 自己 清洗 其实 对照 照片 造成 了 更多 损害\n",
      "\n",
      "Target1:= She had started UNK them herself and stopped when she realized she was doing more damage  . \n",
      "Predict1:< She she , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 觉得 人们 太 容易 被 日复一日 一日 的 琐碎 困住 而 忘记 什么 才 是 真正 重要 的 事\n",
      "\n",
      "Target2:= I feel like it is easy to get caught up in the day-to-day , and forget what really matters to you  . \n",
      "Predict2:< I I think to to to to , , , , , , is is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 21s (- -29m 38s), Epoch: [2/10], Step: [2880/3335], Train Loss: 3.394731640815735, BLEU: 0.24834326854053015\n",
      "\n",
      "Input1:> 我们 自己 也 变得 又 脏 又 臭 但是 我们 心里 很 舒坦\n",
      "\n",
      "Target1:= We got dirty , and we loved it  . \n",
      "Predict1:< We we we we we we , we we we we  .  .\n",
      "\n",
      "Input2:> 不过 过为 为了 真实 的 表明 词汇 和 政治 怎样 互相 影响 我 想 带 你 回到 过去 的 美国 在 它 刚刚 宣布 UNK 独立 的 时候\n",
      "\n",
      "Target2:= But to really show you how words and politics interact , I want to take you back to the United States of America , just after they d achieved independence  . \n",
      "Predict2:< But the , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 27s (- -29m 33s), Epoch: [2/10], Step: [2900/3335], Train Loss: 3.493901228904724, BLEU: 0.23729091884950568\n",
      "\n",
      "Input1:> 当时 的 海啸 UNK 最高 时刻 高达 逾 24 米 冲击 到 内陆 足足 足足有 2 英里 的 地方\n",
      "\n",
      "Target1:= The waters here have been recorded at reaching over 24 meters in height , and traveled over two miles inland  . \n",
      "Predict1:< The the , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 它 是 沥青 做 的 沥青 是 一种 很 好 的 材料 在 上面 开车 很 好 但 也 不 总是 特别 是 像 今天 这样 的 天气 下 大雨 的 时候\n",
      "\n",
      "Target2:= It is made of asphalt , and asphalt is a very nice material to drive on , but not always , especially not on these days as today , when it is raining a lot  . \n",
      "Predict2:< It is is is , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 32s (- -29m 27s), Epoch: [2/10], Step: [2920/3335], Train Loss: 3.580247712135315, BLEU: 0.2444166824280896\n",
      "\n",
      "Input1:> 她 的 这些 照片 其实 实是 由 她 丈夫 的 同事 事发 发现 的 在 当地 的 消防 防部 部队 的 瓦砾 下发 发现 的 那个 地方 其实 离 她家 很远 他们 居然 认出 了 他\n",
      "\n",
      "Target1:= Her photos were found by her husband is colleagues at a local fire department in the debris a long way from where the home had once stood , and they d recognized him  . \n",
      "Predict1:< She she the , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 有 能力 制止 家庭 家庭暴力 暴力 只 需要 要点 点亮 星星 星星之火\n",
      "\n",
      "Target2:= You have the power to end domestic violence simply by shining a spotlight on it  . \n",
      "Predict2:< You you you to to , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 38s (- -29m 22s), Epoch: [2/10], Step: [2940/3335], Train Loss: 3.4249997615814207, BLEU: 0.24601081058808866\n",
      "\n",
      "Input1:> 但 令人 泄气 的 是 帕金森 帕金森症 和 其他 运动 运动障碍 障碍 疾病 都 没有 有生 生物 标志 标志物 所以 你 没法 做 简单 的 血液 检查 我们 能 做到 的 最好 的 就是 像 这个 20 分钟\n",
      "\n",
      "Target1:= But UNK , with Parkinson is disease and other movement disorders , there are no biomarkers , so there is no simple blood test that you can do , and the best that we have is like this \n",
      "Predict1:< But the is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 嫁给 我 的 第二 UNK 丈夫 UNK 差不多 不多 20 年 了 并 育 有 3 个 孩子\n",
      "\n",
      "Target2:= I ve been married for almost 20 years to my second husband and we have three kids together  . \n",
      "Predict2:< I I I , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 43s (- -29m 16s), Epoch: [2/10], Step: [2960/3335], Train Loss: 3.5370273113250734, BLEU: 0.24150115580047057\n",
      "\n",
      "Input1:> 我 是 如此 幸运 在 人生 中 得到 了 这么 多 帮助 和 启示 所以 我 想 帮助 那些 心怀 梦想 的 朝鲜 朝鲜人 在 国际 的 援助 下 一个 实现 梦想 的 机会\n",
      "\n",
      "Target1:= I ve been so lucky , received so much help and inspiration in my life , so I want to help give aspiring North Koreans a chance to prosper with international support  . \n",
      "Predict1:< I I I I , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 每天 要 走 不同 的 路线 这样 才 没有 有人 会 怀疑 我们 要 去 哪里\n",
      "\n",
      "Target2:= Each day , we took a different route so that no one would suspect where we were going  . \n",
      "Predict2:< We we we to to to , , , , , we , we we to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 49s (- -29m 11s), Epoch: [2/10], Step: [2980/3335], Train Loss: 3.511341166496277, BLEU: 0.248225648859198\n",
      "\n",
      "Input1:> 我心 心想 搞 什么 来 真的\n",
      "\n",
      "Target1:= And I am like , &quot; Come on , really  ? \n",
      "Predict1:< And , is ,  ?  ?\n",
      "\n",
      "Input2:> 在 我们 的 谈话 结束 后 我 感觉 非常 的 糟糕 并且 且为 自己 对 这样 残暴 的 现象 缺乏 认知 感觉 到 惭愧 我 不知 知道 还有 多少 人 也 一样 不知 知道\n",
      "\n",
      "Target2:= After we finished talking , I felt so horrible and honestly ashamed at my own lack of knowledge of this atrocity in my own lifetime , and I thought , if I don t know , how many \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28m 54s (- -29m 5s), Epoch: [2/10], Step: [3000/3335], Train Loss: 3.3515522599220278, BLEU: 0.24942989569966886\n",
      "\n",
      "Input1:> 我们 必须 给予 他们 保密 的 承诺 并 尊重 他们 的 隐私 我们 要 擅长 为 他们 提供 帮助 这样 他们 才 会上 上门 才 会 纷至沓来\n",
      "\n",
      "Target1:= You have to offer them UNK , privacy , you have to be fantastic at helping them , and then they will come , and they will come in droves  . \n",
      "Predict1:< We are we to to to , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他们 在 大桶 里 搅拌 燃料 然后 把 丝绸 深深 深深地 浸 进去 直到 染料 淹到 到手 手肘 这些 染料 都 是 有毒 的\n",
      "\n",
      "Target2:= They mix dye in these big barrels , and they submerge the silk into the liquid up to their elbows , but the dye is toxic  . \n",
      "Predict2:< They are , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 0s (- -29m 0s), Epoch: [2/10], Step: [3020/3335], Train Loss: 3.603215754032135, BLEU: 0.2449090448014065\n",
      "\n",
      "Input1:> 她 也 开始 始自 自己 亲手 清洗 UNK 照片 直到 她 意识 到 她 自己 清洗 其实 对照 照片 造成 了 更多 损害\n",
      "\n",
      "Target1:= She had started UNK them herself and stopped when she realized she was doing more damage  . \n",
      "Predict1:< She she she , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 日本 呆 了 6 个 月 以后 携手 志愿 志愿者 愿者 共有 1 100 名 志愿 志愿者 愿者 加入 帮助 日本 救灾 其中 几百 几百名 百名 帮 我们 手工 清洗 超过 13 5 万张 照片 其中 的\n",
      "\n",
      "Target2:= After six months in Japan , 1,100 volunteers had passed through All Hands , hundreds of whom had helped us UNK over 135,000 photographs , the large majority \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 6s (- -30m 54s), Epoch: [2/10], Step: [3040/3335], Train Loss: 3.622373306751251, BLEU: 0.2582000091851057\n",
      "\n",
      "Input1:> 和 你 的 孩子 你 的 同事 你 的 朋友 和 家人 讨论 这个 话题 将 虐待 UNK 之于 光\n",
      "\n",
      "Target1:= Show abuse the light of day by talking about it with your children , your coworkers , your friends and family  . \n",
      "Predict1:< And you you you you , , , , and , and and and and and\n",
      "\n",
      "Input2:> 那 是 我 第一 第一次 一次 听到 活 在 我 的 国家 的 人们 所在 承受 的 一切\n",
      "\n",
      "Target2:= This was the first time I heard that people in my country were suffering  . \n",
      "Predict2:< And is my I I I I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 11s (- -30m 49s), Epoch: [2/10], Step: [3060/3335], Train Loss: 3.5310712099075316, BLEU: 0.255736506076575\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 生长 着 八种 秃鹫 其中 的 六种 正 濒临 濒临灭绝 灭绝\n",
      "\n",
      "Target1:= We have eight species of vultures that occur in Kenya , of which six are highly threatened with extinction  . \n",
      "Predict1:< The , , , , , , , the the the\n",
      "\n",
      "Input2:> UNK 身上 上体 体现 了 一种 可能 可能性\n",
      "\n",
      "Target2:= Kofi is the embodiment of possibility  . \n",
      "Predict2:< And , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 17s (- -30m 43s), Epoch: [2/10], Step: [3080/3335], Train Loss: 3.5453590035438536, BLEU: 0.2468901882449244\n",
      "\n",
      "Input1:> 怎样 更 能 表达 我们 对 空置 的 店面 的 期待 使 我们 的 社区 能 反映 映出 我们 现在 的 需求 求和 梦想\n",
      "\n",
      "Target1:= And how can we share more of our hopes for our vacant storefronts , so our communities can reflect our needs and dreams today  ? \n",
      "Predict1:< And we we we we we we we we , we we , we we we we we we we\n",
      "\n",
      "Input2:> 这个 个数 数字 是 在 跨 大西 大西洋 西洋 洋奴 奴隶 贸易 中 从 非洲 被 贩卖 的 UNK 总数 的 两倍\n",
      "\n",
      "Target2:= That is double the amount of people taken from Africa during the entire trans-Atlantic slave trade  . \n",
      "Predict2:< This is the the the the the the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 22s (- -30m 37s), Epoch: [2/10], Step: [3100/3335], Train Loss: 3.448008930683136, BLEU: 0.24547131345343556\n",
      "\n",
      "Input1:> 而且 这 是 很重 重要 的 一 课 我 认为 你们 能够 够学 学到 的 也 是 我 想 留给 你们 的 要点\n",
      "\n",
      "Target1:= And that is the important lesson I think you can take away , and the one I want to leave you with  . \n",
      "Predict1:< And is is is , I , I , , , , , , , , , I ,\n",
      "\n",
      "Input2:> 英语 在 韩国 太重 重要 了 所以 我 必须 须要 开始 学习 第三 UNK 语言 英语\n",
      "\n",
      "Target2:= English was so important in South Korea , so I had to start learning my third language  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 28s (- -30m 32s), Epoch: [2/10], Step: [3120/3335], Train Loss: 3.5595922589302065, BLEU: 0.2564254648291852\n",
      "\n",
      "Input1:> 那个 善良 的 陌生 陌生人 生人 象征 着 我 以及 朝鲜 朝鲜人 最 需要 的 希望 他 使 我 明白 陌生 陌生人 生人 的 善意 和 国际 社会 的 支持 真是 我们 朝鲜 朝鲜人 需要 的 希望 之\n",
      "\n",
      "Target1:= The kind stranger symbolized new hope for me and the North Korean people when we needed it most , and he showed me the kindness of strangers and the support of the international community are truly the rays \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 想象 这个 小镇 肯定 备受 摧残\n",
      "\n",
      "Target2:= As you can imagine , the town had been devastated  . \n",
      "Predict2:< You can can you you the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 33s (- -30m 26s), Epoch: [2/10], Step: [3140/3335], Train Loss: 3.485807418823242, BLEU: 0.2542644181778471\n",
      "\n",
      "Input1:> 但 让 我 坚定 坚定信念 信念 的 是 即使 现实 如此 就算 他们 不 被 当作 正常 正常人 常人 看 这 也 只 说明 了 一件 事 他们 是非 非凡 的 虽然 患有 自闭 自闭症 但 非凡 无比\n",
      "\n",
      "Target1:= But what lifted my heart and strengthened my soul was that even though this was the case , although they were not seen as ordinary , this could only mean one thing : that they were extraordinary -- \n",
      "Predict1:< But I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 学校 我们 用 很多 时间 来 学习 UNK UNK 的 历史 但是 我们 从来 从来没 没有 对外 外面 的 世界 有 过 太多 了解 除了 知道 美国 韩国 和 日本 是 敌人 外\n",
      "\n",
      "Target2:= In school , we spent a lot of time studying the history of Kim UNK , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies  . \n",
      "Predict2:< In we we , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 39s (- -30m 21s), Epoch: [2/10], Step: [3160/3335], Train Loss: 3.5061516165733337, BLEU: 0.2506097364221769\n",
      "\n",
      "Input1:> 这样 的 能人 尚未 未出 出生\n",
      "\n",
      "Target1:= This person has never been born  . \n",
      "Predict1:< And is is the  .  .  .\n",
      "\n",
      "Input2:> 目睹 这样 UNK 的 场景 是 一件 极其 痛苦 的 事\n",
      "\n",
      "Target2:= It is difficult to witness something so overwhelming  . \n",
      "Predict2:< The is is is is is is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 44s (- -30m 15s), Epoch: [2/10], Step: [3180/3335], Train Loss: 3.490537130832672, BLEU: 0.2498960258264472\n",
      "\n",
      "Input1:> 我 在 两年 年前 把 这个 装置 UNK 在 了 我家 家中 从那时起 那时 起 我们 就 再 没有 狮子 捕食 家禽 的 烦恼 了\n",
      "\n",
      "Target1:= So I set it up in my home two years ago , and since then , we have never experienced any problem with lions  . \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , the the the\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢 鼓掌\n",
      "\n",
      "Target2:= Thank you very much  . \n",
      "Predict2:< Thank you very  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 50s (- -30m 10s), Epoch: [2/10], Step: [3200/3335], Train Loss: 3.3210412740707396, BLEU: 0.25407770200361546\n",
      "\n",
      "Input1:> 我 发现 美国 国人 能 看出 我们 变化 中 的 脆弱 脆弱性\n",
      "\n",
      "Target1:= I find that Americans see the fragility in changes  . \n",
      "Predict1:< I I I to to to the the the of  .  .  .\n",
      "\n",
      "Input2:> 如果 你 在 这种 路上 上行 行驶 你 会 带出 这些 集料 就是 我们 看到 的\n",
      "\n",
      "Target2:= Then if you drive over the road , you take out the aggregates -- what we just saw here  . \n",
      "Predict2:< If you you you you , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29m 55s (- -30m 4s), Epoch: [2/10], Step: [3220/3335], Train Loss: 3.430987286567688, BLEU: 0.2540831754684582\n",
      "\n",
      "Input1:> 当 我 看到 我 身边 的 人们 UNK 肚子 时 这 让 我 觉得 非常 羞愧 这 加强 了 我 做 食物 森林 的 决心 人们 问 我 UNK 你 难道 不怕 人们 来 偷 你 食物 吗\n",
      "\n",
      "Target1:= It made me feel ashamed to see people that were this close to me that were hungry , and this only reinforced why I do this , and people asked me , &quot; Fin , aren t you \n",
      "Predict1:< And I I I I I , I , , I , , , I , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 对 是 的 所以 以是 这样 的 在 来电 期间 你 得 告诉 我们 你 是否 患有 这种 疾病 TR 嗯\n",
      "\n",
      "Target2:= Yeah  . Yeah  . So what will happen is that , during the call you have to indicate whether or not you have the disease or not , you see  . Right  . \n",
      "Predict2:< And is , , , , , , , , , , , you , , , you , , you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 1s (- -31m 59s), Epoch: [2/10], Step: [3240/3335], Train Loss: 3.468885123729706, BLEU: 0.2625264932535187\n",
      "\n",
      "Input1:> 回到 我 的 问题 为什么 什么 我 要 留下 留下来 下来 了\n",
      "\n",
      "Target1:= Back to my question : Why did I stay  ? \n",
      "Predict1:< And I I I I I I I I  .\n",
      "\n",
      "Input2:> 我 在 埃斯 UNK 兰斯 的 第一 第一年 一年 只是 出去 遛 街 在 那些 日子 里 我 有 了 第一 第一位 一位 客户 我 帮助 了 这 第一 第一个 一个 个人 他 是 UNK UNK 利人 在\n",
      "\n",
      "Target2:= And so what I did in UNK that first year was to just walk the streets , and in three days I had my first client , and I helped this first guy who was smoking fish from \n",
      "Predict2:< I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 6s (- -31m 53s), Epoch: [2/10], Step: [3260/3335], Train Loss: 3.437354254722595, BLEU: 0.2609839756070413\n",
      "\n",
      "Input1:> 光 在 慢动作 动作 下 会 是 什么 什么样\n",
      "\n",
      "Target1:= How does light look in slow motion  ? \n",
      "Predict1:< And is , , ,  ?  ?  ?\n",
      "\n",
      "Input2:> 我 住 在 这栋 房子 附近 我 就 想 如何 让 它 在 这 片 社区 区里 成为 一个 更好 的 地方 也 思考 了 另 一件 事 这件 事 彻底 彻底改变 改变 了 我 的 人生\n",
      "\n",
      "Target2:= I live near this house , and I thought about how I could make it a nicer space for my neighborhood , and I also thought about something that changed my life forever  . \n",
      "Predict2:< I I I I I I I I I , I , I , I , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 12s (- -31m 48s), Epoch: [2/10], Step: [3280/3335], Train Loss: 3.5277915835380553, BLEU: 0.25072586777551564\n",
      "\n",
      "Input1:> 从 1971 年 到 1977 年 我 看起 看起来 起来 很 年轻 但 其实 我 老 了 我 曾 在 赞比亚 比亚 肯尼 肯尼亚 尼亚 象牙 象牙海岸 海岸 阿尔及利亚 利亚 索马里 马里 参与 过 不少 与非 非洲 各国\n",
      "\n",
      "Target1:= From 1971 to 1977 -- I look young , but I am not — -- I worked in Zambia , Kenya , Ivory Coast , Algeria , Somalia , in projects of technical cooperation with African countries  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 呆 在 日本 的 第一 第一周 一周 的 最后 几天 我 发现 发现自己 自己 在 帮 UNK 的 一个 疏散 中心\n",
      "\n",
      "Target2:= At the end of my first week there , I found myself helping out in an evacuation center in the town  . \n",
      "Predict2:< I I I I , I , , , , , , , , , the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 17s (- -31m 42s), Epoch: [2/10], Step: [3300/3335], Train Loss: 3.5643555521965027, BLEU: 0.25897856919615\n",
      "\n",
      "Input1:> 所以 他们 给 他 一个 他们 能够 想到 的 最低 低微 的 最 微不足道 不足 不足道 的 最为 可悲 的 头衔\n",
      "\n",
      "Target1:= So they wanted to give him the UNK , UNK , most pathetic title that they could think of  . \n",
      "Predict1:< So they they to to to to , , , , , they , they to to  .\n",
      "\n",
      "Input2:> 虽然 我 也 时常 憧憬 外面 的 世界 我 以为 我会 在 朝鲜 鲜度 度过 我 的 一生 直到 所有 有事 事情 都 忽然 改变\n",
      "\n",
      "Target2:= Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed  . \n",
      "Predict2:< And I I I I I I I I , I , I , I , I I , I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 23s (- -31m 37s), Epoch: [2/10], Step: [3320/3335], Train Loss: 3.5327803254127503, BLEU: 0.26104215889208754\n",
      "\n",
      "Input1:> 但是 在 今天 一整 整个 家庭 被 世代 奴役 的 代价 可能 就 只有 18 美金 的 欠债\n",
      "\n",
      "Target1:= Yet today , entire families can be enslaved for generations over a debt as small as $ 18  . \n",
      "Predict1:< But the the the , , , , , , , , the the the\n",
      "\n",
      "Input2:> 我 突然 明白 我们 创造 的 最重 重要 的 事情 之一 是 我们 的 记忆\n",
      "\n",
      "Target2:= And it hit me : One of the most important things we all make are memories  . \n",
      "Predict2:< I I think we we , we , we we we we we  .  .\n",
      "[3.510052764415741, 3.5408966064453127, 3.2979426980018616, 3.397197902202606, 3.534105455875397, 3.482458567619324, 3.5161154866218567, 3.5318225264549254, 3.398360550403595, 3.456430661678314, 3.383474659919739, 3.470285475254059, 3.6210897326469422, 3.5140493512153625, 3.5426509499549867, 3.427303409576416, 3.4726403951644897, 3.520396375656128, 3.5963674068450926, 3.6712921142578123, 3.4518476128578186, 3.4884423136711122, 3.461902391910553, 3.489122951030731, 3.355366384983063, 3.366362953186035, 3.543223524093628, 3.4657609701156615, 3.456070053577423, 3.4908087849617004, 3.4379523396492004, 3.498739171028137, 3.3968768954277038, 3.586659252643585, 3.470386064052582, 3.519069766998291, 3.5215264678001406, 3.4380592703819275, 3.4574529767036437, 3.5381688833236695, 3.5082306385040285, 3.428694951534271, 3.5138978123664857, 3.720611560344696, 3.758130359649658, 3.5878894209861754, 3.426309847831726, 3.3840793490409853, 3.5230902314186094, 3.5231069207191466, 3.454880428314209, 3.6200636863708495, 3.623232400417328, 3.5490960836410523, 3.510421073436737, 3.4698970317840576, 3.495111036300659, 3.4560638070106506, 3.3580992221832275, 3.4656935334205627, 3.4209280133247377, 3.552421677112579, 3.603846085071564, 3.5228471994400024, 3.5175101161003113, 3.4965953707695006, 3.5607233881950378, 3.4644684314727785, 3.487239670753479, 3.6136677980422975, 3.668859434127808, 3.3869709253311155, 3.4813894867897033, 3.591507375240326, 3.5478954911231995, 3.518847572803497, 3.6335194945335387, 3.563645601272583, 3.493287992477417, 3.527575957775116, 3.4237345337867735, 3.3108620047569275, 3.464619541168213, 3.5175366401672363, 3.4650726675987245, 3.5197460532188414, 3.6093155264854433, 3.3871394872665403, 3.3616899728775023, 3.463534212112427, 3.4015641808509827, 3.3670650124549866, 3.466900622844696, 3.5745412945747375, 3.4924893021583556, 3.446205401420593, 3.4440111756324767, 3.530017352104187, 3.5103333830833434, 3.5252883195877076, 3.5064117431640627, 3.3985719680786133, 3.529053580760956, 3.4365368127822875, 3.393114244937897, 3.4845874667167664, 3.44054913520813, 3.5127314209938048, 3.6206875324249266, 3.4641669034957885, 3.565259349346161, 3.444185400009155, 3.5772144556045533, 3.447941815853119, 3.682283365726471, 3.407685875892639, 3.522149133682251, 3.457913064956665, 3.548243987560272, 3.5487733125686645, 3.5176759243011473, 3.5099847674369813, 3.4922006845474245, 3.4666547417640685, 3.518550789356232, 3.6120861768722534, 3.357075548171997, 3.5568914294242857, 3.4946786761283875, 3.518175733089447, 3.5185879111289977, 3.5501160621643066, 3.5492988228797913, 3.494688403606415, 3.47178772687912, 3.493012750148773, 3.425713002681732, 3.556734037399292, 3.4490211963653565, 3.482218933105469, 3.585243010520935, 3.5806391716003416, 3.621969020366669, 3.394731640815735, 3.493901228904724, 3.580247712135315, 3.4249997615814207, 3.5370273113250734, 3.511341166496277, 3.3515522599220278, 3.603215754032135, 3.622373306751251, 3.5310712099075316, 3.5453590035438536, 3.448008930683136, 3.5595922589302065, 3.485807418823242, 3.5061516165733337, 3.490537130832672, 3.3210412740707396, 3.430987286567688, 3.468885123729706, 3.437354254722595, 3.5277915835380553, 3.5643555521965027, 3.5327803254127503]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 30s (- -29m 0s), Epoch: [3/10], Step: [20/3335], Train Loss: 3.432033860683441, BLEU: 0.14902780914426722\n",
      "\n",
      "Input1:> 因为 在家 家庭 家庭暴力 暴力 中 的 最后 一步 就是 杀掉 她\n",
      "\n",
      "Target1:= Because the final step in the domestic violence pattern is kill her  . \n",
      "Predict1:< Because is is the , , , , ,  .  .\n",
      "\n",
      "Input2:> 但是 第一 第一次 一次 见面 给 我 留下 最深 印象 的 是 他 的 智慧 和 幽默 而 他 的 外貌 看起 看起来 起来 像 农场 男孩\n",
      "\n",
      "Target2:= But what made the biggest impression on me that first meeting was that he was smart and funny and he looked like a farm boy  . \n",
      "Predict2:< But he was , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 36s (- -30m 9s), Epoch: [3/10], Step: [40/3335], Train Loss: 3.348463034629822, BLEU: 0.17523208929378717\n",
      "\n",
      "Input1:> 我们 能 减少 和 病人 有关 的 后勤 问题\n",
      "\n",
      "Target1:= We can reduce logistical difficulties with patients  . \n",
      "Predict1:< We can we to to to  .  .\n",
      "\n",
      "Input2:> 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Have you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 41s (- -31m 48s), Epoch: [3/10], Step: [60/3335], Train Loss: 3.3758938670158387, BLEU: 0.23749152657159533\n",
      "\n",
      "Input1:> 图象 象是 强大 的 但 同时 又 是 表面 UNK 的\n",
      "\n",
      "Target1:= Image is powerful , but also image is superficial  . \n",
      "Predict1:< The is is is , is is is  .  .\n",
      "\n",
      "Input2:> 他们 中 的 其中 一个 就是 这位 老奶奶 奶奶\n",
      "\n",
      "Target2:= One of them was this grandmother  . \n",
      "Predict2:< They are a a a of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 47s (- -31m 35s), Epoch: [3/10], Step: [80/3335], Train Loss: 3.304491400718689, BLEU: 0.24306932432320721\n",
      "\n",
      "Input1:> 发生 了 什么\n",
      "\n",
      "Target1:= What is going on here  ? \n",
      "Predict1:< Have  ?  ?  ?\n",
      "\n",
      "Input2:> 食物 是 问题 食物 也 是 解决 解决方案 方案\n",
      "\n",
      "Target2:= Food is the problem and food is the solution  . \n",
      "Predict2:< The is is is is is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 52s (- -31m 25s), Epoch: [3/10], Step: [100/3335], Train Loss: 3.3428423404693604, BLEU: 0.24486056745078338\n",
      "\n",
      "Input1:> 对 我们 而言 它 渐渐 渐变 变得 神圣 起来\n",
      "\n",
      "Target1:= It is really become sacred to us  . \n",
      "Predict1:< And is we , , ,  .  .  .\n",
      "\n",
      "Input2:> 很多 人 认为 这个 短语 出自 出自于 金属 什么 的\n",
      "\n",
      "Target2:= Most people think that is down to the metal  . \n",
      "Predict2:< And is is is , is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30m 58s (- -31m 17s), Epoch: [3/10], Step: [120/3335], Train Loss: 3.411056566238403, BLEU: 0.24808076958867648\n",
      "\n",
      "Input1:> 你 会 好奇 是否 会 有效\n",
      "\n",
      "Target1:= And now you are of course curious if it also worked  . \n",
      "Predict1:< You you you you you  .  .\n",
      "\n",
      "Input2:> 因此 科学 科学家 学家 和 工程 工程师 们 当 你们 解决 了 这个 方程 方程式 程式 以后 就 想尽 一切 一切办法 办法 跟 我 说 说 高科技 科技 吧\n",
      "\n",
      "Target2:= And so , scientists and engineers , when you ve solved this equation , by all means , talk nerdy to me  . \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 3s (- -31m 9s), Epoch: [3/10], Step: [140/3335], Train Loss: 3.450154161453247, BLEU: 0.23909443373868955\n",
      "\n",
      "Input1:> 在 死 之前 我 想 过 隐居 的 生活\n",
      "\n",
      "Target1:= &quot; Before I die , I want to live off the grid  . &quot; \n",
      "Predict1:< I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 锤 声 微波 微波炉 声 你们 也许 会同 同意 这条 路 很 不错\n",
      "\n",
      "Target2:= You probably all agree with me that this is a very nice road  . \n",
      "Predict2:< And is , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 9s (- -31m 2s), Epoch: [3/10], Step: [160/3335], Train Loss: 3.491550028324127, BLEU: 0.25891371685223397\n",
      "\n",
      "Input1:> 我 在 印度 拜访 过 一个 村子 全家 全家人 家人 都 是 丝绸 贸易 的 奴隶\n",
      "\n",
      "Target1:= I visited villages in India where entire families were enslaved in the silk trade  . \n",
      "Predict1:< I I I a a a a a , , , the  .  .\n",
      "\n",
      "Input2:> 我 第一 第一次 一次 有 了 自己 的 房子 公寓 有 了 第一 第一张 一张 信用 信用卡 我 还有 一个 非常 大 的 秘密\n",
      "\n",
      "Target2:= I had my first apartment , my first little green American Express card , and I had a very big secret  . \n",
      "Predict2:< I I I a , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 15s (- -32m 55s), Epoch: [3/10], Step: [180/3335], Train Loss: 3.474711549282074, BLEU: 0.2559003793431668\n",
      "\n",
      "Input1:> 我 不知 知道 那 张 是 怎么 怎么回事 回事 回事儿 事儿\n",
      "\n",
      "Target1:= That was , I don t know what happened there  . \n",
      "Predict1:< I I t , , , , ,  .  .\n",
      "\n",
      "Input2:> 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也 珍爱 UNK 女儿 的 家庭\n",
      "\n",
      "Target2:= I was very lucky to grow up in a family where education was prized and daughters were treasured  . \n",
      "Predict2:< I I a a a a a a a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 20s (- -32m 48s), Epoch: [3/10], Step: [200/3335], Train Loss: 3.5588143348693846, BLEU: 0.2552376828292196\n",
      "\n",
      "Input1:> 我们 周围 的 人 能 以 很多 多种 方式 来 帮 我们 把 生活 变得 更 美好\n",
      "\n",
      "Target1:= There are a lot of ways the people around us can help improve our lives  . \n",
      "Predict1:< We we we to to to to , , , , , , ,  .\n",
      "\n",
      "Input2:> 并且 它 一点 也 不显 显得 非常 极其 地位 显赫 比起 UNK 和 UNK 来\n",
      "\n",
      "Target2:= And it didn t have much more grandeur than the term &quot; foreman &quot; or &quot; overseer  . &quot; \n",
      "Predict2:< And it it it it , and , and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 26s (- -32m 42s), Epoch: [3/10], Step: [220/3335], Train Loss: 3.4791202425956724, BLEU: 0.2613191501818557\n",
      "\n",
      "Input1:> 当 她们 走出 矿井 时 全身 UNK\n",
      "\n",
      "Target1:= When they came out of the shaft , they were soaking wet from their own sweat  . \n",
      "Predict1:< When , , , , ,  .  .\n",
      "\n",
      "Input2:> 有 足够 的 空间 去 种植 七亿 亿两 两千 两千五百 千五 五百 五百万 百万 的 番茄\n",
      "\n",
      "Target2:= That is enough space to plant UNK million tomato plants  . \n",
      "Predict2:< And is the , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 31s (- -32m 36s), Epoch: [3/10], Step: [240/3335], Train Loss: 3.515571653842926, BLEU: 0.2664876754546074\n",
      "\n",
      "Input1:> 所以 在 过去 的 几年 里 我 尝试 着 以 不同 的 方式 如 用 贴纸 展板 和 粉笔 这些 简单 工具 来 在 公共 空间 里 与 邻居 分享 更多 的 东西\n",
      "\n",
      "Target1:= So over the past few years , I ve tried ways to share more with my neighbors in public space , using simple tools like stickers , stencils and chalk  . \n",
      "Predict1:< So , the , , , , , , , , , , , , , , , , , , , the , , the , the the the the the the the\n",
      "\n",
      "Input2:> 我 的 父亲 那 就是 他 他 是 他家 家族 中 第一 第一个 一个 受到 教育 的 人\n",
      "\n",
      "Target2:= And my father -- that is him -- he was the first ever in his family to receive an education  . \n",
      "Predict2:< And was was , , , , , is is is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 37s (- -32m 30s), Epoch: [3/10], Step: [260/3335], Train Loss: 3.5176634907722475, BLEU: 0.26017733263459525\n",
      "\n",
      "Input1:> 但是 这 一次 我 要 去 挖掘 那些 见 不得 光 的 秘密\n",
      "\n",
      "Target1:= But this time , I would see the skeletons hidden in the closet  . \n",
      "Predict1:< But I I I I to I , I  .  .  .\n",
      "\n",
      "Input2:> 绿地 组织 又 开始 安置 也许 是 20 个 花园\n",
      "\n",
      "Target2:= So Green UNK has gone on to plant maybe 20 gardens  . \n",
      "Predict2:< And , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 42s (- -32m 24s), Epoch: [3/10], Step: [280/3335], Train Loss: 3.515778589248657, BLEU: 0.25017417765326116\n",
      "\n",
      "Input1:> 突然 间 我 似乎 无法 骄傲 骄傲地 称 任何 一个 国家 为 我 自己 的 国家\n",
      "\n",
      "Target1:= Suddenly , there was no country I could proudly call my own  . \n",
      "Predict1:< And I I I I I I I , I I I I I I  .\n",
      "\n",
      "Input2:> 所以 我们 做 的 是 把 这些 光子 送进 送进去 进去 并重 重复 百万 万多 多次 每次 都 以 极好 的 同步 录像 然后 从 几个 GB 的 数据 中 我们 编织 起来 一幅 图 而 这幅 图\n",
      "\n",
      "Target2:= So , what we do is we send that bullet , those packet of photons , millions of times , and record again and again with very clever synchronization , and from the gigabytes of data , we \n",
      "Predict2:< So we we we to to the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 48s (- -32m 18s), Epoch: [3/10], Step: [300/3335], Train Loss: 3.5024274349212647, BLEU: 0.24938002071551152\n",
      "\n",
      "Input1:> 所以 洛杉矶 时报 报报 报道 了 斯蒂夫 蒂夫 洛佩兹 写 了 个 关于 它 的 故事 并 和议 议员 以及 一个 绿地 组织 的 成员 进行 了 交谈 他们 将 请愿 请愿书 愿书 书上 上传 传到 了 Change\n",
      "\n",
      "Target1:= So L .A . Times got ahold of it  . Steve Lopez did a story on it and talked to the UNK , and one of the Green UNK members , they put up a petition on Change .org , and \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 两个 词 来自 于 拉丁 词根 UNK 意 即 父亲\n",
      "\n",
      "Target2:= The two words come from the Latin root &quot; UNK , &quot; which means &quot; father  . &quot; \n",
      "Predict2:< And is is the , , , , the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 53s (- -32m 12s), Epoch: [3/10], Step: [320/3335], Train Loss: 3.4717066287994385, BLEU: 0.2544221462739444\n",
      "\n",
      "Input1:> 那个 善良 的 陌生 陌生人 生人 象征 着 我 以及 朝鲜 朝鲜人 最 需要 的 希望 他 使 我 明白 陌生 陌生人 生人 的 善意 和 国际 社会 的 支持 真是 我们 朝鲜 朝鲜人 需要 的 希望 之\n",
      "\n",
      "Target1:= The kind stranger symbolized new hope for me and the North Korean people when we needed it most , and he showed me the kindness of strangers and the support of the international community are truly the rays \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 做 了 老化 实验 加 了 很多 负载 用 我们 的 感应 UNK 电机 给 它们 加热 重复 加热 和 测试\n",
      "\n",
      "Target2:= So we did aging on the samples , did a lot of loading on it , healed them with our induction machine , and healed them and tested them again  . \n",
      "Predict2:< We we we we to and and and and and and and and and and and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31m 59s (- -32m 6s), Epoch: [3/10], Step: [340/3335], Train Loss: 3.5398224711418154, BLEU: 0.2642911528845329\n",
      "\n",
      "Input1:> 尽管 我们 都 是 朝鲜 UNK 族人 但是 我们 的 内在 已经 经变 变得 非常 不同 了 由于 67 年 的 分裂\n",
      "\n",
      "Target1:= We are all Korean , but inside , we have become very different due to 67 years of division  . \n",
      "Predict1:< And we we we we , , , , , , , , , , the\n",
      "\n",
      "Input2:> 你 可以 想象 这个 小镇 肯定 备受 摧残\n",
      "\n",
      "Target2:= As you can imagine , the town had been devastated  . \n",
      "Predict2:< You can can you you you  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 4s (- -32m 0s), Epoch: [3/10], Step: [360/3335], Train Loss: 3.60284823179245, BLEU: 0.26813953105822036\n",
      "\n",
      "Input1:> 我 觉得 人们 太 容易 被 日复一日 一日 的 琐碎 困住 而 忘记 什么 才 是 真正 重要 的 事\n",
      "\n",
      "Target1:= I feel like it is easy to get caught up in the day-to-day , and forget what really matters to you  . \n",
      "Predict1:< I I think to to to to to is is is is is is is  .\n",
      "\n",
      "Input2:> 没有 一个 朗朗 朗朗上口 上口 的 名字\n",
      "\n",
      "Target2:= Not that catchy  . \n",
      "Predict2:< There is a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 10s (- -33m 54s), Epoch: [3/10], Step: [380/3335], Train Loss: 3.346724581718445, BLEU: 0.252827537651623\n",
      "\n",
      "Input1:> 每周 我们 都 在 临时 照相 照相馆 相馆 馆里 架起 扫描 设备 人们 前来 这些 临时 照相 照相馆 相馆 领回 他们 的 照片\n",
      "\n",
      "Target1:= Once a week , we would set up our scanning equipment in the temporary photo libraries that had been set up , where people were reclaiming their photos  . \n",
      "Predict1:< And we we we , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 每当 有人 打喷嚏 喷嚏 时 我 都 感觉 新奥尔良 奥尔良 来 了 一只 游行 队伍\n",
      "\n",
      "Target2:= I feel like every time someone UNK , New Orleans has a parade  . \n",
      "Predict2:< And was , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 15s (- -33m 49s), Epoch: [3/10], Step: [400/3335], Train Loss: 3.343217670917511, BLEU: 0.25537354780255145\n",
      "\n",
      "Input1:> 2011 年 3 月 之前 我 是 摄影 修 片 师 常驻 纽约 纽约市\n",
      "\n",
      "Target1:= Before March , 2011 , I was a photographic UNK based in New York City  . \n",
      "Predict1:< And , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 怎么 会 有人 人愿 愿意 和 一个 一个打 她 的 男人 继续 住在一起 一起\n",
      "\n",
      "Target2:= Why would anyone stay with a man who beats her  ? \n",
      "Predict2:< And is a , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 21s (- -33m 43s), Epoch: [3/10], Step: [420/3335], Train Loss: 3.4043576955795287, BLEU: 0.25307273445125184\n",
      "\n",
      "Input1:> 掌声 谢谢\n",
      "\n",
      "Target1:= Thanks  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 我们 帮助 助学 学校 和 那些 被 埋没 的 房屋 打扫 清理 以便 重建 翻新\n",
      "\n",
      "Target2:= We cleaned schools  . We UNK and gutted homes ready for renovation and rehabilitation  . \n",
      "Predict2:< We we to to to and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 26s (- -33m 37s), Epoch: [3/10], Step: [440/3335], Train Loss: 3.634182059764862, BLEU: 0.251994705169739\n",
      "\n",
      "Input1:> 我们 在 这段 路上 取 了 很多 多样 样品 在 实验 实验室 室里 进行 各种 测试\n",
      "\n",
      "Target1:= So we took a lot of samples from this road and we tested them in the lab  . \n",
      "Predict1:< We we we the in in , , , , the the the  .\n",
      "\n",
      "Input2:> 没有 后门\n",
      "\n",
      "Target2:= There were no back doors  . \n",
      "Predict2:< Have is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 32s (- -33m 31s), Epoch: [3/10], Step: [460/3335], Train Loss: 3.5506060481071473, BLEU: 0.2547294286953453\n",
      "\n",
      "Input1:> 但是 这 一次 我 要 去 挖掘 那些 见 不得 光 的 秘密\n",
      "\n",
      "Target1:= But this time , I would see the skeletons hidden in the closet  . \n",
      "Predict1:< But I I I I I to I I  .  .  .\n",
      "\n",
      "Input2:> 而 我 永远 不会 再 拥有 的 永远 是 一个 嘴里 说 爱 我 的 人 用 上膛 了 的 枪 指着 我 的 脑袋\n",
      "\n",
      "Target2:= What I will never have again , ever , is a loaded gun held to my head by someone who says that he loves me  . \n",
      "Predict2:< And I I I , I , , , I , , , , , , , , , I ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 37s (- -33m 26s), Epoch: [3/10], Step: [480/3335], Train Loss: 3.425030994415283, BLEU: 0.2592591002318855\n",
      "\n",
      "Input1:> 今天 我 只要 要求 一件 事\n",
      "\n",
      "Target1:= Today I have just one request  . \n",
      "Predict1:< I I I I to  .  .  .\n",
      "\n",
      "Input2:> 我 确实 实有 太多 从没 机会 穿 的 8 英寸 高跟鞋 但是 免费 的 东西 却是 我 在 现实 现实生活 生活 中 得到 的 而 这 恰恰 是 我们 不 愿意 谈 的\n",
      "\n",
      "Target2:= I do have too many UNK heels which I never get to wear , except for earlier , but the free stuff that I get is the free stuff that I get in real life , and that \n",
      "Predict2:< I I I I I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 43s (- -33m 20s), Epoch: [3/10], Step: [500/3335], Train Loss: 3.4155769467353823, BLEU: 0.2597716621911647\n",
      "\n",
      "Input1:> 我们 时常 拍照\n",
      "\n",
      "Target1:= We take photos constantly  . \n",
      "Predict1:< We we to  .  .\n",
      "\n",
      "Input2:> 海啸 发生 的 当天 他 正在 负责 确保 海啸 防御 大门 处于 关闭 状态\n",
      "\n",
      "Target2:= The day of the tsunami , he d actually been in charge of making sure the tsunami gates were closed  . \n",
      "Predict2:< And he , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 48s (- -33m 14s), Epoch: [3/10], Step: [520/3335], Train Loss: 3.4313263416290285, BLEU: 0.29912266330684867\n",
      "\n",
      "Input1:> 由于 那里 里面 的 闷热 和 尘土 我 的 相机 烫得 没法 拿 也 无法 运转\n",
      "\n",
      "Target1:= So pervasive was the heat and the dust that my camera became too hot to even touch and ceased working  . \n",
      "Predict1:< And the , , , , , , , , , , and and and\n",
      "\n",
      "Input2:> 全球 超过 500 名 志愿 志愿者 愿者 帮 我们 找回 了 90 个 家庭 的 数百 数百张 百张 照片 这些 照片 全部 被 复原 修复\n",
      "\n",
      "Target2:= Over five hundred volunteers around the globe helped us get 90 families hundreds of photographs back , fully restored and UNK  . \n",
      "Predict2:< The is , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 54s (- -33m 9s), Epoch: [3/10], Step: [540/3335], Train Loss: 3.5071033358573915, BLEU: 0.36344930059429575\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 生长 着 八种 秃鹫 其中 的 六种 正 濒临 濒临灭绝 灭绝\n",
      "\n",
      "Target1:= We have eight species of vultures that occur in Kenya , of which six are highly threatened with extinction  . \n",
      "Predict1:< The , , , , , , , , the\n",
      "\n",
      "Input2:> 战争 和 被 驱逐 的 生活 怎样 制造 了 这个 充满 风格 和 时尚 的 环境 这个 只能 通过 过为 士兵 和 自杀 自杀性 爆炸 袭击 袭击者 设计 一款 时装 来 实现 于是 我 把 当地 阿富汗 狐狸\n",
      "\n",
      "Target2:= The war and the UNK life that comes with it have created this environment of style and fashion that can only be described through creating a fashion line for soldiers and suicide bombers where I take local Afghan \n",
      "Predict2:< And the , , , and , and , and , and , and and and and and and and and and and and and and and and and , and , and , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32m 59s (- -33m 3s), Epoch: [3/10], Step: [560/3335], Train Loss: 3.4279942512512207, BLEU: 0.30404948589797587\n",
      "\n",
      "Input1:> 她 的 两个 个儿 儿子 虽然 已经 不小 了 还 在 在 两个 不同 的 学校 上学\n",
      "\n",
      "Target1:= Her two little boys , not so little anymore , but her two boys were both at school , separate schools  . \n",
      "Predict1:< She was the the , , , , , , the the the  .\n",
      "\n",
      "Input2:> 我 希望 我们 给 这些 被 奴隶 的 人 一道 道光\n",
      "\n",
      "Target2:= I want to shine a light on slavery  . \n",
      "Predict2:< I I to to to to the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 5s (- -34m 58s), Epoch: [3/10], Step: [580/3335], Train Loss: 3.2689972877502442, BLEU: 0.25479427497746066\n",
      "\n",
      "Input1:> 每个 小 隔间 都 很暗 很脏 墙上 画 着 号码 由 UNK 夹板 和 窗帘 隔开\n",
      "\n",
      "Target1:= Each cubicle is dark and UNK , identified with a painted number on the wall , and partitioned by plywood and a curtain  . \n",
      "Predict1:< And , , , , , , , , , , and\n",
      "\n",
      "Input2:> 的 一些 人 实际 实际上 在 说 这 是 她 自己 的 错 就 好像 受害 受害者 是 有意 的 和 意图 摧残 她们 的 男人 相爱\n",
      "\n",
      "Target2:= is code for some people for , &quot; It is her fault for staying , &quot; as if victims intentionally choose to fall in love with men intent upon destroying us  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 10s (- -34m 52s), Epoch: [3/10], Step: [600/3335], Train Loss: 3.5191334962844847, BLEU: 0.26018935755155737\n",
      "\n",
      "Input1:> 但是 现在 你 知道 现在 有 多少 个 国家 有 总统 吗\n",
      "\n",
      "Target1:= But now , do you know how many nations have a president  ? \n",
      "Predict1:< But , you , you , you , you you  .\n",
      "\n",
      "Input2:> 现在 就 像 爱 哲 顿 博士 一个 科学 科学家 学家 科学 变成 了 艺术 一种 高速 摄影 摄影艺术 艺术 而且 我 意识 到 我们 每次 收集 到 的 大量 资料 不仅 是 科学 学成 成像 而且 还是\n",
      "\n",
      "Target2:= Now , like Doc Edgerton , a scientist himself , science became art , an art of UNK photography , and I realized that all the gigabytes of data that we are collecting every time is not just \n",
      "Predict2:< Now , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 16s (- -34m 46s), Epoch: [3/10], Step: [620/3335], Train Loss: 3.602749025821686, BLEU: 0.25765771150301725\n",
      "\n",
      "Input1:> 这 是 我 所生 生活 的 地方 肯尼 肯尼亚 尼亚 在内 内罗毕 国家 公园 的 南部\n",
      "\n",
      "Target1:= This is where I live  . I live in Kenya , at the south parts of the Nairobi National Park  . \n",
      "Predict1:< This is a the the of the of the of  .  .  .\n",
      "\n",
      "Input2:> 谢谢\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 21s (- -34m 41s), Epoch: [3/10], Step: [640/3335], Train Loss: 3.5743011951446535, BLEU: 0.31859661745927437\n",
      "\n",
      "Input1:> 回到 UNK 我 忍不住 不住 想 哭 但是 身旁 的 废 奴 主义 同伴 迅速 地拉 拉住 了 我 他 说 Lisa 别哭 别 在 这里 哭\n",
      "\n",
      "Target1:= Back in the kilns , I wanted to cry , but the UNK next to me quickly grabbed me and he said , &quot; Lisa , don t do that  . Just don t do that here  . \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 她 的 名字 叫 琼 对 我 来讲 她 就 像 我 的 母亲 一样 她 死 得 很 突然 没有 有人 预料 预料到 料到\n",
      "\n",
      "Target2:= Her name was Joan , and she was a mother to me , and her death was sudden and unexpected  . \n",
      "Predict2:< She she she , she , she , , she , , , , , she , , , , she\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 27s (- -34m 35s), Epoch: [3/10], Step: [660/3335], Train Loss: 3.5083127856254577, BLEU: 0.37563055904279347\n",
      "\n",
      "Input1:> 我 当时 也 不知 知道 第二 第二步 二步 是 孤立 受害 受害者\n",
      "\n",
      "Target1:= I also didn t know that the second step is to isolate the victim  . \n",
      "Predict1:< I I t , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 我 当时 就 意识 到 那 是 我 人生 很 有 象征 象征意义 意义 的 一个 时刻\n",
      "\n",
      "Target2:= I realized that this was a symbolic moment in my life  . \n",
      "Predict2:< I I I I I , I , I , I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 32s (- -34m 29s), Epoch: [3/10], Step: [680/3335], Train Loss: 3.5306931376457213, BLEU: 0.37795727349969466\n",
      "\n",
      "Input1:> 鼓掌 现在 请 记住 这 整个 事情 其实 实是 在 一 纳秒 内 发生 的 也 就是 光 走 的 时间 但 现在 我 再 把 这个 录像 放慢 一百 一百万 一百万倍 百万 百万倍 万倍 让 你 看到\n",
      "\n",
      "Target1:= Now , remember , the whole event is effectively taking place in less than a nanosecond — that is how much time it takes for light to travel — but I am slowing down in this video by \n",
      "Predict1:< Now , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 或许 它 正 发生 生在 你 的 身上\n",
      "\n",
      "Target2:= It may even be your story  . \n",
      "Predict2:< It is is you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 38s (- -34m 24s), Epoch: [3/10], Step: [700/3335], Train Loss: 3.586691904067993, BLEU: 0.3682233396216168\n",
      "\n",
      "Input1:> 因此 我 和 我 的 团队 洛杉矶 绿地 我们 聚在一起 一起 开始 种植 自己 的 食物 森林 果树 还有 整整 九种 蔬菜\n",
      "\n",
      "Target1:= So me and my group , L .A . Green UNK , we got together and we started planting my food forest , fruit trees , you know , the whole nine , vegetables  . \n",
      "Predict1:< So I I I to to to , , , , , and , and and and and and and\n",
      "\n",
      "Input2:> 其实 我 干 了 10 年\n",
      "\n",
      "Target2:= Actually , for 10 years  . \n",
      "Predict2:< I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 44s (- -34m 18s), Epoch: [3/10], Step: [720/3335], Train Loss: 3.456716072559357, BLEU: 0.31541115415233845\n",
      "\n",
      "Input1:> 这 张 是 在 911 的 几周 后 拍 的 我 那时 尝试 解释 911 那天 发生 的 事 试图 采用 让 这个 五岁 孩子 明白 的 方式\n",
      "\n",
      "Target1:= This one was taken just weeks after 9 / 11 , and I found myself trying to explain what had happened that day in ways a five-year-old could understand  . \n",
      "Predict1:< This is a a the , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 政府 指责 我 给 了 我 张 告 票 通知 我 必须 移除 我 的 花园 之后 告 票 变成 了 警告 函\n",
      "\n",
      "Target2:= The city came down on me , and basically gave me a citation saying that I had to remove my garden , which this citation was turning into a warrant  . \n",
      "Predict2:< And I I I I I I I I , I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 49s (- -34m 13s), Epoch: [3/10], Step: [740/3335], Train Loss: 3.543354880809784, BLEU: 0.2656331910105809\n",
      "\n",
      "Input1:> 我 是 如此 幸运 在 人生 中 得到 了 这么 多 帮助 和 启示 所以 我 想 帮助 那些 心怀 梦想 的 朝鲜 朝鲜人 在 国际 的 援助 下 一个 实现 梦想 的 机会\n",
      "\n",
      "Target1:= I ve been so lucky , received so much help and inspiration in my life , so I want to help give aspiring North Koreans a chance to prosper with international support  . \n",
      "Predict1:< I I I to , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 没有 任何 比较\n",
      "\n",
      "Target2:= They have nothing to compare it to  . \n",
      "Predict2:< It is t  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33m 55s (- -34m 7s), Epoch: [3/10], Step: [760/3335], Train Loss: 3.526156151294708, BLEU: 0.2663896205088839\n",
      "\n",
      "Input1:> 他 查看 每个 个人 的 身份 身份证 并 开始 问问 问题\n",
      "\n",
      "Target1:= He took everyone is I .D . cards , and he started asking them questions  . \n",
      "Predict1:< He is a , , , , , ,  .  .\n",
      "\n",
      "Input2:> 坑坑 坑坑洼洼 坑洼 洼 当然 会 变成 成问题 问题 但 我们 有 解决 的 办法\n",
      "\n",
      "Target2:= UNK , of course , that can become a problem , but we have a solution  . \n",
      "Predict2:< And , , , , , , , , , , , , to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 0s (- -34m 1s), Epoch: [3/10], Step: [780/3335], Train Loss: 3.4621673464775085, BLEU: 0.2568163018553996\n",
      "\n",
      "Input1:> 即使 他们 学会 了 中文 找到 了 工作 他们 的 整个 世界 也 会 在 顷刻 顷刻间 颠覆\n",
      "\n",
      "Target1:= Even after learning a new language and getting a job , their whole world can be turned upside down in an instant  . \n",
      "Predict1:< And they they they , , they , , they , they they they they the  .\n",
      "\n",
      "Input2:> 我 想要 种 一整 整个 街区 的 菜园 在 这个 街区 人们 能够 分享 食物\n",
      "\n",
      "Target2:= I want to plant a whole block of gardens where people can share in the food in the same block  . \n",
      "Predict2:< I I to to to the the the , , , , the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 6s (- -35m 56s), Epoch: [3/10], Step: [800/3335], Train Loss: 3.4048696517944337, BLEU: 0.2578432211883333\n",
      "\n",
      "Input1:> 谁 还 会 认真 认真对待 真对 对待 他 如果 他 只有 一个 这样 愚蠢 的 小头 头衔 比如 美利 美利坚 联邦 联邦政府 邦政府 政府 主持 UNK of the United UNK of America\n",
      "\n",
      "Target1:= And who is going to take him seriously if he is got a silly little title like President of the United States of America  ? \n",
      "Predict1:< And he , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 并且 我们 把 所有 的 原始 原始数据 数据 到 算进 算进去 进去 时 进行 很 有趣 的 处理\n",
      "\n",
      "Target2:= And we can take all that raw data and treat it in very interesting ways  . \n",
      "Predict2:< And we we to to the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 11s (- -35m 50s), Epoch: [3/10], Step: [820/3335], Train Loss: 3.449732446670532, BLEU: 0.2630365581682989\n",
      "\n",
      "Input1:> 你 只要 去 看看 模特 就 知道 了 因为 她们 有 最 瘦 的 大腿 最 漂亮 的 头发 最酷 的 衣服 而 她们 却 可能 是 世界 上 最 没有 有身 理 安全 安全感 的 女人\n",
      "\n",
      "Target1:= you just need to meet a group of models , because they have the thinnest thighs and the shiniest hair and the coolest clothes , and they are the most physically insecure women probably on the planet  . \n",
      "Predict1:< You know know , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 碰巧 一年 年后 我们 又 回到 了 纽约 纽约市 所以 我们 决定 拍 一张 同样 的 照片\n",
      "\n",
      "Target2:= We happened to be back in New York exactly a year later , so we decided to take the same picture  . \n",
      "Predict2:< And we , , , , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 17s (- -35m 45s), Epoch: [3/10], Step: [840/3335], Train Loss: 3.46313636302948, BLEU: 0.2564685135646088\n",
      "\n",
      "Input1:> 一个 真正 的 学校\n",
      "\n",
      "Target1:= A real school  . \n",
      "Predict1:< It is a  .\n",
      "\n",
      "Input2:> 我 告诉 你们 一个 秘密\n",
      "\n",
      "Target2:= Let me tell you a secret  . \n",
      "Predict2:< I I you you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 22s (- -35m 39s), Epoch: [3/10], Step: [860/3335], Train Loss: 3.4431886434555055, BLEU: 0.2577231953435881\n",
      "\n",
      "Input1:> 直至 现在 我 才 意识 到 丢失 了 这些 照片 肯定 是 这些 人 失去 的 一块 重大 大部 大部分 部分\n",
      "\n",
      "Target1:= Now , it wasn t until this point that I realized that these photos were such a huge part of the personal loss these people had felt  . \n",
      "Predict1:< Now , I , , , , , , , , , , , the the the the  .\n",
      "\n",
      "Input2:> 我 在 加纳 地下 50 米 深 的 一个 非法 矿井 里\n",
      "\n",
      "Target2:= I am 150 feet down an illegal mine shaft in Ghana  . \n",
      "Predict2:< I I a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 28s (- -35m 34s), Epoch: [3/10], Step: [880/3335], Train Loss: 3.5005523800849914, BLEU: 0.31293926963893165\n",
      "\n",
      "Input1:> 我 的 父亲 那 就是 他 他 是 他家 家族 中 第一 第一个 一个 受到 教育 的 人\n",
      "\n",
      "Target1:= And my father -- that is him -- he was the first ever in his family to receive an education  . \n",
      "Predict1:< And was was , , , , , , is is  .  .\n",
      "\n",
      "Input2:> 我们 总 呆 在 漆黑 漆黑一片 一片 不见 UNK 见天 见天日 天日 的 屋子 屋子里 工作 工作室 没有 窗 主要 要是 为了 抵御 阳光\n",
      "\n",
      "Target2:= We hide in dark , windowless rooms , and generally avoid sunlight  . \n",
      "Predict2:< We we to to to , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 33s (- -35m 28s), Epoch: [3/10], Step: [900/3335], Train Loss: 3.543252694606781, BLEU: 0.36577670149221386\n",
      "\n",
      "Input1:> 这种 人 不存 存在\n",
      "\n",
      "Target1:= It doesn t exist  . \n",
      "Predict1:< It is is the  .\n",
      "\n",
      "Input2:> 谢谢 大家\n",
      "\n",
      "Target2:= Thank you very much  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 39s (- -35m 23s), Epoch: [3/10], Step: [920/3335], Train Loss: 3.3723565340042114, BLEU: 0.4703039263053211\n",
      "\n",
      "Input1:> 你 可以 看到 到底 底部 的 颤动 轨迹 是 一种 不规则 规则 的 声 UNK 颤动\n",
      "\n",
      "Target1:= So on the bottom trace , you can see an example of irregular vocal fold tremor  . \n",
      "Predict1:< You can can you the the the the the the the the the the\n",
      "\n",
      "Input2:> 非法 非法性 性交 性交易 交易 是 我们 在 听到 奴役 时 最常 联想 想到 的 因为 全球 皆知 我 曾 被 警告 说 若 我 坚持 拍摄 这个 行业 人身 人身安全 安全 将 难以 得到 保障\n",
      "\n",
      "Target2:= Sex trafficking is what we often think of when we hear the word slavery , and because of this worldwide awareness , I was warned that it would be difficult for me to work safely within this particular \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 44s (- -35m 17s), Epoch: [3/10], Step: [940/3335], Train Loss: 3.4098299741744995, BLEU: 0.45043644616215006\n",
      "\n",
      "Input1:> 我 没想 想着 免费 因为 免费 是 不能 持续 的\n",
      "\n",
      "Target1:= I am not talking about no free shit , because free is not sustainable  . \n",
      "Predict1:< I I t to that , is  .  .  .\n",
      "\n",
      "Input2:> 同时 沥青 沥青路 会 产生 很大 噪音\n",
      "\n",
      "Target2:= Also , asphalt can create a lot of noise  . \n",
      "Predict2:< And is the the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 50s (- -35m 11s), Epoch: [3/10], Step: [960/3335], Train Loss: 3.4004079818725588, BLEU: 0.4173049336538242\n",
      "\n",
      "Input1:> 我们 说 能到 到处 旅行 和 有创造力 创造 创造力 的 热情 的 灵感 不断 的 人 一起 起工 工作 真的 很棒\n",
      "\n",
      "Target1:= We say , &quot; It is really amazing to travel , and it is amazing to get to work with creative , inspired , passionate people  . &quot; \n",
      "Predict1:< We we to to to , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 不论 不论是 大公 公司 还是 小企业 企业 都 能 把 以下 三件 事 做 得 很 好 产品 产品质量 品质 质量 要 过硬 市场 市场营销 营销 要 高效 财务 UNK 管理 要 精明\n",
      "\n",
      "Target2:= The smallest company , the biggest company , has to be capable of doing three things beautifully : The product that you want to sell has to be fantastic , you have to have fantastic marketing , and \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 55s (- -35m 6s), Epoch: [3/10], Step: [980/3335], Train Loss: 3.3647728323936463, BLEU: 0.45897625211063925\n",
      "\n",
      "Input1:> 这 就是 洛杉矶 UNK 贩 酒店 快餐 快餐店 闲置 的 土地\n",
      "\n",
      "Target1:= This is South Central : liquor stores , fast food , vacant lots  . \n",
      "Predict1:< This is a the the the the the  .  .  .\n",
      "\n",
      "Input2:> 我们 所 拥有 的 最 珍贵 的 两样 东西 一个 是 时间 还有 一个 是 与 他人 的 联系\n",
      "\n",
      "Target2:= Two of the most valuable things we have are time and our relationships with other people  . \n",
      "Predict2:< We are a a a a , , , , , , , , , the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 1s (- -35m 0s), Epoch: [3/10], Step: [1000/3335], Train Loss: 3.473980975151062, BLEU: 0.5131408611372741\n",
      "\n",
      "Input1:> 那么 最后 一个 问题 是 当 模特 是 什么 感觉\n",
      "\n",
      "Target1:= So the last question people ask me is , &quot; What is it like to be a model  ? &quot; \n",
      "Predict1:< So , is is is is is is is is  ?  ?\n",
      "\n",
      "Input2:> 但 两个 词 的 意思 不同\n",
      "\n",
      "Target2:= But they mean two different things  . \n",
      "Predict2:< But the is the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 6s (- -36m 55s), Epoch: [3/10], Step: [1020/3335], Train Loss: 3.4252326130867004, BLEU: 0.46185263061196796\n",
      "\n",
      "Input1:> 我们 第一 第一个 一个 项目 给 了 我 写 第一 第一本 一本 本书 的 灵感 书名 名叫 UNK 比西 西河 的 涟漪 在 这个 项目 目中 我们 意大利 意大利人 大利 利人 决定 教 赞比亚 比亚 人种 种粮 粮食\n",
      "\n",
      "Target1:= Our first project , the one that has inspired my first book , &quot; UNK from the UNK , &quot; was a project where we Italians decided to teach Zambian people how to grow food  . \n",
      "Predict1:< We we a a , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 苍白 UNK 无力 面如 UNK 死灰\n",
      "\n",
      "Target2:= We are pale , gray creatures  . \n",
      "Predict2:< We we to to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 12s (- -36m 49s), Epoch: [3/10], Step: [1040/3335], Train Loss: 3.506591558456421, BLEU: 0.4722623482254126\n",
      "\n",
      "Input1:> 事实 事实上 为了 女儿 的 美好 美好未来 未来 她们 通常 是 最初 和 最 有 说服 说服力 的 交涉 者 但是 在 像 阿富汗 这样 的 社会 中 我们 必须 有 男性 的 支持\n",
      "\n",
      "Target1:= In fact , they are often the initial and convincing negotiators of a bright future for their daughters , but in the context of a society like in Afghanistan , we must have the support of men  . \n",
      "Predict1:< In fact , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 一 到 那里 我 就 去 看 了 个 大概\n",
      "\n",
      "Target2:= When we first arrived , I went to have a quick look  . \n",
      "Predict2:< We we to to , , , ,  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 17s (- -36m 44s), Epoch: [3/10], Step: [1060/3335], Train Loss: 3.5069096803665163, BLEU: 0.3930284002703061\n",
      "\n",
      "Input1:> 帮助 一个 公社 温泉 清扫 社里 的 巨大 浴盆\n",
      "\n",
      "Target1:= I was helping clean the UNK , the communal UNK , the huge giant UNK  . \n",
      "Predict1:< And is a a a a  .  .\n",
      "\n",
      "Input2:> 我 不知 知道 你们 的 情况 但 我 个人 而言 除了 这 15 张 照片 我 不会 经常 经常出现 出现 现在 在家 家庭 照片 里面\n",
      "\n",
      "Target2:= I don t know about you , but aside from these 15 shots , I am not in many of the family photos  . \n",
      "Predict2:< I I I I , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 23s (- -36m 38s), Epoch: [3/10], Step: [1080/3335], Train Loss: 3.352100110054016, BLEU: 0.3762770923222555\n",
      "\n",
      "Input1:> 因为 我们 至少 喂饱 了 河马\n",
      "\n",
      "Target1:= Because , you see , at least we fed the hippos  . \n",
      "Predict1:< Because we we we we  .  .  .\n",
      "\n",
      "Input2:> 我 的 故事 源 起 于 有 一次 我 获 邀 到 纽约 纽约市 演讲 我 的 妻子 拍 了 张 照片 我 抱 着 我 的 女儿 那天 是 她 的 一岁 生日 我们 站 在 第\n",
      "\n",
      "Target2:= My story begins when I was in New York City for a speaking engagement , and my wife took this picture of me holding my daughter on her first birthday  . We are on the corner of UNK \n",
      "Predict2:< I was my , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 28s (- -36m 33s), Epoch: [3/10], Step: [1100/3335], Train Loss: 3.421431863307953, BLEU: 0.2732978717039123\n",
      "\n",
      "Input1:> 现在 超人 能 飞\n",
      "\n",
      "Target1:= So , Superman can fly  . \n",
      "Predict1:< Now , is  .  .\n",
      "\n",
      "Input2:> 我们 没有 自由 他们 说道\n",
      "\n",
      "Target2:= &quot; We have no freedom , &quot; they said  . \n",
      "Predict2:< We are t t  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 34s (- -36m 27s), Epoch: [3/10], Step: [1120/3335], Train Loss: 3.5794066786766052, BLEU: 0.2680989822538931\n",
      "\n",
      "Input1:> 你 的 钱 可能 被 偷 你 可能 因 战争 而 背井离乡 离乡\n",
      "\n",
      "Target1:= Your money can be stolen  . You can be forced to leave your home during a war  . \n",
      "Predict1:< You you you you , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target2:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict2:< He he is , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 39s (- -36m 22s), Epoch: [3/10], Step: [1140/3335], Train Loss: 3.4529467940330507, BLEU: 0.27243259214260357\n",
      "\n",
      "Input1:> 很多 人 认为 这个 短语 出自 出自于 金属 什么 的\n",
      "\n",
      "Target1:= Most people think that is down to the metal  . \n",
      "Predict1:< And is is , , , is  .  .\n",
      "\n",
      "Input2:> 我 觉得 人们 太 容易 被 日复一日 一日 的 琐碎 困住 而 忘记 什么 才 是 真正 重要 的 事\n",
      "\n",
      "Target2:= I feel like it is easy to get caught up in the day-to-day , and forget what really matters to you  . \n",
      "Predict2:< I I think to to that is , is , is is is is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 45s (- -36m 16s), Epoch: [3/10], Step: [1160/3335], Train Loss: 3.490801417827606, BLEU: 0.3873674744484144\n",
      "\n",
      "Input1:> 现实 实是 当 你 毕业 后 有 了 简历 做 过 了 几份 工作 你 就 不能 再说 其它 的 了 所以 如果 你 说 你 想成 成为 美国 总统 但 你 的 简历 上 写 着 有\n",
      "\n",
      "Target1:= Unfortunately after you ve gone to school , and you have a résumé and you ve done a few jobs , you can t say anything anymore , so if you say you want to be the President \n",
      "Predict1:< And you , you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 这里 工作 的 人 通常 都 要 忍受 客人 人们 残忍 的 性虐 性虐待 虐待\n",
      "\n",
      "Target2:= The workers here often endure tragic sexual abuse at the hands of their customers  . \n",
      "Predict2:< In the the , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 50s (- -36m 10s), Epoch: [3/10], Step: [1180/3335], Train Loss: 3.477327656745911, BLEU: 0.4566861810203635\n",
      "\n",
      "Input1:> 我们 苍白 UNK 无力 面如 UNK 死灰\n",
      "\n",
      "Target1:= We are pale , gray creatures  . \n",
      "Predict1:< We we we to to  .  .\n",
      "\n",
      "Input2:> 我 总是 那个 拿 相机 拍照 的 人\n",
      "\n",
      "Target2:= I am always the one taking the picture  . \n",
      "Predict2:< I I I to the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 35m 56s (- -36m 5s), Epoch: [3/10], Step: [1200/3335], Train Loss: 3.5241736650466917, BLEU: 0.31706545045305656\n",
      "\n",
      "Input1:> 因此 我 做 的 就是 在 我 的 房子 前 种 上 食物 森林\n",
      "\n",
      "Target1:= So what I did , I planted a food forest in front of my house  . \n",
      "Predict1:< So I I I I I the the the the the the the the  .  .\n",
      "\n",
      "Input2:> 最重 重要 的 是 热情\n",
      "\n",
      "Target2:= The most important thing is passion  . \n",
      "Predict2:< The is is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 1s (- -37m 59s), Epoch: [3/10], Step: [1220/3335], Train Loss: 3.444532608985901, BLEU: 0.46256922367504993\n",
      "\n",
      "Input1:> 尽管 我 足够 幸运 运能 离开 朝鲜 我 的 好多 同胞 却 没有 这样 的 好运 好运气 运气\n",
      "\n",
      "Target1:= Even though I was really fortunate to get out , many other North Koreans have not been so lucky  . \n",
      "Predict1:< And I I I I I , I , I , I , I I I  .\n",
      "\n",
      "Input2:> 尽管 我们 都 是 朝鲜 UNK 族人 但是 我们 的 内在 已经 经变 变得 非常 不同 了 由于 67 年 的 分裂\n",
      "\n",
      "Target2:= We are all Korean , but inside , we have become very different due to 67 years of division  . \n",
      "Predict2:< And we we we we we , we , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 7s (- -37m 54s), Epoch: [3/10], Step: [1240/3335], Train Loss: 3.3564573407173155, BLEU: 0.38788626572755514\n",
      "\n",
      "Input1:> 是 创业 创业者 业者 们 是 他们 正在 为 之 奋斗\n",
      "\n",
      "Target1:= It will be entrepreneurs , and they are doing it now  . \n",
      "Predict1:< It is , , , , they  .  .  .\n",
      "\n",
      "Input2:> 一旦 价格 价格低 价格低廉 低廉 它 也 就 能 被 大规 大规模 规模 地 扩展\n",
      "\n",
      "Target2:= When something becomes ultra-low cost , it becomes massively scalable  . \n",
      "Predict2:< And it , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 12s (- -37m 48s), Epoch: [3/10], Step: [1260/3335], Train Loss: 3.456550693511963, BLEU: 0.4862703962089635\n",
      "\n",
      "Input1:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target1:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict1:< He he is , , , , , , he , , , , he ,  .\n",
      "\n",
      "Input2:> 你 可以 又 一次 看到 波纹 在 桌子 上 展开 背景 是 西红柿 和 墙\n",
      "\n",
      "Target2:= You can watch the ripples again washing over the table , the tomato and the wall in the back  . \n",
      "Predict2:< You you see see , , , , , , , , the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 18s (- -37m 43s), Epoch: [3/10], Step: [1280/3335], Train Loss: 3.40307195186615, BLEU: 0.4705034442042886\n",
      "\n",
      "Input1:> 可以 想一想 小孩 学说 学说话 说话 得 花 多长 多长时间 长时间 时间\n",
      "\n",
      "Target1:= I mean , just think how long it takes a child to learn to speak  . \n",
      "Predict1:< You can can , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 但是 第一 第一次 一次 见面 给 我 留下 最深 印象 的 是 他 的 智慧 和 幽默 而 他 的 外貌 看起 看起来 起来 像 农场 男孩\n",
      "\n",
      "Target2:= But what made the biggest impression on me that first meeting was that he was smart and funny and he looked like a farm boy  . \n",
      "Predict2:< But he was , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 24s (- -37m 37s), Epoch: [3/10], Step: [1300/3335], Train Loss: 3.5466930150985716, BLEU: 0.43683699337038123\n",
      "\n",
      "Input1:> 那么 秃鹫 都 面临 着 哪些 问题 呢\n",
      "\n",
      "Target1:= So what is the problem with vultures  ? \n",
      "Predict1:< So , is is  ?  ?\n",
      "\n",
      "Input2:> 我 生活 在 洛杉矶 UNK\n",
      "\n",
      "Target2:= I live in South Central  . \n",
      "Predict2:< I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 29s (- -37m 32s), Epoch: [3/10], Step: [1320/3335], Train Loss: 3.419064521789551, BLEU: 0.373435178324693\n",
      "\n",
      "Input1:> 尽管 适应 韩国 的 生活 并 不是 易事 但是 我 还是 决定\n",
      "\n",
      "Target1:= Even though adjusting to life in South Korea was not easy , I made a plan  . \n",
      "Predict1:< And , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 而且 这 是 很重 重要 的 一 课 我 认为 你们 能够 够学 学到 的 也 是 我 想 留给 你们 的 要点\n",
      "\n",
      "Target2:= And that is the important lesson I think you can take away , and the one I want to leave you with  . \n",
      "Predict2:< And is is is I I I , I , I , I , I , I , I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 35s (- -37m 26s), Epoch: [3/10], Step: [1340/3335], Train Loss: 3.5078872203826905, BLEU: 0.32356792269101436\n",
      "\n",
      "Input1:> 你 知道 吗 你 可以 选择 任何 的 职业\n",
      "\n",
      "Target1:= You know  ? You can be anything  . \n",
      "Predict1:< You you you you you you you you  .\n",
      "\n",
      "Input2:> 掌声 随着 时间 时间推移 推移 越来 越来越 越多 需要 修复 的 照片 被 运送 送过来 过来 我们 也 需要 更多 修 图 师 来 帮忙 处理 这些 照片 于是 我 又 登陆 脸谱 网 和 职业 链接 网站\n",
      "\n",
      "Target2:= Over time , more photos came in , thankfully , and more retouchers were needed , and so I reached out again on Facebook and LinkedIn , and within five days , 80 people wanted to help from \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 40s (- -37m 21s), Epoch: [3/10], Step: [1360/3335], Train Loss: 3.54655065536499, BLEU: 0.47450931221052106\n",
      "\n",
      "Input1:> 然而 世上 上有 630 万 的 人 患有 这种 疾病 而且 他们 还 得 忍受 无法 法治 治愈 的 虚弱 震颤 僵化 等 其他 伴随 而来 的 症状 的 折磨 所以 我们 需要 客观 的 工具 在 病重\n",
      "\n",
      "Target1:= However , there are 6 .3 million people worldwide who have the disease , and they have to live with incurable weakness , tremor , rigidity and the other symptoms that go along with the disease , so what \n",
      "Predict1:< But the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 今天 我 站 在 这里 是因为 因为 我 是 个 模特\n",
      "\n",
      "Target2:= And I am on this stage because I am a model  . \n",
      "Predict2:< I I I I I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 46s (- -37m 15s), Epoch: [3/10], Step: [1380/3335], Train Loss: 3.468257796764374, BLEU: 0.4340362280643796\n",
      "\n",
      "Input1:> 我心 心想 搞 什么 来 真的\n",
      "\n",
      "Target1:= And I am like , &quot; Come on , really  ? \n",
      "Predict1:< And , is ,  ?  ?\n",
      "\n",
      "Input2:> 事实 事实上 这项 实验 得到 的 结论 是 如果 我们 每 四年 在 这段 路上 用 我们 的 加热 热机 机器 超大 大号 微波 微波炉 我们 要 把 这个 机器 搬到 公路 路上 用 我们 每 四年 上\n",
      "\n",
      "Target2:= So actually , the conclusion from this research is that if we go on the road every four years with our healing machine -- this is the big version we have made to go on the real road \n",
      "Predict2:< In fact , , , we we we we we , we , we , we , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 51s (- -37m 9s), Epoch: [3/10], Step: [1400/3335], Train Loss: 3.4527866005897523, BLEU: 0.3980712159363011\n",
      "\n",
      "Input1:> 事实 实是 就 因为 他们 的 思维 不符 符合 社会 对 正常 正常人 常人 的 定义 他们 经常 被 孤立 被 误解\n",
      "\n",
      "Target1:= And in fact , because their minds don t fit into society is version of normal , they are often bypassed and misunderstood  . \n",
      "Predict1:< They they , they , they , they , they they they they they they they they they they\n",
      "\n",
      "Input2:> 这 就是 我 和 那些 学生 们 对话 时 的 感受 我 对 他们 的 想法 感到 惊奇 而且 我 希望 其他 其他人 他人 也 能 体验 这种 在 仙境 中 的 感觉\n",
      "\n",
      "Target2:= That is just how I felt as I had those conversations with the students  . I was amazed at the ideas that they had , and I wanted others to experience this wonderland as well  . \n",
      "Predict2:< And is is I I I I I I I I I I I I and and and and and and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36m 57s (- -37m 4s), Epoch: [3/10], Step: [1420/3335], Train Loss: 3.386165404319763, BLEU: 0.4000496251681309\n",
      "\n",
      "Input1:> 不是 的 它 是 出自 出自于 一个 新闻 新闻自由 自由 卫士\n",
      "\n",
      "Target1:= It is not  . It is down to a campaigner for the freedom of the press  . \n",
      "Predict1:< It is is is , , , is is  .  .  .\n",
      "\n",
      "Input2:> 所以 以为 为什么 什么 说 秃鹫 重要 呢 首先 秃鹫 对生 生态 循环 做出 了 至关 至关重要 重要 的 作用 他们 净化 净化环境 环境\n",
      "\n",
      "Target2:= So why are vultures important  ? First of all , they provide vital ecological services  . They clean up  . \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 2s (- -38m 59s), Epoch: [3/10], Step: [1440/3335], Train Loss: 3.455624055862427, BLEU: 0.2668751333480683\n",
      "\n",
      "Input1:> 我们 的 科学 科学家 学家 和 工程 工程师 们 解决 的 都 是 我们 所 面临 的 最大 的 难题 从 能源 到 环境 再 到 卫生 卫生保健 保健 等等 如果 我们 不明 明白 也 不理 理解 的话\n",
      "\n",
      "Target1:= Our scientists and engineers are the ones that are tackling our grandest challenges , from energy to environment to health care , among others , and if we don t know about it and understand it , then \n",
      "Predict1:< We we we the the , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 很 怪\n",
      "\n",
      "Target2:= That was awkward  . \n",
      "Predict2:< It is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 8s (- -38m 53s), Epoch: [3/10], Step: [1460/3335], Train Loss: 3.3951960802078247, BLEU: 0.3802610538675111\n",
      "\n",
      "Input1:> 客户 热衷 热衷于 自我 提升 的 这份 热情 才 是 最重 重要 的\n",
      "\n",
      "Target1:= The passion that the person has for her own growth is the most important thing  . \n",
      "Predict1:< And the is the the the is the is  .  .\n",
      "\n",
      "Input2:> 就 如 你 所见 由 太阳 太阳能 接收 板 来 给 电池 充电 再 由 电池 来 指示 指示器 给 提供 能量 我 把 这个 叫做 变压 变压器\n",
      "\n",
      "Target2:= As you can see , the solar panel charges the battery , and the battery supplies the power to the small indicator box  . I call it a transformer  . \n",
      "Predict2:< You , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 13s (- -38m 47s), Epoch: [3/10], Step: [1480/3335], Train Loss: 3.3031417965888976, BLEU: 0.3241162691056534\n",
      "\n",
      "Input1:> 所以 这些 照片 远远 远不止 不止 是 记录 了 一个 瞬间 或 一次 特别 别的 旅程\n",
      "\n",
      "Target1:= So these photos are far more than proxies for a single moment , or even a specific trip  . \n",
      "Predict1:< So , is a a , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 而 这 也 是 为什么 什么 它 在 街上 的 原因\n",
      "\n",
      "Target2:= That is why it is on the street  . \n",
      "Predict2:< And this is is is is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 19s (- -38m 42s), Epoch: [3/10], Step: [1500/3335], Train Loss: 3.480949032306671, BLEU: 0.48459474658651674\n",
      "\n",
      "Input1:> 我 目睹 了 我 的 花园 变为 为了 教育 的 工具 变为 为了 我 社区 改革 的 工具\n",
      "\n",
      "Target1:= I have witnessed my garden become a tool for the education , a tool for the transformation of my neighborhood  . \n",
      "Predict1:< I I my my my my my , my , , my , my ,  .\n",
      "\n",
      "Input2:> 我们 种 的 西红柿 漂亮 极了 在意 意大利 大利 一个 西红柿 长 这么 大 在 赞比亚 比亚 能 长 这么 大\n",
      "\n",
      "Target2:= We had these magnificent tomatoes  . In Italy , a tomato would grow to this size  . In Zambia , to this size  . \n",
      "Predict2:< We we a a a , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 24s (- -38m 36s), Epoch: [3/10], Step: [1520/3335], Train Loss: 3.432154667377472, BLEU: 0.4004505270424625\n",
      "\n",
      "Input1:> 我们 的 手工 清洗 活动 非常 顺利\n",
      "\n",
      "Target1:= Now , we were lucky with our UNK  . \n",
      "Predict1:< We are we to  .  .  .\n",
      "\n",
      "Input2:> 但 一些 静物 照相 又 如何 呢\n",
      "\n",
      "Target2:= And what about some still-life photography  ? \n",
      "Predict2:< But what do  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 30s (- -38m 31s), Epoch: [3/10], Step: [1540/3335], Train Loss: 3.6562718629837034, BLEU: 0.4587322914950637\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 即将 拥有 全 非洲 最大 大规 大规模 规模 的 风力 发电 发电厂 电厂 UNK 台 涡轮 轮发 发电 发电机 电机 即将 被 安装 在 图尔 UNK UNK 湖区\n",
      "\n",
      "Target1:= Kenya is going to have one of the largest wind farms in Africa : UNK wind turbines are going to be up at Lake Turkana  . \n",
      "Predict1:< The , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 最 坏 的 消息 是 我 要 把 这件 毛衣 从头 头上 UNK 套进 套进去 进去 因为 这时 这时候 时候 你们 会 笑 我 所以 当 我 穿衣 穿衣服 衣服 的 时候 你们 什么 都 不要 做\n",
      "\n",
      "Target2:= The worst part is putting this sweater over my head , because that is when you ll all laugh at me , so don t do anything while it is over my head  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 35s (- -38m 25s), Epoch: [3/10], Step: [1560/3335], Train Loss: 3.447623777389526, BLEU: 0.48489256182184376\n",
      "\n",
      "Input1:> 我心 心灰 心灰意冷\n",
      "\n",
      "Target1:= I lost all hope  . \n",
      "Predict1:< And is is  .  .\n",
      "\n",
      "Input2:> 我 的 邻居 邻居们 都 听说 了 这个 办法\n",
      "\n",
      "Target2:= And my neighboring homes heard about this idea  . \n",
      "Predict2:< I I , the  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 41s (- -38m 20s), Epoch: [3/10], Step: [1580/3335], Train Loss: 3.6008131742477416, BLEU: 0.4739830784867657\n",
      "\n",
      "Input1:> 我 想 跟 大家 分享 一些 人们 在 那 面墙 墙上 写 的 东西\n",
      "\n",
      "Target1:= And I d like to share a few things that people wrote on this wall  . \n",
      "Predict1:< I I to to to to to to to to to to to  .\n",
      "\n",
      "Input2:> 这 很 怪\n",
      "\n",
      "Target2:= That was awkward  . \n",
      "Predict2:< It is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 46s (- -38m 14s), Epoch: [3/10], Step: [1600/3335], Train Loss: 3.483093523979187, BLEU: 0.4057409420213362\n",
      "\n",
      "Input1:> 他 表现 现出 的 深深 的 UNK\n",
      "\n",
      "Target1:= And he was very , very sorry  . \n",
      "Predict1:< He he is the  .  .  .\n",
      "\n",
      "Input2:> 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 52s (- -38m 9s), Epoch: [3/10], Step: [1620/3335], Train Loss: 3.5019808292388914, BLEU: 0.44594418345517306\n",
      "\n",
      "Input1:> 我们 怎么 才能 能干 干预 这些 隐藏 于 舆论 之外 但 却 堪称 普遍 的 恶事\n",
      "\n",
      "Target1:= How can we affect something so insidious , yet so pervasive  ? \n",
      "Predict1:< We we we to to , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但 让 我 坚定 坚定信念 信念 的 是 即使 现实 如此 就算 他们 不 被 当作 正常 正常人 常人 看 这 也 只 说明 了 一件 事 他们 是非 非凡 的 虽然 患有 自闭 自闭症 但 非凡 无比\n",
      "\n",
      "Target2:= But what lifted my heart and strengthened my soul was that even though this was the case , although they were not seen as ordinary , this could only mean one thing : that they were extraordinary -- \n",
      "Predict2:< But I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37m 57s (- -38m 3s), Epoch: [3/10], Step: [1640/3335], Train Loss: 3.3515435695648192, BLEU: 0.4485471845204165\n",
      "\n",
      "Input1:> 在 沥青 沥青路 路上 上会 溅 起 很多 水\n",
      "\n",
      "Target1:= Then you can have a lot of splash water in the asphalt  . \n",
      "Predict1:< In the , , , , ,  .  .\n",
      "\n",
      "Input2:> 而且 这 是 很重 重要 的 一 课 我 认为 你们 能够 够学 学到 的 也 是 我 想 留给 你们 的 要点\n",
      "\n",
      "Target2:= And that is the important lesson I think you can take away , and the one I want to leave you with  . \n",
      "Predict2:< And is is is , I , I , I , , , , , , , I , I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 3s (- -39m 58s), Epoch: [3/10], Step: [1660/3335], Train Loss: 3.41634886264801, BLEU: 0.4544800771931094\n",
      "\n",
      "Input1:> 谢谢\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> UNK 今年 22 岁 又 高 又 帅\n",
      "\n",
      "Target2:= Remi is 22 , tall and very handsome  . \n",
      "Predict2:< And , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 8s (- -39m 52s), Epoch: [3/10], Step: [1680/3335], Train Loss: 3.4531784296035766, BLEU: 0.44407431789484914\n",
      "\n",
      "Input1:> 自从 我 足够 大拿 拿起 相机 以来 我 一直 热衷 摄影 但 我 今天 想 和 你们 分享 15 张 我 最 珍爱 的 照片 它们 没有 一张 是 我 拍 的\n",
      "\n",
      "Target1:= Photography has been my passion ever since I was old enough to pick up a camera , but today I want to share with you the 15 most treasured photos of mine , and I didn t take \n",
      "Predict1:< I I I I I I I I I I I I I I I I , I I I , I I I , I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 是 充满 创业 热情 的 当地 当地人 你们 为 他们 鼓掌\n",
      "\n",
      "Target2:= Local , passionate people  . That is who you have applauded  . \n",
      "Predict2:< It is the , , , ,  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 14s (- -39m 47s), Epoch: [3/10], Step: [1700/3335], Train Loss: 3.3925369024276733, BLEU: 0.3936331109660752\n",
      "\n",
      "Input1:> 我们 用 我们 的 双手 获得 获得成功 得成 成功\n",
      "\n",
      "Target1:= We had a victory on our hands  . \n",
      "Predict1:< We we we to the the  .  .  .\n",
      "\n",
      "Input2:> 冬天 很 舒服 但 夏天 却 极热\n",
      "\n",
      "Target2:= It was cozy in winter but extremely hot in summer  . \n",
      "Predict2:< The , is , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 19s (- -39m 41s), Epoch: [3/10], Step: [1720/3335], Train Loss: 3.5111898303031923, BLEU: 0.38990156365011785\n",
      "\n",
      "Input1:> 一天 一周 实际 实际上 是 一年\n",
      "\n",
      "Target1:= A day , a week  ? Actually , a whole year  . \n",
      "Predict1:< The is is , ,  .  .  .\n",
      "\n",
      "Input2:> 内燃 内燃机 燃机 的 燃料 总 有 耗尽 的 一天\n",
      "\n",
      "Target2:= The internal combustion engine is not sustainable  . \n",
      "Predict2:< And is , , , , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 25s (- -39m 36s), Epoch: [3/10], Step: [1740/3335], Train Loss: 3.345906901359558, BLEU: 0.38733576748634846\n",
      "\n",
      "Input1:> 所以 你 已经 拿到 了 这个 奖学 奖学金 Richard UNK 是 的\n",
      "\n",
      "Target1:= So you got this scholarship  . Yep  . \n",
      "Predict1:< So , you you the the the the the  .  .\n",
      "\n",
      "Input2:> 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "\n",
      "Target2:= It was the only way we both could be educated  . \n",
      "Predict2:< This is is we we we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 30s (- -39m 30s), Epoch: [3/10], Step: [1760/3335], Train Loss: 3.5192678689956667, BLEU: 0.43391773468585293\n",
      "\n",
      "Input1:> 在 死 之前 我 想 在 上百 上百万 百万 的 观众 面前 唱歌\n",
      "\n",
      "Target1:= &quot; Before I die , I want to sing for millions  . &quot; \n",
      "Predict1:< I I I I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 这些 迹象 表明 如果 我们 能 有 一个 表达 自己 的 意愿 并 与 他人 分享 的 机会 那么 公共 空间 将 发挥 巨大 的 作用\n",
      "\n",
      "Target2:= Together , we ve shown how powerful our public spaces can be if we are given the opportunity to have a voice and share more with one another  . \n",
      "Predict2:< And are are a , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 36s (- -39m 25s), Epoch: [3/10], Step: [1780/3335], Train Loss: 3.521306312084198, BLEU: 0.4573616815505892\n",
      "\n",
      "Input1:> 有时 有时候 时候 一些 年长 的 女士 从未 从未见过 扫描 扫描仪 待 她们 找到 自己 遗失 的 照片 后 在 十分 十分钟 分钟 内 她们 可以 把 照片 给 我们 进行 扫描 然后 后上 上传 至 云端 服务\n",
      "\n",
      "Target1:= The older ladies sometimes hadn t seen a scanner before , but within 10 minutes of them finding their lost photo , they could give it to us , have it scanned , uploaded to a cloud server \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 无法 想象 向日葵 有 多么 的 神奇 以及 它 将 对 人 产生 何种 影响\n",
      "\n",
      "Target2:= You just couldn t imagine how amazing a sunflower is and how it affects people  . \n",
      "Predict2:< You you you to the , , , , , , , the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 41s (- -39m 19s), Epoch: [3/10], Step: [1800/3335], Train Loss: 3.361321759223938, BLEU: 0.4782758685948917\n",
      "\n",
      "Input1:> 我 住 在 这栋 房子 附近 我 就 想 如何 让 它 在 这 片 社区 区里 成为 一个 更好 的 地方 也 思考 了 另 一件 事 这件 事 彻底 彻底改变 改变 了 我 的 人生\n",
      "\n",
      "Target1:= I live near this house , and I thought about how I could make it a nicer space for my neighborhood , and I also thought about something that changed my life forever  . \n",
      "Predict1:< I I I I I I I I , I , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 全球 超过 500 名 志愿 志愿者 愿者 帮 我们 找回 了 90 个 家庭 的 数百 数百张 百张 照片 这些 照片 全部 被 复原 修复\n",
      "\n",
      "Target2:= Over five hundred volunteers around the globe helped us get 90 families hundreds of photographs back , fully restored and UNK  . \n",
      "Predict2:< The is the , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 47s (- -39m 13s), Epoch: [3/10], Step: [1820/3335], Train Loss: 3.488546943664551, BLEU: 0.48366075351809656\n",
      "\n",
      "Input1:> 除了 这个 伤 以外 他 还 患有 肺结核 结核 即使 这样 他 也 必须 每天 在 井下 没日没夜 的 工作\n",
      "\n",
      "Target1:= On top of that , UNK has tuberculosis , yet he is still forced to work day in and day out in that mine shaft  . \n",
      "Predict1:< And he is the , , , , , , , , , , , , , he  .\n",
      "\n",
      "Input2:> 事实 事实上 我 要 先 让 它 冷却 下来 才能 向 你们 展示 加热 修复 是否 有效\n",
      "\n",
      "Target2:= Actually , it still has to cool down first before I can show you that the healing works  . \n",
      "Predict2:< And , , , , , , , , , , , , , to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 52s (- -39m 8s), Epoch: [3/10], Step: [1840/3335], Train Loss: 3.5447662472724915, BLEU: 0.4399985255638575\n",
      "\n",
      "Input1:> 我们 正在 做 什么 呢 我们 正在 研究 这些 秃鹫 我们 在 它们 身上 安装 定位 定位器\n",
      "\n",
      "Target1:= So what is being done  ? Well , we are conducting research on these birds  . We are putting transmitters on them  . \n",
      "Predict1:< We we we we we we we we we we we we we we we\n",
      "\n",
      "Input2:> 我们 这些 西方 援助 UNK 在 过去 50 年 里 向 非洲 非洲大陆 大陆 投入 了 两万 万亿 万亿美元 亿美元 美元\n",
      "\n",
      "Target2:= We Western donor countries have given the African continent two trillion American dollars in the last 50 years  . \n",
      "Predict2:< We we are in in in , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38m 58s (- -39m 2s), Epoch: [3/10], Step: [1860/3335], Train Loss: 3.3898482084274293, BLEU: 0.4770667423889254\n",
      "\n",
      "Input1:> 思考 死亡 能够 让 你 对 自己 的 人生 有 更 清醒 的 认识\n",
      "\n",
      "Target1:= Thinking about death clarifies your life  . \n",
      "Predict1:< And can you to to to to to to  .  .  .\n",
      "\n",
      "Input2:> 我 想 不能 再 这样 下去 了\n",
      "\n",
      "Target2:= And I figured , this has to stop  . \n",
      "Predict2:< I I t to to to  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 3s (- -40m 57s), Epoch: [3/10], Step: [1880/3335], Train Loss: 3.5354219913482665, BLEU: 0.4732302954709635\n",
      "\n",
      "Input1:> 和 你 的 孩子 你 的 同事 你 的 朋友 和 家人 讨论 这个 话题 将 虐待 UNK 之于 光\n",
      "\n",
      "Target1:= Show abuse the light of day by talking about it with your children , your coworkers , your friends and family  . \n",
      "Predict1:< And you you you you and and and and and and and and and and and and\n",
      "\n",
      "Input2:> 我 得到 这些 是因为 因为 我 的 外表 而 不是 我 是 身份 有些 人为 为了 他们 的 外表 而 不是 身份 付出 了 代价\n",
      "\n",
      "Target2:= And I got these free things because of how I look , not who I am , and there are people paying a cost for how they look and not who they are  . \n",
      "Predict2:< I I t I that , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 9s (- -40m 51s), Epoch: [3/10], Step: [1900/3335], Train Loss: 3.4646899700164795, BLEU: 0.47317415342068214\n",
      "\n",
      "Input1:> 所以 到 最后 参议 参议院 议院 胜利 了 众议 众议院 议院 输 了 因为 没有 有人 会 感到 UNK 体面 当 他们 被告 告知 他们 现在 是 美利 美利坚 联邦 联邦政府 邦政府 政府 总统\n",
      "\n",
      "Target1:= And so , in the end , the Senate won and the House of Representatives lost , because nobody is going to feel that humble when they are told that they are now the President of the United \n",
      "Predict1:< So they they , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 之所以 所以 决定 离开 是因为 因为 最后 一次 残暴 的 殴打 突破 了 我 能 承受 的 极限\n",
      "\n",
      "Target2:= I was able to leave , because of one final , sadistic beating that broke through my denial  . \n",
      "Predict2:< I I I , I , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 14s (- -40m 46s), Epoch: [3/10], Step: [1920/3335], Train Loss: 3.5203861474990843, BLEU: 0.47848621101001215\n",
      "\n",
      "Input1:> 不久 久之 之后 我 参加 做 志愿 志愿者 愿者 的 一个 一个组 组织 携手 志愿 志愿者 愿者 团队 马上 就 赶赴 了 现场 仅仅 在 几天 之内 在 当地 开展 应急 急救 救援 活动\n",
      "\n",
      "Target1:= Soon after , an organization I volunteer with , All Hands Volunteers , were on the ground , within days , working as part of the response efforts  . \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 很 感谢 你 UNK 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you so much  . Thank you  . \n",
      "Predict2:< Thank you you you  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 20s (- -40m 40s), Epoch: [3/10], Step: [1940/3335], Train Loss: 3.4856660962104797, BLEU: 0.4938659888370805\n",
      "\n",
      "Input1:> 那么 结果 呢 40 年 之后 1900 年 美利 美利坚 美利坚合 美利坚合众国 合众 合众国 国有 了 1 001 家 汽车 制造 制造商 有 1 001 家 之 多\n",
      "\n",
      "Target1:= So what happens  ? In 40 years  time , in the year 1900 , in the United States of America , there were 1,001 car manufacturing companies -- 1,001  . \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 微波 微波炉 跟 感应 UNK 电机 的 原理 类似\n",
      "\n",
      "Target2:= So a microwave is a similar system  . \n",
      "Predict2:< The is : the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 25s (- -40m 35s), Epoch: [3/10], Step: [1960/3335], Train Loss: 3.4613327860832213, BLEU: 0.4817559382681636\n",
      "\n",
      "Input1:> 所以 我 找到 了 一个 个旧 的 车用 蓄电 蓄电池 电池 一个 指示 指示器 这 是 一个 从 摩托 摩托车 托车 中 找到 的 小 装备 用来 控制 摩托 摩托车 托车 的 转向 转向灯 能够 让 灯 闪烁\n",
      "\n",
      "Target1:= So I got an old car battery , an indicator box  . It is a small device found in a motorcycle , and it helps motorists when they want to turn right or left  . It blinks  . \n",
      "Predict1:< So I I to to a a a a a , , , a , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 现在 我 给 你 展示 一种 新型 照相 技术 叫做 飞秒 成像 这 是 一种 技术 太 快 以至 以至于 至于 它 能 捕捉 捕捉到 捉到 光 的 运动\n",
      "\n",
      "Target2:= I present you a new type of photography , UNK , a new imaging technique so fast that it can create slow motion videos of light in motion  . \n",
      "Predict2:< Now I I to to to you you , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 31s (- -40m 29s), Epoch: [3/10], Step: [1980/3335], Train Loss: 3.3655490636825562, BLEU: 0.4799178234334497\n",
      "\n",
      "Input1:> 掌声 Chris Anderson 你 不会 相信 听到 你 这样 的 故事 是 多么 使 人 振奋\n",
      "\n",
      "Target1:= You have no idea how exciting it is to hear a story like yours  . \n",
      "Predict1:< And , you you you you , you you you you you\n",
      "\n",
      "Input2:> 战争 和 被 驱逐 的 生活 怎样 制造 了 这个 充满 风格 和 时尚 的 环境 这个 只能 通过 过为 士兵 和 自杀 自杀性 爆炸 袭击 袭击者 设计 一款 时装 来 实现 于是 我 把 当地 阿富汗 狐狸\n",
      "\n",
      "Target2:= The war and the UNK life that comes with it have created this environment of style and fashion that can only be described through creating a fashion line for soldiers and suicide bombers where I take local Afghan \n",
      "Predict2:< And the , , and , and , and and and and and and and and and and and and and and and and and and and , and , and , and , and ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 36s (- -40m 24s), Epoch: [3/10], Step: [2000/3335], Train Loss: 3.401737725734711, BLEU: 0.4966733399809729\n",
      "\n",
      "Input1:> 我 想要 欺骗 狮子 让 它们 误以为 以为 是 我 站 在 UNK 旁边\n",
      "\n",
      "Target1:= I was trying to trick the lions &#91; into thinking &#93; that I was standing near the cowshed  . \n",
      "Predict1:< I I I to to to to , , , , I  .  .\n",
      "\n",
      "Input2:> 但 最难 的 是 去 展现 性别 和 种族 种族歧视 歧视 的 职业 传统 因为 我 自己 本身 是 最大 的 受益 受益者\n",
      "\n",
      "Target2:= But mostly it was difficult to unpack a legacy of gender and racial oppression when I am one of the biggest beneficiaries  . \n",
      "Predict2:< But the the the , , , , , , , , , , , , , and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 42s (- -40m 18s), Epoch: [3/10], Step: [2020/3335], Train Loss: 3.436975920200348, BLEU: 0.4456927751734406\n",
      "\n",
      "Input1:> 我们 清理 了 大量 的 发臭 腐烂 的 鱼 尸骨 那些 鱼 都 是 当地 工厂 厂里 里加 加工 处理 的 鱼\n",
      "\n",
      "Target1:= We cleared tons and tons of stinking , rotting fish carcasses from the local fish processing plant  . \n",
      "Predict1:< We we the the the , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 内燃 内燃机 燃机 的 燃料 总 有 耗尽 的 一天\n",
      "\n",
      "Target2:= The internal combustion engine is not sustainable  . \n",
      "Predict2:< So , is the , , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 48s (- -40m 13s), Epoch: [3/10], Step: [2040/3335], Train Loss: 3.5407268166542054, BLEU: 0.4880345278220226\n",
      "\n",
      "Input1:> 而且 这 是 很重 重要 的 一 课 我 认为 你们 能够 够学 学到 的 也 是 我 想 留给 你们 的 要点\n",
      "\n",
      "Target1:= And that is the important lesson I think you can take away , and the one I want to leave you with  . \n",
      "Predict1:< And that is is I I I , I , I , I , , , , , I , I ,\n",
      "\n",
      "Input2:> 以上 这些 就是 我 觉得 能够 帮助 我们 大家 打开 那 扇 大门 的 钥匙 感受 科学 与 工程 工程学 仙境 的 好 方法\n",
      "\n",
      "Target2:= So I think these are just a few keys that can help the rest of us to open that door and see the wonderland that is science and engineering  . \n",
      "Predict2:< And , is , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 53s (- -40m 7s), Epoch: [3/10], Step: [2060/3335], Train Loss: 3.503669011592865, BLEU: 0.4831196446327621\n",
      "\n",
      "Input1:> 看看 以 一位 位非 非洲 女性 的 眼光 我们 所 带来 的 损害 有 多 大\n",
      "\n",
      "Target1:= Read it from an African woman , the damage that we have done  . \n",
      "Predict1:< And , , , , , , , , , the the the  .\n",
      "\n",
      "Input2:> 其实 我 干 了 10 年\n",
      "\n",
      "Target2:= Actually , for 10 years  . \n",
      "Predict2:< I I I I  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39m 59s (- -40m 2s), Epoch: [3/10], Step: [2080/3335], Train Loss: 3.4526479601860047, BLEU: 0.4044005867789439\n",
      "\n",
      "Input1:> 第一 第一个 一个 要 回答 我们 的 问题 高科技 科技 又 怎样\n",
      "\n",
      "Target1:= First question to answer for us : so what  ? \n",
      "Predict1:< The is is is we we we we we we we  .\n",
      "\n",
      "Input2:> 你们 为什么 什么 不早 说 你 又 没 问 过\n",
      "\n",
      "Target2:= &quot; Why didn t you tell us  ? &quot; &quot; You never asked  . &quot; \n",
      "Predict2:< You you you , you you you  ?  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 4s (- -41m 56s), Epoch: [3/10], Step: [2100/3335], Train Loss: 3.55556116104126, BLEU: 0.27184340915463673\n",
      "\n",
      "Input1:> 照片 是 一个 提醒 和 留念 提醒 我们 一些 人 一些 事 某个 地方 某 段 感情 某个 深爱 的 人\n",
      "\n",
      "Target1:= A photo is a reminder of someone or something , a place , a relationship , a loved one  . \n",
      "Predict1:< The is a a , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 这双 UNK 高跟鞋 很 不 舒服 而 好消息 消息 是 我 不 打算 穿 他们\n",
      "\n",
      "Target2:= These heels are very uncomfortable , so good thing I wasn t going to wear them  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 10s (- -41m 51s), Epoch: [3/10], Step: [2120/3335], Train Loss: 3.476121187210083, BLEU: 0.39141337002221505\n",
      "\n",
      "Input1:> 他 查看 每个 个人 的 身份 身份证 并 开始 问问 问题\n",
      "\n",
      "Target1:= He took everyone is I .D . cards , and he started asking them questions  . \n",
      "Predict1:< He is the to to , , ,  .  .  .\n",
      "\n",
      "Input2:> 没人 能 独自 自创 创立 一个 企业 没人\n",
      "\n",
      "Target2:= Nobody started a company alone  . No one  . \n",
      "Predict2:< It : : a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 15s (- -41m 45s), Epoch: [3/10], Step: [2140/3335], Train Loss: 3.493216836452484, BLEU: 0.39001848611567363\n",
      "\n",
      "Input1:> 第一 第一个 一个 问题 是 你 是 怎么 成为 模特 的\n",
      "\n",
      "Target1:= So the first question is , how do you become a model  ? \n",
      "Predict1:< The is is is is is is is  .  .  .\n",
      "\n",
      "Input2:> 谢谢\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 21s (- -41m 39s), Epoch: [3/10], Step: [2160/3335], Train Loss: 3.363050436973572, BLEU: 0.3255427823352328\n",
      "\n",
      "Input1:> 现在 我 所讲 讲述 的 这个 疯狂 爱情 的 故事 是 一个 伪装 伪装成 装成 爱情 的 心理 陷阱 每年 都 会 有 上百 上百万 百万 的 女性 甚至 也 包括 一些 男性 会 掉 进 这个 陷阱\n",
      "\n",
      "Target1:= I am here to tell you the story of crazy love , a psychological trap disguised as love , one that millions of women and even a few men fall into every year  . \n",
      "Predict1:< Now , I I , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 掌声 所以 政府 的 人 说 那 接着 做\n",
      "\n",
      "Target2:= So — — So the government says , &quot; Do it again  . &quot; \n",
      "Predict2:< So , , , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 26s (- -41m 34s), Epoch: [3/10], Step: [2180/3335], Train Loss: 3.471653413772583, BLEU: 0.4892402791988203\n",
      "\n",
      "Input1:> 他 只是 压力 太大 了 婚礼 的 筹备 和 我家 家庭 家庭成员 成员 的 到来 让 他 喘 不过 过气\n",
      "\n",
      "Target1:= He had just been really stressed out by the wedding and by becoming a family with me  . \n",
      "Predict1:< He he a a and and and and and and and and and and and and and and and and\n",
      "\n",
      "Input2:> 我 想 读 给 大家 一封 封电子邮件 电子 电子邮件 邮件 作为 结尾 我 收到 其中 一封 来自 辛迪 是 六个 六个月 之后 我 从 日本 回家 的 那天\n",
      "\n",
      "Target2:= I would like to conclude by reading an email I got from one of them , Cindy , the day I finally got back from Japan after six months  . \n",
      "Predict2:< I I I to to to to , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 32s (- -41m 28s), Epoch: [3/10], Step: [2200/3335], Train Loss: 3.476394999027252, BLEU: 0.43636741531267115\n",
      "\n",
      "Input1:> 然而 那时 他们 再 要 拿 回 这些 照片 所需 花 的 时间 就 截然 截然不同 不同 了 当然 这 取决 取决于 受损 程度\n",
      "\n",
      "Target1:= The time it took , however , to get it back is a completely different story , and it depended obviously on the damage involved  . \n",
      "Predict1:< But they , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 猜到 事情 是 怎么 发展 的\n",
      "\n",
      "Target2:= Well you can see where this is going  . \n",
      "Predict2:< You can can you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 37s (- -41m 23s), Epoch: [3/10], Step: [2220/3335], Train Loss: 3.39001225233078, BLEU: 0.4541667783440318\n",
      "\n",
      "Input1:> 因为 我 每年 们 拍 同样 的 照片 我们 的 视角 并 不一 一样 女儿 她 到达 新 的 里程 里程碑 我 透过 她 的 眼睛 看待 生活 看 她 如何 反应 如何 看待 一切\n",
      "\n",
      "Target1:= Because while we take the same photo , our perspectives change , and she reaches new milestones , and I get to see life through her eyes , and how she interacts with and sees everything  . \n",
      "Predict1:< Because I I , , I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 我 逐渐 渐地 变得 十分 厌恶 狮子\n",
      "\n",
      "Target2:= So I grew up hating lions so much  . \n",
      "Predict2:< So I I to to to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 43s (- -41m 17s), Epoch: [3/10], Step: [2240/3335], Train Loss: 3.4381833910942077, BLEU: 0.488403085545994\n",
      "\n",
      "Input1:> 我 在 埃斯 UNK 兰斯 的 第一 第一年 一年 只是 出去 遛 街 在 那些 日子 里 我 有 了 第一 第一位 一位 客户 我 帮助 了 这 第一 第一个 一个 个人 他 是 UNK UNK 利人 在\n",
      "\n",
      "Target1:= And so what I did in UNK that first year was to just walk the streets , and in three days I had my first client , and I helped this first guy who was smoking fish from \n",
      "Predict1:< I I was , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 不 打算 跟 你们 细说 这笔 钱 所 造成 的 损害\n",
      "\n",
      "Target2:= I am not going to tell you the damage that that money has done  . \n",
      "Predict2:< I I t to to you you , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 48s (- -41m 12s), Epoch: [3/10], Step: [2260/3335], Train Loss: 3.5358700156211853, BLEU: 0.4511827663849189\n",
      "\n",
      "Input1:> 那时 他们 有 一个 问题 要 面对 就是 怎样 称号 乔治 华盛 华盛顿 他们 的 领袖\n",
      "\n",
      "Target1:= And they had to face the question of what to call George Washington , their leader  . \n",
      "Predict1:< And they , a a , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我 意识 到 如果 我 不 反抗 这个 我 曾 深爱 的 男人 会 杀掉 我\n",
      "\n",
      "Target2:= I realized that the man who I loved so much was going to kill me if I let him  . \n",
      "Predict2:< I I I I I I I I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 54s (- -41m 6s), Epoch: [3/10], Step: [2280/3335], Train Loss: 3.4660480260849, BLEU: 0.4485665849160129\n",
      "\n",
      "Input1:> 所以 我们 可以 建立 一个 社区 在 这个 社区 区里 我们 有 来自 自小 小型 小型企业 企业 背景 的 辅助 商 在 咖啡 咖啡馆 中 酒吧 里 为 你 出谋划策 谋划 划策 我们 能 为 你 提供 这位\n",
      "\n",
      "Target1:= So we can create the community where we have facilitators who come from a small business background sitting in cafes , in bars , and your dedicated buddies who will do to you , what somebody did for \n",
      "Predict1:< So we we we we , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 绿地 组织 又 开始 安置 也许 是 20 个 花园\n",
      "\n",
      "Target2:= So Green UNK has gone on to plant maybe 20 gardens  . \n",
      "Predict2:< And , , , , , , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 40m 59s (- -41m 1s), Epoch: [3/10], Step: [2300/3335], Train Loss: 3.3955977320671082, BLEU: 0.454854057675839\n",
      "\n",
      "Input1:> 所以 我 找到 了 一个 个旧 的 车用 蓄电 蓄电池 电池 一个 指示 指示器 这 是 一个 从 摩托 摩托车 托车 中 找到 的 小 装备 用来 控制 摩托 摩托车 托车 的 转向 转向灯 能够 让 灯 闪烁\n",
      "\n",
      "Target1:= So I got an old car battery , an indicator box  . It is a small device found in a motorcycle , and it helps motorists when they want to turn right or left  . It blinks  . \n",
      "Predict1:< So I I a a a a a a , , a , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 不 愿意 像 其他 婴儿 那样 玩儿 实际 实际上 他 好像 对 我 一点 一点儿 点儿 也 不感兴趣 感兴 感兴趣 兴趣\n",
      "\n",
      "Target2:= He didn t want to play like the other babies did , and in fact , he didn t seem very interested in me whatsoever  . \n",
      "Predict2:< He he t , , , , , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 5s (- -42m 55s), Epoch: [3/10], Step: [2320/3335], Train Loss: 3.418517363071442, BLEU: 0.45074859879367785\n",
      "\n",
      "Input1:> 不是 的 它 是 出自 出自于 一个 新闻 新闻自由 自由 卫士\n",
      "\n",
      "Target1:= It is not  . It is down to a campaigner for the freedom of the press  . \n",
      "Predict1:< It is is is is , , is is  .  .\n",
      "\n",
      "Input2:> 康 纳 并 不是 回到 家 向 我 宣布 嘿 虽然 罗曼蒂克 之类 的 很棒 但是 我们 要 进入 下一阶段 阶段 了 我 要 孤立 你 然后 虐待 你 笑 所以 我 要 你 离开 你 自己 的\n",
      "\n",
      "Target2:= Now , UNK did not come home one day and announce , &quot; You know , hey , all this Romeo and Juliet stuff has been great , but I need to move into the next phase where \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 10s (- -42m 50s), Epoch: [3/10], Step: [2340/3335], Train Loss: 3.4356716156005858, BLEU: 0.4915389701597024\n",
      "\n",
      "Input1:> 但是 狮子 十分 聪明\n",
      "\n",
      "Target1:= But lions are very clever  . \n",
      "Predict1:< But , is the  .  .\n",
      "\n",
      "Input2:> 他们 都 是 伤病 暴力 的 受害 受害者\n",
      "\n",
      "Target2:= All of them are victim to injury , illness and violence  . \n",
      "Predict2:< They are the the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 16s (- -42m 44s), Epoch: [3/10], Step: [2360/3335], Train Loss: 3.4261411905288695, BLEU: 0.446113482767755\n",
      "\n",
      "Input1:> 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 这 是 沥青 就 像 我 说 的 石头 头中 中间 有 少量 的 粘合 粘合剂 合剂\n",
      "\n",
      "Target2:= It is a porous asphalt , like I said , so you have only a small amount of binder between the stones  . \n",
      "Predict2:< This is a a , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 21s (- -42m 39s), Epoch: [3/10], Step: [2380/3335], Train Loss: 3.523692560195923, BLEU: 0.45331744114860933\n",
      "\n",
      "Input1:> 我们 所 做 的 是 跟 他们 交朋友 朋友 并发 发现 他们 的 兴趣 和 意向\n",
      "\n",
      "Target1:= And what we do , we become friends , and we find out what that person wants to do  . \n",
      "Predict1:< We are are to the and and and and and and and and  .\n",
      "\n",
      "Input2:> 除了 这个 伤 以外 他 还 患有 肺结核 结核 即使 这样 他 也 必须 每天 在 井下 没日没夜 的 工作\n",
      "\n",
      "Target2:= On top of that , UNK has tuberculosis , yet he is still forced to work day in and day out in that mine shaft  . \n",
      "Predict2:< And he is the , , , , , , , , , , he , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 27s (- -42m 33s), Epoch: [3/10], Step: [2400/3335], Train Loss: 3.5080864667892455, BLEU: 0.4752131485152533\n",
      "\n",
      "Input1:> 所以 很多 很多年 多年 年前 我 有 个 想法 我们 为何 何不 不能 至少 少有 一次 做到 聆听 他们 的 想法 而 不是 把 我们 的 想法 强加 强加于 加于 他们 但 这 不能 以 社区 会议 的\n",
      "\n",
      "Target1:= So years and years ago , I had this idea : Why don t we , for once , instead of arriving in the community to tell people what to do , why don t , for once \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 然后 我 想 酷 就 这样 吧\n",
      "\n",
      "Target2:= And I was like , &quot; Cool  . Bring it  . &quot; \n",
      "Predict2:< And I I to to to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 32s (- -42m 28s), Epoch: [3/10], Step: [2420/3335], Train Loss: 3.4858195900917055, BLEU: 0.40807771847681634\n",
      "\n",
      "Input1:> 开车 车行 行驶 在 加纳 的 一条 路上 与 Free the UNK 的 伙伴 一起 一个 骑 着 UNK 单车 的 同行 同行人 行人 突然 加速 靠近 我们 的 车 敲 着 车窗\n",
      "\n",
      "Target1:= Driving down a road in Ghana with partners of Free the Slaves , a fellow UNK on a UNK suddenly sped up to our UNK and tapped on the window  . \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , the the the the the\n",
      "\n",
      "Input2:> 因为 在家 家庭 家庭暴力 暴力 中 的 最后 一步 就是 杀掉 她\n",
      "\n",
      "Target2:= Because the final step in the domestic violence pattern is kill her  . \n",
      "Predict2:< Because is is , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 38s (- -42m 22s), Epoch: [3/10], Step: [2440/3335], Train Loss: 3.495653784275055, BLEU: 0.38694396979243084\n",
      "\n",
      "Input1:> 五年 年前 我 曾 有 过 一个 有如 漫游 爱丽丝 仙境 的 经历\n",
      "\n",
      "Target1:= Five years ago , I experienced a bit of what it must have been like to be Alice in Wonderland  . \n",
      "Predict1:< And I a a a a a a a  .  .\n",
      "\n",
      "Input2:> 我 甚至 将 我 的 朋友 带回 我 的 社区 帮助 他们 在 没有 赶 狮 灯 的 人家 家里 里安 安装 这个 我 教 他们 怎么 安装\n",
      "\n",
      "Target2:= I even took my friends back to my community , and we are installing the lights to the homes which don t have &#91; any &#93; , and I am teaching them how to put them  . \n",
      "Predict2:< I I I I to , I , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 43s (- -42m 17s), Epoch: [3/10], Step: [2460/3335], Train Loss: 3.429253542423248, BLEU: 0.4889484983905156\n",
      "\n",
      "Input1:> 我们 是 一个 自由 的 组织 由来 来自 不同 同行 同行业 行业 不同 同城 城市 的 园丁 组成 大家 全都 是 志愿 志愿者 愿者 我们 所 做 的 一切 一切都是 义务 的\n",
      "\n",
      "Target1:= What we do , we are a UNK kind of group , where it is composed of gardeners from all walks of life , from all over the city , and it is completely volunteer , and everything \n",
      "Predict1:< We we a a a of , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 也 就是 为什么 什么 常春 常春藤 大学 的 学位 华尔街 的 工作 和 光明 的 未来 对 他 意义 重大\n",
      "\n",
      "Target2:= Which is why that Ivy League degree and the Wall Street job and his bright shiny future meant so much to him  . \n",
      "Predict2:< And is is is the the the the , , , , the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 49s (- -42m 11s), Epoch: [3/10], Step: [2480/3335], Train Loss: 3.590279281139374, BLEU: 0.4595572694545004\n",
      "\n",
      "Input1:> 我们 所 做 的 是 跟 他们 交朋友 朋友 并发 发现 他们 的 兴趣 和 意向\n",
      "\n",
      "Target1:= And what we do , we become friends , and we find out what that person wants to do  . \n",
      "Predict1:< We are them the the and and and and and and and and  .\n",
      "\n",
      "Input2:> 2011 年 3 月 之前 我 是 摄影 修 片 师 常驻 纽约 纽约市\n",
      "\n",
      "Target2:= Before March , 2011 , I was a photographic UNK based in New York City  . \n",
      "Predict2:< And , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41m 54s (- -42m 6s), Epoch: [3/10], Step: [2500/3335], Train Loss: 3.4211182475090025, BLEU: 0.4442886803393251\n",
      "\n",
      "Input1:> 鼓掌 瞧 在 洛杉矶 我 有 了 遗产\n",
      "\n",
      "Target1:= See , I have a legacy in South Central  . \n",
      "Predict1:< And is , , I  .  .  .  .\n",
      "\n",
      "Input2:> 这 也 就是 为什么 什么 在 非洲 白人 被称作 称作 老爷 也 就是 老板\n",
      "\n",
      "Target2:= That is why the white people in Africa are called &quot; UNK , &quot; boss  . \n",
      "Predict2:< And this is is is is is is is is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 0s (- -42m 0s), Epoch: [3/10], Step: [2520/3335], Train Loss: 3.5486456155776978, BLEU: 0.494674718720764\n",
      "\n",
      "Input1:> 其中 一个 孩子 的 学校 被 海啸 冲击 到 了\n",
      "\n",
      "Target1:= One of them got caught up in the water  . \n",
      "Predict1:< And is a a a , ,  .  .  .\n",
      "\n",
      "Input2:> 事实 事实上 它们 大多 大多数 多数 是 随机 找来 的 游客 拍 的\n",
      "\n",
      "Target2:= In fact , most of them were taken by random tourists  . \n",
      "Predict2:< In fact , , , they , they  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 5s (- -43m 55s), Epoch: [3/10], Step: [2540/3335], Train Loss: 3.5040220499038695, BLEU: 0.49101358330350064\n",
      "\n",
      "Input1:> 如今 芭蕾 需要 非凡 的 专业 专业知识 知识 和 杰出 的 专业 技巧 甚至 一定 的 初级 适应 适应性 这 可能 就是 所说 的 天赋\n",
      "\n",
      "Target1:= Now , ballet requires an extraordinary level of expertise and a high level of skill , and probably a level of initial suitability that may well have a genetic component to it  . \n",
      "Predict1:< And the the , , the the the the the , , , , , the the the the\n",
      "\n",
      "Input2:> 要 想 穿越 朝鲜 与 韩国 的 边境 几乎 是 不可 可能 的 所以 颇具 讽刺 意味 的 是 我 坐飞机 飞机 到 了 中国 然后 再 前往 中朝 边境\n",
      "\n",
      "Target2:= It is almost impossible to cross the border between North Korea and South Korea , so , ironically , I took a flight back to China and I headed toward the North Korean border  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 11s (- -43m 49s), Epoch: [3/10], Step: [2560/3335], Train Loss: 3.3527569651603697, BLEU: 0.48857534703711325\n",
      "\n",
      "Input1:> 灯泡 来自 于 一个 坏掉 的 手电 手电筒 电筒\n",
      "\n",
      "Target1:= And that is a small torch from a broken flashlight  . \n",
      "Predict1:< The : : a a a a  .\n",
      "\n",
      "Input2:> 他 骄傲 的 不仅 不仅仅 仅仅 是因为 因为 我 的 大学 学学 学历 也 是因为 因为 在 女性 中 我 是 第一 第一个 一个 在 女性 中 我 是 第一 第一个 一个 开车 车带 他 在 喀布尔 布尔\n",
      "\n",
      "Target2:= He not only UNK about my college degree , but also that I was the first woman , and that I am the first woman to drive him through the streets of Kabul  . \n",
      "Predict2:< And was was , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 17s (- -43m 43s), Epoch: [3/10], Step: [2580/3335], Train Loss: 3.526845729351044, BLEU: 0.4947187202859734\n",
      "\n",
      "Input1:> 如果 你 曾经 也 想 过 如果 我 的 大腿 更 瘦 头发 更好 我会 更 开心 吗\n",
      "\n",
      "Target1:= And if you ever are wondering , &quot; If I have thinner thighs and UNK hair , will I be happier  ? &quot; \n",
      "Predict1:< If if you to to to , , , , , , , , , , to\n",
      "\n",
      "Input2:> 而 这 也 是 为什么 什么 它 在 街上 的 原因\n",
      "\n",
      "Target2:= That is why it is on the street  . \n",
      "Predict2:< And it is is is it is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 22s (- -43m 38s), Epoch: [3/10], Step: [2600/3335], Train Loss: 3.440662348270416, BLEU: 0.49009397454375386\n",
      "\n",
      "Input1:> 然后 我 想 酷 就 这样 吧\n",
      "\n",
      "Target1:= And I was like , &quot; Cool  . Bring it  . &quot; \n",
      "Predict1:< And I I to to to to  .  .\n",
      "\n",
      "Input2:> 但 我们 还是 会 得到 大量 来自 不同 环境 下 的 采样 数据 而且 重点 是 不同 的 环境 因为 这样 我们 才能 消除 除掉 那些 混淆 的 因素 寻求 这种 疾病 真正 的 标记\n",
      "\n",
      "Target2:= But we ll get a very large sample of data that is collected from all different circumstances , and it is getting it in different circumstances that matter because then we are looking at ironing out the confounding \n",
      "Predict2:< But we we we to to that , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 28s (- -43m 32s), Epoch: [3/10], Step: [2620/3335], Train Loss: 3.6341787695884706, BLEU: 0.44631183341957054\n",
      "\n",
      "Input1:> 所以 我们 带 着意 意大利 大利 的 种子 来到 了 赞比亚 比亚 南部 一个 壮丽 迷人 的 山谷 通向 UNK 比西 西河 我们 教 当地 当地人 人种 种植 意大利 大利 利西 西红柿 和 UNK 葫芦 等等\n",
      "\n",
      "Target1:= So we arrived there with Italian seeds in southern Zambia in this absolutely magnificent valley going down to the UNK River , and we taught the local people how to grow Italian tomatoes and UNK and  . . . \n",
      "Predict1:< So we we we , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 一家 一家人 家人 的 肖像\n",
      "\n",
      "Target2:= This is a family portrait  . \n",
      "Predict2:< This is a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 33s (- -43m 27s), Epoch: [3/10], Step: [2640/3335], Train Loss: 3.3616459131240846, BLEU: 0.47893475808556213\n",
      "\n",
      "Input1:> UNK 我 知道 电力 栅栏 已经 有 了 但是 我 只 想 做 我 自己 的 那 一个\n",
      "\n",
      "Target1:= But I know electric fences are already invented , but I want to make mine  . \n",
      "Predict1:< I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 即使 即使如此 如此 他 也 梦想 想着 能够 够重 重获 自由 在 Free the UNK 这样 的 组织 帮助 下 接受 受教 教育 就是 这样 的 信念 面对 令人 难以 难以想象 想象 的 苦难 让 我 充满 敬畏\n",
      "\n",
      "Target2:= Even still , he has a dream that he will become free and become educated with the help of local activists like Free the Slaves , and it is this sort of determination , in the face of \n",
      "Predict2:< And he , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 38s (- -43m 21s), Epoch: [3/10], Step: [2660/3335], Train Loss: 3.4677491664886473, BLEU: 0.444020622990738\n",
      "\n",
      "Input1:> 请 不要 要说 我 正常\n",
      "\n",
      "Target1:= Please -- don t tell me I am normal  . \n",
      "Predict1:< I I I I I  .  .\n",
      "\n",
      "Input2:> 他们 会 把 尸体 上 的 肉 吃 的 干干净净 干净 净 只 剩 UNK\n",
      "\n",
      "Target2:= They clean up carcasses right to the bone  . \n",
      "Predict2:< They they to the the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 44s (- -43m 16s), Epoch: [3/10], Step: [2680/3335], Train Loss: 3.4568105220794676, BLEU: 0.5161363807588331\n",
      "\n",
      "Input1:> 我们 必须 学会 如何 让 这些 人 给出 他们 的 意见\n",
      "\n",
      "Target1:= You have to learn how to get these people to come and talk to you  . \n",
      "Predict1:< We need need to to to to to to to to\n",
      "\n",
      "Input2:> 我们 都 知道 自己 正 冒 着 生命 的 危险 老师 学生 以及 家长 都 很 明白\n",
      "\n",
      "Target2:= We all knew we were risking our lives -- the teacher , the students and our parents  . \n",
      "Predict2:< We know know that , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 50s (- -43m 10s), Epoch: [3/10], Step: [2700/3335], Train Loss: 3.543716299533844, BLEU: 0.48162501966101984\n",
      "\n",
      "Input1:> 音乐 而且 因为 我们 有 一个 可以 运行 如此 之 快 的 相机 也 就是 我们 的 飞秒 相机 这个 相机 就 有 一些 特别 别的 能力\n",
      "\n",
      "Target1:= And because we have a camera that can run so fast , our UNK , it has some unique abilities  . \n",
      "Predict1:< And is is , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 任何 健康 或者 患有 帕金森 疾病 的 人 都 能 打进 进来 很 便宜 留下 录音 每次 只要 几分 分钱 我 很 高兴 宣布 我们 仅 在 8 小时 内 就 达到 了 我们 目标 标的 6\n",
      "\n",
      "Target2:= Anyone healthy or with Parkinson is can call in , cheaply , and leave recordings , a few cents each , and I am really happy to announce that we ve already hit six percent of our target \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 42m 55s (- -43m 5s), Epoch: [3/10], Step: [2720/3335], Train Loss: 3.455614757537842, BLEU: 0.48875630000338005\n",
      "\n",
      "Input1:> 他们 也 都 具有 精确 精确性 虽然 这么 说 但是 基于 语音 的 测试 却 不是 专家 UNK 级别 别的\n",
      "\n",
      "Target1:= And they are both accurate  . Okay , but in addition , UNK tests are UNK  . \n",
      "Predict1:< They they t , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他们 把 这些 照片 等 东西 收集 起来 把 它们 送往 各个 镇里 进行 保管\n",
      "\n",
      "Target2:= They were collecting them up , and handing them in to various places around the different towns for UNK  . \n",
      "Predict2:< They they they to the , , , , , they , they  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 1s (- -44m 59s), Epoch: [3/10], Step: [2740/3335], Train Loss: 3.564588725566864, BLEU: 0.4108906751149942\n",
      "\n",
      "Input1:> 发现 家庭 家庭暴力 暴力 的 预兆 并 认真 真的 干预 减少 发生 的 可能 可能性 给 受害 受害者 提供 安全 的 出路\n",
      "\n",
      "Target1:= UNK the early signs of violence and UNK intervene , UNK it , show victims a safe way out  . \n",
      "Predict1:< And is is the of , , , , , , , , , the\n",
      "\n",
      "Input2:> 世界 上 所有 有成 成功 的 企业 只有 一个 共同 共同点 同点 只有 一个 没有 一个 企业 是 靠 一 人 之 力 创建 的\n",
      "\n",
      "Target2:= There is only one thing that all the successful companies in the world have in common , only one : None were started by one person  . \n",
      "Predict2:< It is a of of , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 6s (- -44m 54s), Epoch: [3/10], Step: [2760/3335], Train Loss: 3.4754311203956605, BLEU: 0.49083434081891164\n",
      "\n",
      "Input1:> 今天 我 只要 要求 一件 事\n",
      "\n",
      "Target1:= Today I have just one request  . \n",
      "Predict1:< I I I I I  .  .  .\n",
      "\n",
      "Input2:> 我 希望 这些 影像 能够 在 你们 这样 的 观者 心中 唤起 一股 力量 我 希望 这股 力量 的 星星 星星之火 能够 成为 被 奴役 者 的 希望 之 光 因为 若 没有 这 光亮 奴役 之 兽\n",
      "\n",
      "Target2:= I hope that these images awaken a force in those who view them , people like you , and I hope that force will ignite a fire , and that fire will shine a light on slavery , \n",
      "Predict2:< I I I to you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 12s (- -44m 48s), Epoch: [3/10], Step: [2780/3335], Train Loss: 3.561812901496887, BLEU: 0.50202242079058\n",
      "\n",
      "Input1:> 我们 说 能到 到处 旅行 和 有创造力 创造 创造力 的 热情 的 灵感 不断 的 人 一起 起工 工作 真的 很棒\n",
      "\n",
      "Target1:= We say , &quot; It is really amazing to travel , and it is amazing to get to work with creative , inspired , passionate people  . &quot; \n",
      "Predict1:< We are to to the , , , , , , , and , and and and\n",
      "\n",
      "Input2:> 我 发现 美国 国人 能 看出 我们 变化 中 的 脆弱 脆弱性\n",
      "\n",
      "Target2:= I find that Americans see the fragility in changes  . \n",
      "Predict2:< I I I to the the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 17s (- -44m 43s), Epoch: [3/10], Step: [2800/3335], Train Loss: 3.500483453273773, BLEU: 0.5109487823898675\n",
      "\n",
      "Input1:> 因此 我 觉得 太好了 这 是 我 的 责任 我 必须 维护 它 所以 我 想 做 什么 就 可以 做 什么 了\n",
      "\n",
      "Target1:= So I am like , &quot; Cool  . I can do whatever the hell I want , since it is my responsibility and I gotta maintain it  . &quot; \n",
      "Predict1:< So I I I I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 他 刚 起步 时 并非 单枪 单枪匹马\n",
      "\n",
      "Target2:= He wasn t alone when he started  . \n",
      "Predict2:< He he he the the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 23s (- -44m 37s), Epoch: [3/10], Step: [2820/3335], Train Loss: 3.423713004589081, BLEU: 0.4952289327815584\n",
      "\n",
      "Input1:> 我 想 这 就是 内罗毕 公园 的 狮子 UNK 子会 那么 少 的 原因\n",
      "\n",
      "Target1:= And I think this is why the Nairobi National Park lions are few  . \n",
      "Predict1:< I I think to to that the the the the the the  .  .\n",
      "\n",
      "Input2:> 因为 国会 国会议员 会议 会议员 议员 们 不能 保持 自然 自然环境 环境 的 清洁\n",
      "\n",
      "Target2:= Because MPs do not keep the environment clean  . \n",
      "Predict2:< Because is is , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 28s (- -44m 32s), Epoch: [3/10], Step: [2840/3335], Train Loss: 3.4233432054519652, BLEU: 0.5027116046609048\n",
      "\n",
      "Input1:> 我 第一 第一次 一次 有 了 自己 的 房子 公寓 有 了 第一 第一张 一张 信用 信用卡 我 还有 一个 非常 大 的 秘密\n",
      "\n",
      "Target1:= I had my first apartment , my first little green American Express card , and I had a very big secret  . \n",
      "Predict1:< I was a a , , , , , , , , , , , , , a a  .\n",
      "\n",
      "Input2:> 当 我 在 拍摄 的 途中 我 带 了 很多 蜡烛 在 翻译 的 帮助 下 我 把 蜡烛 分给 了 我 的 拍摄 对象 我 希望 能 点亮 他们 的 故事 和 他们 的 困境 当 我们\n",
      "\n",
      "Target2:= When I was working in the field , I brought lots of candles with me , and with the help of my interpreter , I imparted to the people I was photographing that I wanted to illuminate their \n",
      "Predict2:< When I I my my my my , my , , my , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 34s (- -44m 26s), Epoch: [3/10], Step: [2860/3335], Train Loss: 3.5579973220825196, BLEU: 0.49629377829539684\n",
      "\n",
      "Input1:> 如果 你 在 这种 路上 上行 行驶 你 会 带出 这些 集料 就是 我们 看到 的\n",
      "\n",
      "Target1:= Then if you drive over the road , you take out the aggregates -- what we just saw here  . \n",
      "Predict1:< If you you you you you , , , , , you , , you\n",
      "\n",
      "Input2:> 掌声 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 39s (- -44m 21s), Epoch: [3/10], Step: [2880/3335], Train Loss: 3.564257526397705, BLEU: 0.4816761570000953\n",
      "\n",
      "Input1:> 同时 我 在 考虑 如果 没有 途径 去 得到 健康 的 食物 你 的 感受 是 什么 如果 每次 你 从 家里 出来 看到 在 你 的 社区 现有 的 食物 系统 的 不良 不良影响 影响 你 的\n",
      "\n",
      "Target1:= And I was wondering , how would you feel if you had no access to healthy food , if every time you walk out your door you see the ill effects that the present food system has on \n",
      "Predict1:< And I I you you you you you you you you , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 向 所有 所有人 有人 求助 警察 邻居 我 的 朋友 和 家人 完全 陌生 的 人 今天 我 能 站 在 这里 因为 你们 每个 个人 都 帮助 了 我\n",
      "\n",
      "Target2:= I told everyone : the police , my neighbors , my friends and family , total strangers , and I am here today because you all helped me  . \n",
      "Predict2:< I I I I , I , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 45s (- -44m 15s), Epoch: [3/10], Step: [2900/3335], Train Loss: 3.5814663052558897, BLEU: 0.45361509980282255\n",
      "\n",
      "Input1:> 她们 的 出现 使 我 感觉 非常 糟糕 我 告诉 他们 你们 不用 像 这样 来 拿 食物\n",
      "\n",
      "Target1:= So I am like , man , it made me feel bad that they were there , and I told them , you know , you don t have to do this like this  . \n",
      "Predict1:< And they , , , , , , , , , , , , , , I , I , I\n",
      "\n",
      "Input2:> 这样 的 能人 尚未 未出 出生\n",
      "\n",
      "Target2:= This person has never been born  . \n",
      "Predict2:< So is the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 50s (- -44m 10s), Epoch: [3/10], Step: [2920/3335], Train Loss: 3.4709955930709837, BLEU: 0.49890351931642546\n",
      "\n",
      "Input1:> 我 获得 了 乘飞机 飞机 来 TED 的 机会 这 是 我 的 第一 第一次 一次\n",
      "\n",
      "Target1:= I got a chance to come by plane for my first time for TED  . \n",
      "Predict1:< I was I , I , , , I , I  .  .  .\n",
      "\n",
      "Input2:> 我 让 这 五个 渔民 合伙 运作 不是 把 美味 的 金枪 金枪鱼 以 60 美分 一 公斤 的 低价 卖 给 奥尔巴 奥尔巴尼 尔巴 尼 美国 的 罐头 工厂 而是 以 15 美元 一 公斤 的 高价\n",
      "\n",
      "Target2:= And I helped these five fishermen to work together and get this beautiful tuna not to the cannery in Albany for 60 cents a kilo , but we found a way to take the fish for sushi to \n",
      "Predict2:< I I to to to the the , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43m 56s (- -44m 4s), Epoch: [3/10], Step: [2940/3335], Train Loss: 3.5198282599449158, BLEU: 0.496368729400999\n",
      "\n",
      "Input1:> 我们 在世 在世界上 世界 上 300 个 社区 开展 了 这个 项目\n",
      "\n",
      "Target1:= We ve done it in 300 communities around the world  . \n",
      "Predict1:< We we we the the the the the  .  .  .\n",
      "\n",
      "Input2:> 我们 用 的 就是 清洁 厨具 用 的 钢丝 丝绒 钢丝 丝绒 可以 UNK 很小 小块 我们 把 这些 些小 小块 混进 沥青\n",
      "\n",
      "Target2:= So what we can do is use steel wool just to clean pans , and the steel wool we can cut in very small pieces , and these very small pieces we can mix to the bitumen  . \n",
      "Predict2:< We we we the the the , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 1s (- -45m 59s), Epoch: [3/10], Step: [2960/3335], Train Loss: 3.4814570546150208, BLEU: 0.44235058341206707\n",
      "\n",
      "Input1:> 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 而且 和 芭蕾 一样 这 也 需要 经过 特殊 地 训练\n",
      "\n",
      "Target2:= And like ballet , it takes an extraordinary level of training  . \n",
      "Predict2:< And it is it , , and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 7s (- -45m 53s), Epoch: [3/10], Step: [2980/3335], Train Loss: 3.430968630313873, BLEU: 0.4934635221658077\n",
      "\n",
      "Input1:> 政府 免费 授权 了 一段 高速 高速公路 公路 给 我们 在 UNK 号 上 400 米 的 距离 我们 就 在 那里 做 了 材料 测试\n",
      "\n",
      "Target1:= So they donated to us a piece of highway , 400 meters of the UNK , where we had to make a test track to test this material  . \n",
      "Predict1:< And is the , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 蜜月 期间 我 又 被 打 了 两次\n",
      "\n",
      "Target2:= It happened twice more on the honeymoon  . \n",
      "Predict2:< And : : I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 12s (- -45m 48s), Epoch: [3/10], Step: [3000/3335], Train Loss: 3.458703172206879, BLEU: 0.5065618249996267\n",
      "\n",
      "Input1:> 中途 不能 休息 不能 吃饭 不能 喝水 由于 极度 的 缺水 他们 基本 基本上 也 不 需要 上 厕所\n",
      "\n",
      "Target1:= There were no breaks for food , no water breaks , and the severe dehydration made urinating pretty much inconsequential  . \n",
      "Predict1:< And they , , , , , , , , , , , , they ,\n",
      "\n",
      "Input2:> 现在 如果 我 用 一个 激光 光笔 并且 在 万亿 亿分 之一 一秒 内 开关 一次 也 就是 几 飞秒 我 就 会 制造 一些 光子 这些 光子 仅仅 是 几毫米 毫米 宽 并且 这些 光子 像 子弹\n",
      "\n",
      "Target2:= Now if I take a laser pointer and turn it on and off in one UNK of a second -- which is several femtoseconds -- I ll create a packet of photons barely a millimeter wide , and \n",
      "Predict2:< Now , I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 18s (- -45m 42s), Epoch: [3/10], Step: [3020/3335], Train Loss: 3.4023464560508727, BLEU: 0.4884841301836274\n",
      "\n",
      "Input1:> 见到 他 是 在 一个 收容 收容所 所里 Free the UNK 组织 用于 复元 奴役 受害 受害者 的 一个 地方\n",
      "\n",
      "Target1:= I met him at a shelter where Free the Slaves UNK victims of slavery  . \n",
      "Predict1:< And is a a a , , , , , , , a a a a a  .\n",
      "\n",
      "Input2:> 十几 十几岁 几岁 时 我 跟 朋友 一起 起开 开车 她 是 个 糟糕 的 司机 闯 了 红灯 我们 被 停车 我 只 说 了 句 长官 抱歉 我们 就 继续 上路 了\n",
      "\n",
      "Target2:= When I was a teenager , I was driving with my friend who was an awful driver and she ran a red and of course , we got pulled over , and all it took was a &quot; \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 23s (- -45m 37s), Epoch: [3/10], Step: [3040/3335], Train Loss: 3.5405584812164306, BLEU: 0.5036519958075555\n",
      "\n",
      "Input1:> 我 在 这个 舞台 台上 是因为 因为 我 是 个 漂亮 的 白种 女性 在 我们 业内 我们 称 此为 性感 性感女 女孩\n",
      "\n",
      "Target1:= I am on this stage because I am a pretty , white woman , and in my industry we call that a sexy girl  . \n",
      "Predict1:< I I I I I , I , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 思考 死亡 能够 让 你 对 自己 的 人生 有 更 清醒 的 认识\n",
      "\n",
      "Target2:= Thinking about death clarifies your life  . \n",
      "Predict2:< And can you to to to to to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 29s (- -45m 31s), Epoch: [3/10], Step: [3060/3335], Train Loss: 3.506119465827942, BLEU: 0.4639007658667103\n",
      "\n",
      "Input1:> 微波 微波炉 跟 感应 UNK 电机 的 原理 类似\n",
      "\n",
      "Target1:= So a microwave is a similar system  . \n",
      "Predict1:< The : : , , the  .  .  .\n",
      "\n",
      "Input2:> 议员 甚至 前来 拜访 表达 了 他 有 多么 赞赏 和 热爱 我们 所 做 的\n",
      "\n",
      "Target2:= My UNK even called in and said how they endorse and love what we are doing  . \n",
      "Predict2:< And is , , , , , , , , and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 34s (- -45m 26s), Epoch: [3/10], Step: [3080/3335], Train Loss: 3.4931566596031187, BLEU: 0.5077635599986595\n",
      "\n",
      "Input1:> 我 想要 让 他们 知道 我们 将 为 他们 作证 尽 我们 所能 让 他们 的 人生 有所 改变\n",
      "\n",
      "Target1:= I wanted them to know that we will be bearing witness to them , and that we will do whatever we can to help make a difference in their lives  . \n",
      "Predict1:< I I t to to to to , , , , , , we we to them  .  .\n",
      "\n",
      "Input2:> 在 一个 10 000 人 的 社区 区里 我们 有 200 个 客户\n",
      "\n",
      "Target2:= In a community of 10,000 people , we get 200 clients  . \n",
      "Predict2:< In in a a a , , , a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 40s (- -45m 20s), Epoch: [3/10], Step: [3100/3335], Train Loss: 3.4969860076904298, BLEU: 0.4698025876110535\n",
      "\n",
      "Input1:> 早先 我 在 战争 中长 长大 周末 在 朋友 朋友家 UNK 过夜 周六 看 足球 足球比赛 比赛 跟 种族 种族歧视 歧视 的 孩子 打架 以及 参加 宗教 UNK 民族 民族主义 主义 游行 在 人群 中大 大喊 打倒 共产\n",
      "\n",
      "Target1:= Long before that I had grown up with the war , but alongside weekend sleepovers and Saturday soccer games and UNK with racist children of the UNK and UNK demonstrations chanting , &quot; Down with communism and long \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 然后 他们 放 我 走 了 真是 个 奇迹\n",
      "\n",
      "Target2:= And they let me go  . It was a miracle  . \n",
      "Predict2:< And then I I , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 45s (- -45m 14s), Epoch: [3/10], Step: [3120/3335], Train Loss: 3.563095211982727, BLEU: 0.47892540152669544\n",
      "\n",
      "Input1:> 你 的 下 一步 计划 是 什么\n",
      "\n",
      "Target1:= What is the next one on your list  ? \n",
      "Predict1:< What you you  ?  ?  ?\n",
      "\n",
      "Input2:> 每当 你 看到 秃鹫 就 像 这 只 首先 浮现 现在 你 脑海 脑海中 海中 一定 是 这 是 一些 令人 令人厌恶 厌恶 贪得 贪得无厌 并且 追逐 腐肉 的 可怕 怕生 生物 让 人 禁不住 不住 联想 想到\n",
      "\n",
      "Target2:= When you see a vulture like this , the first thing that comes to your mind is , these are disgusting , ugly , greedy creatures that are just after your flesh , associated with politicians  . \n",
      "Predict2:< And you you you , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 51s (- -45m 9s), Epoch: [3/10], Step: [3140/3335], Train Loss: 3.3540547728538512, BLEU: 0.496985860174911\n",
      "\n",
      "Input1:> 我们 需要 客观 地 测量 进展 最后 想要 知道 何时 有治 治愈 之 方 的 唯一 一方 方法 就是 当 我们 拥有 了 能 准确 回答 那些 问题 的 客观 观测 测量 UNK\n",
      "\n",
      "Target1:= We need to be able to measure progression objectively , and ultimately , the only way we are going to know when we actually have a cure is when we have an objective measure that can answer that \n",
      "Predict1:< We we to to to to to , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 一些 人 甚至 都 不 认为 他们 在 被 人 奴役 每天 天工 工作 十六 UNK 六七 六七个 七个 小时 得不到 不到 半点 酬劳 因为 他们 生来 就 过 着 这样 的 日子\n",
      "\n",
      "Target2:= Some don t even know they are enslaved , people working 16 , 17 hours a day without any pay , because this has been the case all their lives  . \n",
      "Predict2:< And they people people people people they , they , they they they they they they they they they they , they they they they they they , they they\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 44m 56s (- -45m 3s), Epoch: [3/10], Step: [3160/3335], Train Loss: 3.481860613822937, BLEU: 0.4547599654173014\n",
      "\n",
      "Input1:> 你 想 过为 为什么 什么 点 句式 符号 号叫 子弹 点儿 吗\n",
      "\n",
      "Target1:= Have you ever wondered why they are called bullet points  ? \n",
      "Predict1:< You you to to to  ?  ?  ?  ?  ?  ?  ?\n",
      "\n",
      "Input2:> 他 不 愿意 像 其他 婴儿 那样 玩儿 实际 实际上 他 好像 对 我 一点 一点儿 点儿 也 不感兴趣 感兴 感兴趣 兴趣\n",
      "\n",
      "Target2:= He didn t want to play like the other babies did , and in fact , he didn t seem very interested in me whatsoever  . \n",
      "Predict2:< He he t to , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 2s (- -46m 58s), Epoch: [3/10], Step: [3180/3335], Train Loss: 3.5893288135528563, BLEU: 0.4950060450593076\n",
      "\n",
      "Input1:> 然而 我们 干什么 什么 UNK 什么\n",
      "\n",
      "Target1:= Instead , everything we touched we killed  . \n",
      "Predict1:< But we we we we  ?  ?\n",
      "\n",
      "Input2:> 当然 当地 当地人 对此 表示 毫无 无兴趣 兴趣 所以 我们 付 工资 请 他们 来 劳作 他们 也 是 一会 一会儿 会儿 来 一会 一会儿 会儿 不来 的\n",
      "\n",
      "Target2:= And of course the local people had absolutely no interest in doing that , so we paid them to come and work , and sometimes they would show up  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 7s (- -46m 52s), Epoch: [3/10], Step: [3200/3335], Train Loss: 3.471208119392395, BLEU: 0.42382649736075734\n",
      "\n",
      "Input1:> 政客 们 小心 小心翼翼 翼翼 地 选择 用词 并 利用 用词 词汇 塑造 和 控制 现实 但是 事实 事实上 现实 改变 词汇 远远 远超过 超过 词汇 能 改变 的 现实\n",
      "\n",
      "Target1:= Politicians try to pick words and use words to shape reality and control reality , but in fact , reality changes words far more than words can ever change reality  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 最近 纽约 纽约时报 时报 报报 报道 美国 每年 有 10 万 到 30 万 小孩 被 买卖 UNK 成为 性 奴\n",
      "\n",
      "Target2:= Recently , the New York Times reported that between 100,000 and 300,000 American children are sold into sex slavery every year  . \n",
      "Predict2:< In , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 13s (- -46m 47s), Epoch: [3/10], Step: [3220/3335], Train Loss: 3.480552315711975, BLEU: 0.48845837307705886\n",
      "\n",
      "Input1:> 你 可以 猜到 事情 是 怎么 发展 的\n",
      "\n",
      "Target1:= Well you can see where this is going  . \n",
      "Predict1:< You can can you you  .\n",
      "\n",
      "Input2:> 英语 在 韩国 太重 重要 了 所以 我 必须 须要 开始 学习 第三 UNK 语言 英语\n",
      "\n",
      "Target2:= English was so important in South Korea , so I had to start learning my third language  . \n",
      "Predict2:< And , , , I I , , , , , I to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 19s (- -46m 41s), Epoch: [3/10], Step: [3240/3335], Train Loss: 3.4185057520866393, BLEU: 0.4565637473395435\n",
      "\n",
      "Input1:> 就是 这样\n",
      "\n",
      "Target1:= Absolutely  . \n",
      "Predict1:< That is is  .  .\n",
      "\n",
      "Input2:> 一年 之内 我 做 了 27 个 项目 政府 的 人 跑 来 问 我 你 是 怎么 怎么办 办到 的\n",
      "\n",
      "Target2:= In a year , I had 27 projects going on , and the government came to see me to say , &quot; How can you do that  ? \n",
      "Predict2:< So , I , I , , , , , , , , , , , I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 24s (- -46m 36s), Epoch: [3/10], Step: [3260/3335], Train Loss: 3.417088270187378, BLEU: 0.45234907710077454\n",
      "\n",
      "Input1:> 我 生活 在 洛杉矶 UNK\n",
      "\n",
      "Target1:= I live in South Central  . \n",
      "Predict1:< I I I to  .  .\n",
      "\n",
      "Input2:> 种 一棵 一棵树 能够 收到 一千 甚至 一万 万颗 种子\n",
      "\n",
      "Target2:= Growing one plant will give you 1,000 , 10,000 seeds  . \n",
      "Predict2:< And is the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 30s (- -46m 30s), Epoch: [3/10], Step: [3280/3335], Train Loss: 3.3538664817810058, BLEU: 0.48137517301764693\n",
      "\n",
      "Input1:> 和 UNK 还有 UNK 一起 合作 我们 希望 记录 录下 大量 世界 世界各地 各地 的 声音 收集 足够 的 数据 来 达到 这 四个 目标\n",
      "\n",
      "Target1:= With UNK and UNK , we are aiming to record a very large number of voices worldwide to collect enough data to start to tackle these four goals  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , the the the\n",
      "\n",
      "Input2:> 五天 之后 脖子 上 的 十个 手指 指印 刚 消退 我 就 穿 上 我 妈妈 的 婚纱 嫁给 了 他\n",
      "\n",
      "Target2:= Five days later , the ten bruises on my neck had just faded , and I put on my mother is wedding dress , and I married him  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 35s (- -46m 25s), Epoch: [3/10], Step: [3300/3335], Train Loss: 3.351516234874725, BLEU: 0.4399410087958112\n",
      "\n",
      "Input1:> 我 的 意思 是 当然 啦 他们 怎么 能 不 这么 做 呢\n",
      "\n",
      "Target1:= I mean , come on , why wouldn t they  ? \n",
      "Predict1:< I mean , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 锤 声 微波 微波炉 声 你们 也许 会同 同意 这条 路 很 不错\n",
      "\n",
      "Target2:= You probably all agree with me that this is a very nice road  . \n",
      "Predict2:< You is a , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 41s (- -46m 19s), Epoch: [3/10], Step: [3320/3335], Train Loss: 3.4971360683441164, BLEU: 0.480878183265633\n",
      "\n",
      "Input1:> 所以 很多 很多年 多年 年前 我 有 个 想法 我们 为何 何不 不能 至少 少有 一次 做到 聆听 他们 的 想法 而 不是 把 我们 的 想法 强加 强加于 加于 他们 但 这 不能 以 社区 会议 的\n",
      "\n",
      "Target1:= So years and years ago , I had this idea : Why don t we , for once , instead of arriving in the community to tell people what to do , why don t , for once \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 那些 时刻 令人 触动 也 非常 常有 激励 意义 我 一直 听 人们 说 要 有 创意 直到 那时 清理 那些 照片 我 才 真的 做到 了 所谓 的 创新\n",
      "\n",
      "Target2:= Now , it was emotional and it was inspiring , and I ve always heard about thinking outside the box , but it wasn t until I had actually gotten outside of my box that something happened  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "[3.432033860683441, 3.348463034629822, 3.3758938670158387, 3.304491400718689, 3.3428423404693604, 3.411056566238403, 3.450154161453247, 3.491550028324127, 3.474711549282074, 3.5588143348693846, 3.4791202425956724, 3.515571653842926, 3.5176634907722475, 3.515778589248657, 3.5024274349212647, 3.4717066287994385, 3.5398224711418154, 3.60284823179245, 3.346724581718445, 3.343217670917511, 3.4043576955795287, 3.634182059764862, 3.5506060481071473, 3.425030994415283, 3.4155769467353823, 3.4313263416290285, 3.5071033358573915, 3.4279942512512207, 3.2689972877502442, 3.5191334962844847, 3.602749025821686, 3.5743011951446535, 3.5083127856254577, 3.5306931376457213, 3.586691904067993, 3.456716072559357, 3.543354880809784, 3.526156151294708, 3.4621673464775085, 3.4048696517944337, 3.449732446670532, 3.46313636302948, 3.4431886434555055, 3.5005523800849914, 3.543252694606781, 3.3723565340042114, 3.4098299741744995, 3.4004079818725588, 3.3647728323936463, 3.473980975151062, 3.4252326130867004, 3.506591558456421, 3.5069096803665163, 3.352100110054016, 3.421431863307953, 3.5794066786766052, 3.4529467940330507, 3.490801417827606, 3.477327656745911, 3.5241736650466917, 3.444532608985901, 3.3564573407173155, 3.456550693511963, 3.40307195186615, 3.5466930150985716, 3.419064521789551, 3.5078872203826905, 3.54655065536499, 3.468257796764374, 3.4527866005897523, 3.386165404319763, 3.455624055862427, 3.3951960802078247, 3.3031417965888976, 3.480949032306671, 3.432154667377472, 3.6562718629837034, 3.447623777389526, 3.6008131742477416, 3.483093523979187, 3.5019808292388914, 3.3515435695648192, 3.41634886264801, 3.4531784296035766, 3.3925369024276733, 3.5111898303031923, 3.345906901359558, 3.5192678689956667, 3.521306312084198, 3.361321759223938, 3.488546943664551, 3.5447662472724915, 3.3898482084274293, 3.5354219913482665, 3.4646899700164795, 3.5203861474990843, 3.4856660962104797, 3.4613327860832213, 3.3655490636825562, 3.401737725734711, 3.436975920200348, 3.5407268166542054, 3.503669011592865, 3.4526479601860047, 3.55556116104126, 3.476121187210083, 3.493216836452484, 3.363050436973572, 3.471653413772583, 3.476394999027252, 3.39001225233078, 3.4381833910942077, 3.5358700156211853, 3.4660480260849, 3.3955977320671082, 3.418517363071442, 3.4356716156005858, 3.4261411905288695, 3.523692560195923, 3.5080864667892455, 3.4858195900917055, 3.495653784275055, 3.429253542423248, 3.590279281139374, 3.4211182475090025, 3.5486456155776978, 3.5040220499038695, 3.3527569651603697, 3.526845729351044, 3.440662348270416, 3.6341787695884706, 3.3616459131240846, 3.4677491664886473, 3.4568105220794676, 3.543716299533844, 3.455614757537842, 3.564588725566864, 3.4754311203956605, 3.561812901496887, 3.500483453273773, 3.423713004589081, 3.4233432054519652, 3.5579973220825196, 3.564257526397705, 3.5814663052558897, 3.4709955930709837, 3.5198282599449158, 3.4814570546150208, 3.430968630313873, 3.458703172206879, 3.4023464560508727, 3.5405584812164306, 3.506119465827942, 3.4931566596031187, 3.4969860076904298, 3.563095211982727, 3.3540547728538512, 3.481860613822937, 3.5893288135528563, 3.471208119392395, 3.480552315711975, 3.4185057520866393, 3.417088270187378, 3.3538664817810058, 3.351516234874725, 3.4971360683441164]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 48s (- -44m 28s), Epoch: [4/10], Step: [20/3335], Train Loss: 3.5677786588668825, BLEU: 0.4868759843412604\n",
      "\n",
      "Input1:> 我 所 做 的 一切 不论 不论是 生活 上 还是 工作 上 我 的 整个 个人 人生 深深 深受 受到 年轻 时 在 非洲 工作 的 那 七年 的 影响\n",
      "\n",
      "Target1:= Everything I do , and everything I do professionally -- my life -- has been shaped by seven years of work as a young man in Africa  . \n",
      "Predict1:< And I I to the the the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 同时 我 在 考虑 如果 没有 途径 去 得到 健康 的 食物 你 的 感受 是 什么 如果 每次 你 从 家里 出来 看到 在 你 的 社区 现有 的 食物 系统 的 不良 不良影响 影响 你 的\n",
      "\n",
      "Target2:= And I was wondering , how would you feel if you had no access to healthy food , if every time you walk out your door you see the ill effects that the present food system has on \n",
      "Predict2:< And I I you you you you you you you , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 54s (- -45m 14s), Epoch: [4/10], Step: [40/3335], Train Loss: 3.4476536870002747, BLEU: 0.48736907424996173\n",
      "\n",
      "Input1:> 我 生活 在 洛杉矶 UNK\n",
      "\n",
      "Target1:= I live in South Central  . \n",
      "Predict1:< I I I in  .  .  .\n",
      "\n",
      "Input2:> 你 知道 的 这样 大概 会 可行\n",
      "\n",
      "Target2:= And , you know , it could have worked  . \n",
      "Predict2:< You you you you you ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45m 59s (- -46m 46s), Epoch: [4/10], Step: [60/3335], Train Loss: 3.416226696968079, BLEU: 0.5115457450658316\n",
      "\n",
      "Input1:> 每 一个 病例 的 情况 都 是 特殊 的 所以 UNK 与 Sam 的 情况 也 是 不同 的\n",
      "\n",
      "Target1:= It manifests in each individual differently , hence why Remi is so different from Sam  . \n",
      "Predict1:< The the is of of of of of , , , the of of of  .\n",
      "\n",
      "Input2:> 确定 的 是 除了 手术 或 像 两天 前 我 为了 工作 做 的 棕 褐色 我们 基本 基本上 是 没法 改变 外表 的 我们 的 外表 虽然 是 表面 UNK 的 固定 的 还是 极大 大地 影响\n",
      "\n",
      "Target2:= And of course , barring surgery , or the fake tan that I got two days ago for work , there is very little that we can do to transform how we look , and how we look \n",
      "Predict2:< The the is the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 5s (- -46m 29s), Epoch: [4/10], Step: [80/3335], Train Loss: 3.4205126404762267, BLEU: 0.5010000999873303\n",
      "\n",
      "Input1:> 但 不幸 的 是 我 得 告诉 你们 在 2007 年时 一名 纽约 纽约大学 大学 的 博士 在 启发 之下 统计 了 所有 在职 模特 的 人数 在 UNK 名 正式 受雇 模特 中 只有 27 名 也\n",
      "\n",
      "Target1:= But unfortunately I have to inform you that in 2007 , a very inspired NYU Ph .D . student counted all the models on the runway , every single one that was hired , and of the UNK models that \n",
      "Predict1:< But I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 看到 了 吧 我 是 个 艺术 艺术家 术家\n",
      "\n",
      "Target2:= See , I am an artist  . \n",
      "Predict2:< I : : I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 10s (- -46m 16s), Epoch: [4/10], Step: [100/3335], Train Loss: 3.4793741703033447, BLEU: 0.4979028703011392\n",
      "\n",
      "Input1:> 现在 含有 双 氯 芬 酸 的 UNK 已经 被禁 禁止 在 印度 使用 印度 印度政府 政府 表现 现出 了 积极 的 态度\n",
      "\n",
      "Target1:= This drug has now been banned for veterinary use in India , and they have taken a stand  . \n",
      "Predict1:< Now , , , , , , , , , , , , , , the the the\n",
      "\n",
      "Input2:> 我 搬到 了 纽约 开始 了 自己 第一 第一份 一份 工作 在 UNK 杂志 杂志社 当 编辑 和 撰稿 撰稿人\n",
      "\n",
      "Target2:= I had moved to New York City for my first job as a writer and editor at Seventeen magazine  . \n",
      "Predict2:< I I , , , and , and and and and and and and and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 16s (- -46m 6s), Epoch: [4/10], Step: [120/3335], Train Loss: 3.5294612526893614, BLEU: 0.46967961642258715\n",
      "\n",
      "Input1:> 你们 自己 去 读读 看\n",
      "\n",
      "Target1:= Just go and read her book  . \n",
      "Predict1:< You you you to  .  .\n",
      "\n",
      "Input2:> 在 阿富汗 你 可以 通过 艺术 艺术作品 作品 创造 展示 很多 多美 美好 的 东西 不过 我 自己 不怎么 怎么 想 描绘 彩虹 我 想要 创作 作出 能够 干扰 身份 挑战 挑战权 权威 的 作品 曝光 那些 UNK\n",
      "\n",
      "Target2:= Now there are a lot of wonderful things that you could make art about in Afghanistan , but personally I don t want to paint rainbows ; I want to make art that disturbs identity and challenges authority \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 21s (- -47m 58s), Epoch: [4/10], Step: [140/3335], Train Loss: 3.636067545413971, BLEU: 0.45064508972541384\n",
      "\n",
      "Input1:> 每当 有人 打喷嚏 喷嚏 时 我 都 感觉 新奥尔良 奥尔良 来 了 一只 游行 队伍\n",
      "\n",
      "Target1:= I feel like every time someone UNK , New Orleans has a parade  . \n",
      "Predict1:< And , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 第一 第一次 一次 我 驾车 去 寻找 秘密 的 海滩 我 迷路 了 他 坐在 副驾驶 驾驶 驾驶座 上 不停 的 打 我 的 头 打得 如此 的 使劲 以至 以至于 至于 我 的 头 不断 的 撞到\n",
      "\n",
      "Target2:= The first time , I was driving to find a secret beach and I got lost , and he punched me in the side of my head so hard that the other side of my head repeatedly hit \n",
      "Predict2:< I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 27s (- -47m 50s), Epoch: [4/10], Step: [160/3335], Train Loss: 3.355557107925415, BLEU: 0.4997960923752528\n",
      "\n",
      "Input1:> 那时 我们 并不知道 不知 知道 这样 的 旅程 会 深深 改变 我们 的 生活\n",
      "\n",
      "Target1:= Back then , we had no idea how much this trip would change our lives  . \n",
      "Predict1:< And we we we we we we we we we we we  .\n",
      "\n",
      "Input2:> 朝鲜 朝鲜人 通往 自由 的 道路 是 如此 遥不可及 不可 及\n",
      "\n",
      "Target2:= North Koreans have to travel incredible distances on the path to freedom  . \n",
      "Predict2:< The : : , , , is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 32s (- -47m 42s), Epoch: [4/10], Step: [180/3335], Train Loss: 3.5787582874298094, BLEU: 0.5142664898605029\n",
      "\n",
      "Input1:> 你们 应该 看看 这些 垃圾 掌声 你们 应该 看看 我们 塞 了 多少 垃圾 给 这些 满怀 信任 的 非洲 非洲人 人民\n",
      "\n",
      "Target1:= You should see the rubbish — -- You should see the rubbish that we have bestowed on unsuspecting African people  . \n",
      "Predict1:< You you you you you , , , , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 这 就 像 在 水池 池里 扔 一块 块石 石头\n",
      "\n",
      "Target2:= It is like throwing a stone in a pond of water  . \n",
      "Predict2:< This is a the , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 38s (- -47m 35s), Epoch: [4/10], Step: [200/3335], Train Loss: 3.505247974395752, BLEU: 0.5350832227721818\n",
      "\n",
      "Input1:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target1:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict1:< We we we to to the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他们 会 把 尸体 上 的 肉 吃 的 干干净净 干净 净 只 剩 UNK\n",
      "\n",
      "Target2:= They clean up carcasses right to the bone  . \n",
      "Predict2:< They they to to the the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 43s (- -47m 28s), Epoch: [4/10], Step: [220/3335], Train Loss: 3.5563385486602783, BLEU: 0.5138610364601418\n",
      "\n",
      "Input1:> 所以 我 必须 找到 一个 解决 解决问题 问题 的 方法\n",
      "\n",
      "Target1:= So I had to find a way of solving this problem  . \n",
      "Predict1:< So I I to to to to to to  .\n",
      "\n",
      "Input2:> 鼓掌 现在 请 记住 这 整个 事情 其实 实是 在 一 纳秒 内 发生 的 也 就是 光 走 的 时间 但 现在 我 再 把 这个 录像 放慢 一百 一百万 一百万倍 百万 百万倍 万倍 让 你 看到\n",
      "\n",
      "Target2:= Now , remember , the whole event is effectively taking place in less than a nanosecond — that is how much time it takes for light to travel — but I am slowing down in this video by \n",
      "Predict2:< Now , is is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 49s (- -47m 22s), Epoch: [4/10], Step: [240/3335], Train Loss: 3.3896560430526734, BLEU: 0.5403934654330138\n",
      "\n",
      "Input1:> 她 两年 年前 退休 了 结果 却 把 我家 变成 了 一所 所学 学校 教育 UNK 社区 中 的 女孩 和 妇女\n",
      "\n",
      "Target1:= She retired two years ago , only to turn our house into a school for girls and women in our neighborhood  . \n",
      "Predict1:< She was the the the , and , and and and and and and and and and and and and\n",
      "\n",
      "Input2:> 我 不 打算 跟 你们 细说 这笔 钱 所 造成 的 损害\n",
      "\n",
      "Target2:= I am not going to tell you the damage that that money has done  . \n",
      "Predict2:< I I t to to you you to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46m 54s (- -47m 15s), Epoch: [4/10], Step: [260/3335], Train Loss: 3.5448543310165403, BLEU: 0.48458656686979135\n",
      "\n",
      "Input1:> 在 我 跟 康 纳 两年 年半 的 婚姻 婚姻生活 生活 中 我 每周 都 会 被 打 一 到 两次\n",
      "\n",
      "Target1:= UNK proceeded to beat me once or twice a week for the next two and a half years of our marriage  . \n",
      "Predict1:< And I I I I I I I , I , I , I I I I\n",
      "\n",
      "Input2:> 所以 当天 傍晚 我 就 登陆 脸谱 网 问 了 一些 修 图 师 朋友 还 没到 第二 第二天 二天 早上 就 有 很多 很多很多 很多 回复 消息 说 他们 乐意 帮忙 我 知道 我们 应该 该行 行动\n",
      "\n",
      "Target2:= So that evening , I just reached out on Facebook and asked a few of them , and by morning the response had been so overwhelming and so positive , I knew we had to give it a \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 0s (- -47m 9s), Epoch: [4/10], Step: [280/3335], Train Loss: 3.5006566882133483, BLEU: 0.4970859417277339\n",
      "\n",
      "Input1:> 你们 能 做 些 什么 呢 你们 可以 行动 动起 起来 制造 声势 你们 可以 写信 写信给 你们 的 政府 并且 告诉 他们 我们 需要 去 关注 这些 被 严重 误解 的 生物 花 些 时间 告诉 你\n",
      "\n",
      "Target1:= How can you help  ? You can become active , make noise  . You can write a letter to your government and tell them that we need to focus on these very misunderstood creatures  . UNK your time \n",
      "Predict1:< You can can you you you you , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 在 未来 当 飞 米 成像 相机 在 你 的 手机 里 的 时候 你 就 可以 去 超市 不用 摸 就 能 检查 检查一下 一下 下水 水果 是否 熟 了\n",
      "\n",
      "Target2:= So , in the future , when this UNK is in your camera phone , you might be able to go to a supermarket and check if the fruit is ripe without actually touching it  . \n",
      "Predict2:< So , you you , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 6s (- -47m 3s), Epoch: [4/10], Step: [300/3335], Train Loss: 3.4503707885742188, BLEU: 0.5060237991453391\n",
      "\n",
      "Input1:> 谁 还 会 认真 认真对待 真对 对待 他 如果 他 只有 一个 这样 愚蠢 的 小头 头衔 比如 美利 美利坚 联邦 联邦政府 邦政府 政府 主持 UNK of the United UNK of America\n",
      "\n",
      "Target1:= And who is going to take him seriously if he is got a silly little title like President of the United States of America  ? \n",
      "Predict1:< And he , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 被 吓到 了\n",
      "\n",
      "Target2:= And I was scared  . \n",
      "Predict2:< I I my  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 11s (- -48m 57s), Epoch: [4/10], Step: [320/3335], Train Loss: 3.343601655960083, BLEU: 0.5193885622353879\n",
      "\n",
      "Input1:> 我们 有志 志愿 志愿者 愿者 团队 帮助 创业 辅助 商 为 客户 集结 资源 及 人力 我们 发现 当地 当地人 人群 的 智慧 能 创造 造出 出奇 奇迹 能 改变 社区 的 文化 和 经济 只要 你 能\n",
      "\n",
      "Target1:= We have groups of volunteers supporting the Enterprise UNK to help you to find resources and people and we have discovered that the miracle of the intelligence of local people is such that you can change the culture \n",
      "Predict1:< We we to to to , and , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 照片 片中 中美 美丽 的 和服 必须 通过 过手 手绘 处理 或者 一点 一点一点 一点 拼接 而 成 再配 配上 掉 了 的 颜色 把 那些 没有 被 水冲 UNK 的 地方 进行 细节 处理\n",
      "\n",
      "Target2:= The kimono in this shot pretty much had to be hand-drawn , or pieced together , picking out the remaining parts of color and detail that the water hadn t damaged  . \n",
      "Predict2:< The the the the the , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 17s (- -48m 51s), Epoch: [4/10], Step: [340/3335], Train Loss: 3.4381697297096254, BLEU: 0.5116025788762676\n",
      "\n",
      "Input1:> 并且 产生 了 很多 建议 这些 建议 貌似 似可 可行\n",
      "\n",
      "Target1:= And there were all sorts of suggestions on the table , which might have made it  . \n",
      "Predict1:< And the are that that , , , ,  .  .\n",
      "\n",
      "Input2:> 我们 现在 一起 躺 在 地板 上 我们 的 身体 都 太虚 虚弱 准备 等 死\n",
      "\n",
      "Target2:= We are lying on the floor together , and our bodies are so weak we are ready to die  . &quot; \n",
      "Predict2:< We are we we in in , , , , , the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 22s (- -48m 45s), Epoch: [4/10], Step: [360/3335], Train Loss: 3.4732531189918516, BLEU: 0.4988139274963916\n",
      "\n",
      "Input1:> 有着 如此 肥沃 的 山谷 当地 当地人 人居 居然 不 耕种 我们 对此 感到 十分 惊奇\n",
      "\n",
      "Target1:= And we were amazed that the local people , in such a fertile valley , would not have any agriculture  . \n",
      "Predict1:< And is , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 有 一天 我 最糟 糟糕 的 恶梦 成真 了 我 被 中国 警察 抓住 并 带去 警察 警察局 接受 受审 审问\n",
      "\n",
      "Target2:= One day , my worst nightmare came true , when I was caught by the Chinese police and brought to the police station for interrogation  . \n",
      "Predict2:< And I , , , , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 28s (- -48m 39s), Epoch: [4/10], Step: [380/3335], Train Loss: 3.414517319202423, BLEU: 0.537881931309676\n",
      "\n",
      "Input1:> 他们 究竟 竟为 为什么 什么 觉得 这样 不好 呢\n",
      "\n",
      "Target1:= Why in the hell would they not okay this  ? \n",
      "Predict1:< They do do do do  ?  ?  ?\n",
      "\n",
      "Input2:> 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定 女孩 上学 是 违法 的\n",
      "\n",
      "Target2:= You see , I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school  . \n",
      "Predict2:< I I , , , , , , , , , , the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 33s (- -48m 33s), Epoch: [4/10], Step: [400/3335], Train Loss: 3.389195239543915, BLEU: 0.5052360366066828\n",
      "\n",
      "Input1:> 下 一个 个人 人们 常 问 的 问题 是 他们 会 修 照片 吗\n",
      "\n",
      "Target1:= The next question people always ask me is , &quot; Do they retouch all the photos  ? &quot; \n",
      "Predict1:< The , , , , , , , , , , ,  ?  ?  ?\n",
      "\n",
      "Input2:> 告诉 别人 你 今天 听到 的\n",
      "\n",
      "Target2:= Talk about what you heard here  . \n",
      "Predict2:< You : you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 39s (- -48m 27s), Epoch: [4/10], Step: [420/3335], Train Loss: 3.428777503967285, BLEU: 0.5145027493872247\n",
      "\n",
      "Input1:> 那些 枪 对 我 来说 是 一个 明显 的 信号 尽管 他 并 没有 拿起 枪 指着 我 我 已经 无时 无时不刻 不 处在 危险 的 边缘\n",
      "\n",
      "Target1:= But those guns were really a message for me , and even though he hadn t raised a hand to me , my life was already in grave danger every minute of every day  . \n",
      "Predict1:< And was , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 一些 朝鲜 朝鲜人 在 中国 到 外国 国大 大使 大使馆 使馆 去 寻求 庇护 但是 很多 人 被 中国 警察 抓住 并 被 送回 朝鲜\n",
      "\n",
      "Target2:= Some North Koreans in China seek asylum in foreign embassies , but many can be caught by the Chinese police and UNK  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 44s (- -48m 21s), Epoch: [4/10], Step: [440/3335], Train Loss: 3.4067443728446962, BLEU: 0.5103377943441447\n",
      "\n",
      "Input1:> 禁止 在 一小 一小块 小块 土地 地上 种 食物 而 之前 这块 土地 却 没人 关心 笑\n",
      "\n",
      "Target1:= A warrant for planting food on a piece of land that you could care less about  ? &quot; \n",
      "Predict1:< The , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 援助 的 首要 原则 就是 尊重\n",
      "\n",
      "Target2:= The first principle of aid is respect  . \n",
      "Predict2:< And is is the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 50s (- -48m 15s), Epoch: [4/10], Step: [460/3335], Train Loss: 3.3800053238868712, BLEU: 0.5574999902853227\n",
      "\n",
      "Input1:> 很多 妇女 还 背着 自己 的 小孩 他们 在 淘金 在 被 汞 污染 的 脏水 水里 行走\n",
      "\n",
      "Target1:= Many women had children strapped to their backs while they were panning for gold , wading in water poisoned by mercury  . \n",
      "Predict1:< They are them to the , , , , , they , they , they  .  .\n",
      "\n",
      "Input2:> 一个 真正 的 学校\n",
      "\n",
      "Target2:= A real school  . \n",
      "Predict2:< It is a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47m 55s (- -48m 10s), Epoch: [4/10], Step: [480/3335], Train Loss: 3.4830841779708863, BLEU: 0.42829847173341395\n",
      "\n",
      "Input1:> 这 意味 意味着 患者 能 自己 搞定\n",
      "\n",
      "Target1:= That means they can be self-administered  . \n",
      "Predict1:< It is is the  .  .  .\n",
      "\n",
      "Input2:> 很多 秃鹫 被 毒死 中毒 的 原因 是因为 因为 在 肯尼 肯尼亚 尼亚 正在 发生 的 人类 与 野生 野生动物 生动 动物 之间 对于 生存 生存空间 空间 的 争夺 牧人 人们 使用 毒药 来 对付 掠 食 动物\n",
      "\n",
      "Target2:= The reason is that they are getting poisoned , and the reason that they are getting poisoned is because there is human-wildlife conflicts  . The pastoral communities are using this poison to target predators , and in return \n",
      "Predict2:< In the the of the , , , , , , , , , , , , , , , , , , , , , , the , the , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 1s (- -48m 4s), Epoch: [4/10], Step: [500/3335], Train Loss: 3.406867277622223, BLEU: 0.5205982864218317\n",
      "\n",
      "Input1:> 所以 我 希望 分享 这种 主动 且 有意 有意识 意识 去 创造 记忆 的 想法\n",
      "\n",
      "Target1:= So I want to share the idea of taking an active role in consciously creating memories  . \n",
      "Predict1:< So I I to to to to to to to to to  .  .\n",
      "\n",
      "Input2:> 让 你们 的 想法 简单 单易 易懂 并不等于 不等 等于 简化 它\n",
      "\n",
      "Target2:= And making your ideas accessible is not the same as dumbing it down  . \n",
      "Predict2:< And you you to to you you , is is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 6s (- -49m 58s), Epoch: [4/10], Step: [520/3335], Train Loss: 3.492591905593872, BLEU: 0.5153403786389591\n",
      "\n",
      "Input1:> 那些 生生 生生不息 不息 的 巨型 橡树 总是 可以 抚慰 我 的 灵魂 几百 几百年 百年 年来 情侣 醉汉 和 追 梦 人们 总会 稍息 在 树影 下 我 深信 这 一座 座充 充满 着 音乐 UNK UNK\n",
      "\n",
      "Target1:= My soul is always UNK by the giant live oak trees , shading lovers , drunks and dreamers for hundreds of years , and I trust a city that always makes way for music  . \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 此时 此时此刻 此刻 你 可能 在 想 哇 这 才 神奇 了 或者 喔 她 真 蠢 但是 在 整个 过程 中 事实 事实上 我 在 谈论 的 是 你\n",
      "\n",
      "Target2:= Right now , maybe you are thinking , &quot; Wow , this is fascinating , &quot; or , &quot; Wow , how stupid was she , &quot; but this whole time , I ve actually been talking about \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 12s (- -49m 52s), Epoch: [4/10], Step: [540/3335], Train Loss: 3.5263291597366333, BLEU: 0.4763865242145846\n",
      "\n",
      "Input1:> 他 从 一个 渔村 被 解救 解救出来 救出 出来\n",
      "\n",
      "Target1:= Kofi was rescued from a fishing village  . \n",
      "Predict1:< He is a a a a  .  .  .\n",
      "\n",
      "Input2:> 谁 还 会 认真 认真对待 真对 对待 他 如果 他 只有 一个 这样 愚蠢 的 小头 头衔 比如 美利 美利坚 联邦 联邦政府 邦政府 政府 主持 UNK of the United UNK of America\n",
      "\n",
      "Target2:= And who is going to take him seriously if he is got a silly little title like President of the United States of America  ? \n",
      "Predict2:< And he , a , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 17s (- -49m 47s), Epoch: [4/10], Step: [560/3335], Train Loss: 3.3748878121376036, BLEU: 0.4664859156205948\n",
      "\n",
      "Input1:> 就 好像 涂鸦 艺术 艺术家 术家 一样 他们 美化 墙壁 而 我 美化 草坪 绿化 绿化带\n",
      "\n",
      "Target1:= Just like a graffiti artist , where they UNK walls , me , I beautiful lawns , UNK  . \n",
      "Predict1:< And , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 到 家 时 电话 电话响 了 一个 声音 警告 他 如果 他 再 将 女儿 送去 学校 刚才 的 事情 就 会 再次 再次发生 发生\n",
      "\n",
      "Target2:= As he arrived home , the phone rang , a voice warning him that if he sent his daughter back to school , they would try again  . \n",
      "Predict2:< He he he to , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 23s (- -49m 41s), Epoch: [4/10], Step: [580/3335], Train Loss: 3.5263229966163636, BLEU: 0.46492279837536044\n",
      "\n",
      "Input1:> 这 应该 该是 援助 的 首要 原则\n",
      "\n",
      "Target1:= This should be the first principle of aid  . \n",
      "Predict1:< It is is the the  .  .  .\n",
      "\n",
      "Input2:> 现在 如果 你 把 这个 个子 子弹 这些 光子 打入 这个 瓶子 里 这些 光子 子会 怎么 怎么样 撞击 瓶子\n",
      "\n",
      "Target2:= Now , if you take that bullet and take this packet of photons and fire into this bottle , how will those photons shatter into this bottle  ? \n",
      "Predict2:< Now if you you you you you , , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 28s (- -49m 35s), Epoch: [4/10], Step: [600/3335], Train Loss: 3.4019383072853087, BLEU: 0.47603016527217523\n",
      "\n",
      "Input1:> 在 这 张 照片 里 我 事实 事实上 在 现实 现实生活 生活 中 根本 没有 男朋友 朋友\n",
      "\n",
      "Target1:= And in this picture , I had actually never had a boyfriend in real life  . \n",
      "Predict1:< And this , , , I I , I , I I I I  .\n",
      "\n",
      "Input2:> 我 的 第一 第一个 一个 答案 是 我 也 不知 知道 因为 他们 没让 我 来 掌管 这件 事\n",
      "\n",
      "Target2:= And the first answer is , &quot; I don t know , they don t put me in charge of that  . &quot; \n",
      "Predict2:< I I I , I , I , I , I , I , I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 34s (- -49m 30s), Epoch: [4/10], Step: [620/3335], Train Loss: 3.5312399744987486, BLEU: 0.5149690048395814\n",
      "\n",
      "Input1:> 现在 她 去 纽约 关注 的 是 大学 大学校园 学校 校园 因为 她 决定 定要 去 纽约 读书\n",
      "\n",
      "Target1:= And now what she is looking at in New York are colleges , because she is determined to go to school in New York  . \n",
      "Predict1:< And she , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 意识 到 如果 我 不 反抗 这个 我 曾 深爱 的 男人 会 杀掉 我\n",
      "\n",
      "Target2:= I realized that the man who I loved so much was going to kill me if I let him  . \n",
      "Predict2:< I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 40s (- -49m 24s), Epoch: [4/10], Step: [640/3335], Train Loss: 3.348922276496887, BLEU: 0.4862632041506945\n",
      "\n",
      "Input1:> 最 坏 的 消息 是 我 要 把 这件 毛衣 从头 头上 UNK 套进 套进去 进去 因为 这时 这时候 时候 你们 会 笑 我 所以 当 我 穿衣 穿衣服 衣服 的 时候 你们 什么 都 不要 做\n",
      "\n",
      "Target1:= The worst part is putting this sweater over my head , because that is when you ll all laugh at me , so don t do anything while it is over my head  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 无需 去 设计 一系 一系列 系列 新 的 医院 来 完成 测试\n",
      "\n",
      "Target2:= You don t have to design a whole new set of hospitals to do it  . \n",
      "Predict2:< You you to to to the the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 45s (- -49m 18s), Epoch: [4/10], Step: [660/3335], Train Loss: 3.4544426679611204, BLEU: 0.5078976638041459\n",
      "\n",
      "Input1:> 现在 如果 你 用 普通 的 子弹 走 同样 的 路程 平 且 放慢 视频 同样 是 一百 一百万 一百万倍 百万 百万倍 万倍 你 知道 你 需要 等 多久 来看 看到 这个 全过程 过程 吗\n",
      "\n",
      "Target1:= Now , if you take an ordinary bullet and let it go the same distance and slow down the video again by a factor of 10 billion , do you know how long you ll have to sit \n",
      "Predict1:< Now if you you you you you you , , , , , , you , , , you , , you , , you , , you , , you you\n",
      "\n",
      "Input2:> 但 这些 波纹 应该 向 我们 移动\n",
      "\n",
      "Target2:= The ripples should be moving towards us  . \n",
      "Predict2:< But we is we we we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 51s (- -49m 13s), Epoch: [4/10], Step: [680/3335], Train Loss: 3.4722309350967406, BLEU: 0.5378108934239708\n",
      "\n",
      "Input1:> 我 的 梦想 很大 但 他们 对 我 的 期待 甚至 更 高\n",
      "\n",
      "Target1:= I dream big , but my family dreams even bigger for me  . \n",
      "Predict1:< I I my my my , , , I to  .  .  .\n",
      "\n",
      "Input2:> 我 真的 被 吓坏 了 这些 学生 生发 发达 的 大脑 还有 他们 所 读 的 巨著 及 他们 使用 的 又 长 又 UNK 的 词语 通通 通都 吓到 我 了\n",
      "\n",
      "Target2:= Really scared  . Scared of these students with their big brains and their big books and their big , unfamiliar words  . \n",
      "Predict2:< I I I to to the the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 48m 56s (- -49m 7s), Epoch: [4/10], Step: [700/3335], Train Loss: 3.461645805835724, BLEU: 0.5245234937910473\n",
      "\n",
      "Input1:> 他们 会 把 尸体 上 的 肉 吃 的 干干净净 干净 净 只 剩 UNK\n",
      "\n",
      "Target1:= They clean up carcasses right to the bone  . \n",
      "Predict1:< They they to the the the the the the the  .\n",
      "\n",
      "Input2:> 太 不 简单 了\n",
      "\n",
      "Target2:= Extraordinary  . \n",
      "Predict2:< It is not  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 2s (- -49m 1s), Epoch: [4/10], Step: [720/3335], Train Loss: 3.411879849433899, BLEU: 0.5256580982539231\n",
      "\n",
      "Input1:> 你 可以 向 客户 提出 你 的 点子\n",
      "\n",
      "Target1:= You can give somebody an idea  . \n",
      "Predict1:< You can can to the the  .  .  .\n",
      "\n",
      "Input2:> 在 墙 后面 面有 一个 个人 人体 人体模型 模型 我们 将 进行 光反射 反射\n",
      "\n",
      "Target2:= There is a mannequin hidden behind a wall , and we are going to bounce light off the door  . \n",
      "Predict2:< In the a , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 7s (- -50m 56s), Epoch: [4/10], Step: [740/3335], Train Loss: 3.505241572856903, BLEU: 0.5597684591604796\n",
      "\n",
      "Input1:> 当 我 今天 站 在 这里 跟 你们 演讲 的 时候 这些 矿工 们 还 在 这样 的 矿井 里 冒 着 生命 生命危险 危险 干活 没有 工资 没有 补贴 还 经常 丢 命\n",
      "\n",
      "Target1:= As I stand talking to you today , these men are still deep in that hole , risking their lives without payment or compensation , and often dying  . \n",
      "Predict1:< When I I you you , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 相信 打开 那 扇门 的 关键 就是 要 有 很 好 的 交流 与 沟通\n",
      "\n",
      "Target2:= And I believe the key to opening that door is great communication  . \n",
      "Predict2:< I I , that that , is , , is , , is is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 13s (- -50m 50s), Epoch: [4/10], Step: [760/3335], Train Loss: 3.467104208469391, BLEU: 0.5219826707818833\n",
      "\n",
      "Input1:> 谢谢\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 在 死 之前 我 想要 做 完全 全真 真实 的 自己\n",
      "\n",
      "Target2:= &quot; Before I die , I want to be completely myself  . &quot; \n",
      "Predict2:< I I I I I I I I to I to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 18s (- -50m 45s), Epoch: [4/10], Step: [780/3335], Train Loss: 3.429835557937622, BLEU: 0.5482940165251527\n",
      "\n",
      "Input1:> 我 在 那里 成长 我 在 那里 养育 我 的 儿子\n",
      "\n",
      "Target1:= I grew up there  . I raised my sons there  . \n",
      "Predict1:< I I I I I I I I I I I  .  .\n",
      "\n",
      "Input2:> 我 最后 爬出 了 矿井 得以 回家 而 这些 被 奴役 的 矿工 可能 永远 没有 回家 的 那 一天\n",
      "\n",
      "Target2:= I got to climb out of that hole , and I got to go home , but they likely never will , because they are trapped in slavery  . \n",
      "Predict2:< And I I , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 24s (- -50m 39s), Epoch: [4/10], Step: [800/3335], Train Loss: 3.368838059902191, BLEU: 0.5504073446024204\n",
      "\n",
      "Input1:> 这 是 一个 意外 而 他 以后 不会 再 伤害 我\n",
      "\n",
      "Target1:= It was an isolated incident , and he was never going to hurt me again  . \n",
      "Predict1:< This is a a , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 援助 的 首要 原则 就是 尊重\n",
      "\n",
      "Target2:= The first principle of aid is respect  . \n",
      "Predict2:< The is is the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 29s (- -50m 33s), Epoch: [4/10], Step: [820/3335], Train Loss: 3.4459815859794616, BLEU: 0.5203479245735781\n",
      "\n",
      "Input1:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target1:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict1:< And is , you you you you you you you you you you you you\n",
      "\n",
      "Input2:> 内罗毕 国家 公园 的 南部 并 没有 全部 围起 栅栏 这 就 意味 意味着 像 斑马 这样 的 野生 野生动物 生动 动物 可以 自由 地 在 公园 园外 UNK 移动\n",
      "\n",
      "Target2:= Nairobi National Park is not UNK in the south widely , which means wild animals like zebras migrate out of the park freely  . \n",
      "Predict2:< The the the , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 35s (- -50m 28s), Epoch: [4/10], Step: [840/3335], Train Loss: 3.5019674062728883, BLEU: 0.5370025736793204\n",
      "\n",
      "Input1:> 每 20 分钟 我 就 必须 跑 回车 车上 清理 我 的 设备 让 它 在 空调 底下 吹 着 冷气 它 才能 重新 重新启动 启动 我 坐在 车上 想 就 连 摄像 摄像机 受到 的 待遇 都\n",
      "\n",
      "Target1:= Every 20 minutes , I d have to run back to our UNK to clean out my gear and run it under an air conditioner to revive it , and as I sat there , I thought , \n",
      "Predict1:< And I I I I , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 现在 如果 你 把 这个 个子 子弹 这些 光子 打入 这个 瓶子 里 这些 光子 子会 怎么 怎么样 撞击 瓶子\n",
      "\n",
      "Target2:= Now , if you take that bullet and take this packet of photons and fire into this bottle , how will those photons shatter into this bottle  ? \n",
      "Predict2:< Now if you you you you you , , , , , , , , , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 40s (- -50m 22s), Epoch: [4/10], Step: [860/3335], Train Loss: 3.3941235899925233, BLEU: 0.5036475770379384\n",
      "\n",
      "Input1:> 他们 把 这些 照片 等 东西 收集 起来 把 它们 送往 各个 镇里 进行 保管\n",
      "\n",
      "Target1:= They were collecting them up , and handing them in to various places around the different towns for UNK  . \n",
      "Predict1:< They they they to they , , , they they  .  .\n",
      "\n",
      "Input2:> 然而 那时 他们 再 要 拿 回 这些 照片 所需 花 的 时间 就 截然 截然不同 不同 了 当然 这 取决 取决于 受损 程度\n",
      "\n",
      "Target2:= The time it took , however , to get it back is a completely different story , and it depended obviously on the damage involved  . \n",
      "Predict2:< But they , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 46s (- -50m 17s), Epoch: [4/10], Step: [880/3335], Train Loss: 3.3908248782157897, BLEU: 0.5179904552376864\n",
      "\n",
      "Input1:> 居高临下 意味 意味着 我 把 另 一文 文化 文化背景 背景 的 人 当做 我 的 仆人 对待\n",
      "\n",
      "Target1:= UNK , I treat everybody from another culture as if they were my servants  . \n",
      "Predict1:< And I I I I I I , I , I , I I  .  .\n",
      "\n",
      "Input2:> 这 是 我们 亲子 相处 的 时间 是 一年 里面 我们 最 珍惜 也 最 期待 的 时间\n",
      "\n",
      "Target2:= This very focused time we get to spend together is something we cherish and anticipate the entire year  . \n",
      "Predict2:< This is is a we we , we , we , we we , we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 51s (- -50m 11s), Epoch: [4/10], Step: [900/3335], Train Loss: 3.516837203502655, BLEU: 0.5230200181782791\n",
      "\n",
      "Input1:> 然后 他 跟 我 解释 到 在 这样 的 地方 抒发 发自 自己 的 情绪 是 很 危险 的 不仅 对 我 而言 危险 对 这些 工人 人们 也 是\n",
      "\n",
      "Target1:= And he very clearly explained to me that emotional displays are very dangerous in a place like this , not just for me , but for them  . \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 又 一次 我们 坐 的 巴士 被 拦下 了 一个 中国 警官 上 了 车\n",
      "\n",
      "Target2:= One time , our bus was stopped and boarded by a Chinese police officer  . \n",
      "Predict2:< And , , , , , , , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 49m 57s (- -50m 5s), Epoch: [4/10], Step: [920/3335], Train Loss: 3.5200705528259277, BLEU: 0.5218705485608802\n",
      "\n",
      "Input1:> 现在 我 在 这个 房间 房间内 感到 一种 令人 不适 的 紧张 紧张感 因为 我 不该 穿 这条 裙子\n",
      "\n",
      "Target1:= And I feel like there is an uncomfortable tension in the room right now because I should not have worn this dress  . \n",
      "Predict1:< Now I I I I I I I I I , I , I , I  .\n",
      "\n",
      "Input2:> 这 就是 这个 项目 目的 全部 为了 恢复 人性 中 的 一点 小 碎片 挽回 UNK 人们 与 过去 的 连接 连接点 接点\n",
      "\n",
      "Target2:= That is all this project was about , about restoring those little bits of humanity , giving someone that connection back  . \n",
      "Predict2:< And this is the the the the the the the the the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 2s (- -50m 0s), Epoch: [4/10], Step: [940/3335], Train Loss: 3.4943707942962647, BLEU: 0.5249029664190444\n",
      "\n",
      "Input1:> 我 希望 鼓励 今天 在座 的 各位 争取 取到 照片 里面 去 别 犹豫 走向 向前 问路 路人 可以 帮 我们 拍照 吗\n",
      "\n",
      "Target1:= So I want to encourage everyone today to get in the shot , and don t hesitate to go up to someone and ask , &quot; Will you take our picture  ? &quot; \n",
      "Predict1:< I I to to to to to , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 不 贪心 也 不在 不在乎 在乎 别人 是 什么 肤色\n",
      "\n",
      "Target2:= He is not greedy  . He doesn t see skin color  . \n",
      "Predict2:< He he t t he , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 8s (- -51m 54s), Epoch: [4/10], Step: [960/3335], Train Loss: 3.4361815571784975, BLEU: 0.511214209053744\n",
      "\n",
      "Input1:> UNK 今年 22 岁 又 高 又 帅\n",
      "\n",
      "Target1:= Remi is 22 , tall and very handsome  . \n",
      "Predict1:< And : : , , ,  .\n",
      "\n",
      "Input2:> 于是 有 了 第二 第二个 二个 办法 利用 稻草 稻草人 草人\n",
      "\n",
      "Target2:= And a second idea I got was to use a scarecrow  . \n",
      "Predict2:< So , is a , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 13s (- -51m 49s), Epoch: [4/10], Step: [980/3335], Train Loss: 3.391366970539093, BLEU: 0.5143238210088066\n",
      "\n",
      "Input1:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target1:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict1:< We we we to the the , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 政府 指责 我 给 了 我 张 告 票 通知 我 必须 移除 我 的 花园 之后 告 票 变成 了 警告 函\n",
      "\n",
      "Target2:= The city came down on me , and basically gave me a citation saying that I had to remove my garden , which this citation was turning into a warrant  . \n",
      "Predict2:< And I I I I I , I , I , I , I , I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 19s (- -51m 43s), Epoch: [4/10], Step: [1000/3335], Train Loss: 3.4163920164108275, BLEU: 0.5253668791575475\n",
      "\n",
      "Input1:> 这 两个 词 来自 于 拉丁 词根 UNK 意 即 父亲\n",
      "\n",
      "Target1:= The two words come from the Latin root &quot; UNK , &quot; which means &quot; father  . &quot; \n",
      "Predict1:< This is a a , , , , , ,\n",
      "\n",
      "Input2:> 这 是 现在 的 我\n",
      "\n",
      "Target2:= And here is me today  . \n",
      "Predict2:< That is is my  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 24s (- -51m 38s), Epoch: [4/10], Step: [1020/3335], Train Loss: 3.528168869018555, BLEU: 0.5275464155623498\n",
      "\n",
      "Input1:> 在 死 之前 我 想为 我 的 海盗 UNK 行为 接受 受审 审判\n",
      "\n",
      "Target1:= &quot; Before I die , I want to be tried for piracy  . &quot; \n",
      "Predict1:< In , I I I I I I I I I I\n",
      "\n",
      "Input2:> 由于 风化 紫外 紫外线 外线 氧化 等 原因 这种 粘合 粘合剂 合剂 这种 沥青 集料 之间 的 黏胶 会 收缩 如果 它 收缩 就 会 有 小 裂纹 从 集 块 开始 会 出现 分层\n",
      "\n",
      "Target2:= Due to UNK , due to UNK light , due to oxidation , this binder , this bitumen , the glue between the aggregates is going to shrink , and if it shrinks , it gets UNK , \n",
      "Predict2:< And the the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 30s (- -51m 32s), Epoch: [4/10], Step: [1040/3335], Train Loss: 3.5310001492500307, BLEU: 0.5375649136859681\n",
      "\n",
      "Input1:> 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也 珍爱 UNK 女儿 的 家庭\n",
      "\n",
      "Target1:= I was very lucky to grow up in a family where education was prized and daughters were treasured  . \n",
      "Predict1:< I I a a a a a a , a a a  .  .\n",
      "\n",
      "Input2:> 世界 上 所有 有成 成功 的 企业 只有 一个 共同 共同点 同点 只有 一个 没有 一个 企业 是 靠 一 人 之 力 创建 的\n",
      "\n",
      "Target2:= There is only one thing that all the successful companies in the world have in common , only one : None were started by one person  . \n",
      "Predict2:< The is a of of , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 35s (- -51m 26s), Epoch: [4/10], Step: [1060/3335], Train Loss: 3.4917668223381044, BLEU: 0.5304370869014814\n",
      "\n",
      "Input1:> 这 世上 从来 从来没 没有 一个 个人 能 同时 做好 好生 生产 销售 以及 财务 的 工作\n",
      "\n",
      "Target1:= We have never met a single human being in the world who can make it , sell it and look after the money  . \n",
      "Predict1:< It is a a a , , , , , , , ,\n",
      "\n",
      "Input2:> 虽然 偶尔 会 有 一些 些小 小型 殖民 殖民地 的 会议 主持 主持人 并 有 一丝 政府 管理 的 意思 但是 它 真的 是 个 什么 都 不是 的 头衔\n",
      "\n",
      "Target2:= There were occasional presidents of little colonial councils and bits of government , but it was really a nothing title  . \n",
      "Predict2:< And , , , a , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 41s (- -51m 21s), Epoch: [4/10], Step: [1080/3335], Train Loss: 3.528818166255951, BLEU: 0.5184582840306065\n",
      "\n",
      "Input1:> 但 我 并 没有 因此 放弃 我 坚持 坚持不懈 不懈 着\n",
      "\n",
      "Target1:= So I didn t give up  . I continued  . \n",
      "Predict1:< But I I t t t I t I  .  .  .\n",
      "\n",
      "Input2:> 准备 大学 入学 入学考试 考试\n",
      "\n",
      "Target2:= I started studying for the university entrance exam  . \n",
      "Predict2:< The : : ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 47s (- -51m 15s), Epoch: [4/10], Step: [1100/3335], Train Loss: 3.443132221698761, BLEU: 0.5105110078325115\n",
      "\n",
      "Input1:> 他们 必须 须要 赶快 快逃 逃走 所以 我 开始 计划 帮助 他们 逃脱\n",
      "\n",
      "Target1:= They had to get out quickly , so I started planning how to help them escape  . \n",
      "Predict1:< They they to to to , , , , , , to to to  .  .\n",
      "\n",
      "Input2:> 不是 的\n",
      "\n",
      "Target2:= Not exactly  . \n",
      "Predict2:< It is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 52s (- -51m 10s), Epoch: [4/10], Step: [1120/3335], Train Loss: 3.4801522135734557, BLEU: 0.5796198270150592\n",
      "\n",
      "Input1:> 此时 此时此刻 此刻 你 可能 在 想 哇 这 才 神奇 了 或者 喔 她 真 蠢 但是 在 整个 过程 中 事实 事实上 我 在 谈论 的 是 你\n",
      "\n",
      "Target1:= Right now , maybe you are thinking , &quot; Wow , this is fascinating , &quot; or , &quot; Wow , how stupid was she , &quot; but this whole time , I ve actually been talking about \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target2:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict2:< And is you you you you you you you you you you you you you you you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50m 58s (- -51m 4s), Epoch: [4/10], Step: [1140/3335], Train Loss: 3.402263379096985, BLEU: 0.5707746392415521\n",
      "\n",
      "Input1:> 很 感谢 你 UNK 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you so much  . Thank you  . \n",
      "Predict1:< Thank you you you  .  .\n",
      "\n",
      "Input2:> 我们 在 纽约 的 地铁 上 碰巧 坐在 一起 他 先 跟 我 聊 了 起来\n",
      "\n",
      "Target2:= He sat next to me on the New York City subway , and he started chatting me up  . \n",
      "Predict2:< We went the , , , , , , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 3s (- -52m 59s), Epoch: [4/10], Step: [1160/3335], Train Loss: 3.4849227547645567, BLEU: 0.5654284898143601\n",
      "\n",
      "Input1:> 你 无需 去 设计 一系 一系列 系列 新 的 医院 来 完成 测试\n",
      "\n",
      "Target1:= You don t have to design a whole new set of hospitals to do it  . \n",
      "Predict1:< You you to to to the the the the the the  .\n",
      "\n",
      "Input2:> 但是 总 需要 有人 维护 它\n",
      "\n",
      "Target2:= But you have to maintain it  . \n",
      "Predict2:< But it is it to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 9s (- -52m 53s), Epoch: [4/10], Step: [1180/3335], Train Loss: 3.4427913546562197, BLEU: 0.5222329633560764\n",
      "\n",
      "Input1:> 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人 挤 在 一个 小 客厅 厅里\n",
      "\n",
      "Target1:= The school was in a house , more than 100 of us packed in one small living room  . \n",
      "Predict1:< In the a , , , , , , , , , , a  .  .\n",
      "\n",
      "Input2:> 那双 被 染黑 了 的 手 属于 这家 的 父亲 蓝色 和 红色 的 手 属于 两个 个儿 儿子\n",
      "\n",
      "Target2:= The dyed black hands are the father , while the blue and red hands are his sons  . \n",
      "Predict2:< And is the the , , , , , and , and and and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 14s (- -52m 47s), Epoch: [4/10], Step: [1200/3335], Train Loss: 3.423172318935394, BLEU: 0.515884268517106\n",
      "\n",
      "Input1:> 食物 是 问题 食物 也 是 解决 解决方案 方案\n",
      "\n",
      "Target1:= Food is the problem and food is the solution  . \n",
      "Predict1:< It is is is is is is  .  .\n",
      "\n",
      "Input2:> 通过 过透 透露 他 不为 不为人知 的 秘密 他 在 我们 之间 营造 了 奇特 的 相互 相互信任 互信 信任 的 氛围 他 告诉 我 他 从 四岁 开始 他 的 继父 就 开始 持续 野蛮 的 在\n",
      "\n",
      "Target2:= And he also created a magical atmosphere of trust between us by confessing his secret , which was that , as a very young boy starting at age four , he had been savagely and repeatedly physically abused \n",
      "Predict2:< He was , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 20s (- -52m 42s), Epoch: [4/10], Step: [1220/3335], Train Loss: 3.5301421642303468, BLEU: 0.5573919771104361\n",
      "\n",
      "Input1:> 所以 他们 给 他 一个 他们 能够 想到 的 最低 低微 的 最 微不足道 不足 不足道 的 最为 可悲 的 头衔\n",
      "\n",
      "Target1:= So they wanted to give him the UNK , UNK , most pathetic title that they could think of  . \n",
      "Predict1:< So they they to to to to the the the , , , the their their their  .  .\n",
      "\n",
      "Input2:> 如今 芭蕾 需要 非凡 的 专业 专业知识 知识 和 杰出 的 专业 技巧 甚至 一定 的 初级 适应 适应性 这 可能 就是 所说 的 天赋\n",
      "\n",
      "Target2:= Now , ballet requires an extraordinary level of expertise and a high level of skill , and probably a level of initial suitability that may well have a genetic component to it  . \n",
      "Predict2:< And , , , , , the the , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 25s (- -52m 36s), Epoch: [4/10], Step: [1240/3335], Train Loss: 3.518061661720276, BLEU: 0.559185840544313\n",
      "\n",
      "Input1:> 一些 光 流 了 出来 到 了 桌子 上 所以 你 开始 看到 这些 波纹\n",
      "\n",
      "Target1:= Some of the light leaks , goes on the table , and you start seeing these ripples of waves  . \n",
      "Predict1:< And you you you , , you , , , you you you  .\n",
      "\n",
      "Input2:> 又 一次 我们 坐 的 巴士 被 拦下 了 一个 中国 警官 上 了 车\n",
      "\n",
      "Target2:= One time , our bus was stopped and boarded by a Chinese police officer  . \n",
      "Predict2:< And we we we , , , , we a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 31s (- -52m 31s), Epoch: [4/10], Step: [1260/3335], Train Loss: 3.4389845371246337, BLEU: 0.5224134994884746\n",
      "\n",
      "Input1:> 最终 在 经历 了 漫长 的 旅行 后 我 和 家人 终于 在 韩国 重聚 但是 获得 自由 只是 打赢 了 战争 的 一半\n",
      "\n",
      "Target1:= Eventually , after our long journey , my family and I were reunited in South Korea , but getting to freedom is only half the battle  . \n",
      "Predict1:< And , , , , , , , , , , , , , , and , and , and and and and and\n",
      "\n",
      "Input2:> 但是 现在 你 知道 现在 有 多少 个 国家 有 总统 吗\n",
      "\n",
      "Target2:= But now , do you know how many nations have a president  ? \n",
      "Predict2:< But , you you you you you you you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 36s (- -52m 25s), Epoch: [4/10], Step: [1280/3335], Train Loss: 3.426754415035248, BLEU: 0.5289398574600231\n",
      "\n",
      "Input1:> 第一 第一次 一次 我 驾车 去 寻找 秘密 的 海滩 我 迷路 了 他 坐在 副驾驶 驾驶 驾驶座 上 不停 的 打 我 的 头 打得 如此 的 使劲 以至 以至于 至于 我 的 头 不断 的 撞到\n",
      "\n",
      "Target1:= The first time , I was driving to find a secret beach and I got lost , and he punched me in the side of my head so hard that the other side of my head repeatedly hit \n",
      "Predict1:< I first first , , , , I , I , , I , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 相信 打开 那 扇门 的 关键 就是 要 有 很 好 的 交流 与 沟通\n",
      "\n",
      "Target2:= And I believe the key to opening that door is great communication  . \n",
      "Predict2:< I I , that that is is , is is , is is is is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 42s (- -52m 20s), Epoch: [4/10], Step: [1300/3335], Train Loss: 3.3650981783866882, BLEU: 0.492860910300715\n",
      "\n",
      "Input1:> 这 看上 看上去 上去 就 像 这样\n",
      "\n",
      "Target1:= It will look something like this  . \n",
      "Predict1:< And is is it ,  .  .  .\n",
      "\n",
      "Input2:> 例如 密歇根 密歇根州 迪 尔 伯恩 市 的 福特 福特汽车公司 汽车 公司\n",
      "\n",
      "Target2:= Dearborn , Michigan  . Henry Ford  . \n",
      "Predict2:< The : : : , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 47s (- -52m 14s), Epoch: [4/10], Step: [1320/3335], Train Loss: 3.5698448538780214, BLEU: 0.5087351558687238\n",
      "\n",
      "Input1:> 我 把 花园 和 土地 视为 画布 而 植物 和 树木 就是 我 在 画布 上 的 装饰\n",
      "\n",
      "Target1:= I use the garden , the soil , like it is a piece of cloth , and the plants and the trees , that is my UNK for that cloth  . \n",
      "Predict1:< I I the the and and and and and and and and and  .\n",
      "\n",
      "Input2:> 我 的 祖父 在 他 的 年代 是 位非 非凡 的 人\n",
      "\n",
      "Target2:= My grandfather was an extraordinary man for his time  . \n",
      "Predict2:< My was was the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 53s (- -52m 9s), Epoch: [4/10], Step: [1340/3335], Train Loss: 3.540112292766571, BLEU: 0.5418766299356997\n",
      "\n",
      "Input1:> 然后 他们 放 我 走 了 真是 个 奇迹\n",
      "\n",
      "Target1:= And they let me go  . It was a miracle  . \n",
      "Predict1:< And then I I , , , ,  .  .\n",
      "\n",
      "Input2:> 至今 为止 我 已为 我们 家族 社区 中 的 UNK 家庭 家庭装 装上 了 这个 赶 狮 灯 它们 非常 常有 有用\n",
      "\n",
      "Target2:= Since now , I ve set up seven homes around my community , and they are really working  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 51m 58s (- -52m 3s), Epoch: [4/10], Step: [1360/3335], Train Loss: 3.596217942237854, BLEU: 0.5741143985895264\n",
      "\n",
      "Input1:> 许多 光子 最终 终到 到达 了 瓶盖 处 并且 向 四周 散去\n",
      "\n",
      "Target1:= Many of the photons eventually reach the cap and then they explode in various directions  . \n",
      "Predict1:< And is , , , , , , , , the  .  .\n",
      "\n",
      "Input2:> 我 确实 实相 相信 当 我们 把 他们 当做 同胞 就 会 很 难容 容忍 奴役 这样 的 暴行\n",
      "\n",
      "Target2:= I truly believe , if we can see one another as fellow human beings , then it becomes very difficult to tolerate atrocities like slavery  . \n",
      "Predict2:< I I I , , , , , , , , , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 4s (- -53m 57s), Epoch: [4/10], Step: [1380/3335], Train Loss: 3.4451733827590942, BLEU: 0.5346635812508945\n",
      "\n",
      "Input1:> 比如 艾哈迈 艾哈迈德 哈迈德 迈德 这 不是 真名 我 也 不能 让 你们 看见 他 的 长相 他 是 我 一个 学生 的 父亲\n",
      "\n",
      "Target1:= Like Ahmed  . That is not his real name , and I cannot show you his face , but Ahmed is the father of one of my students  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 再 需要 一个 像 这样 的 机器 一个 感应 UNK 电机 它 可以 用来 做饭\n",
      "\n",
      "Target2:= Then you need a machine , like you see here , that you can use for cooking -- an induction machine  . \n",
      "Predict2:< You you you to a a a a a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 9s (- -53m 52s), Epoch: [4/10], Step: [1400/3335], Train Loss: 3.4383995175361632, BLEU: 0.4421106661244753\n",
      "\n",
      "Input1:> 但 这 就是 自我 的 形成\n",
      "\n",
      "Target1:= But this is the geography of self  . \n",
      "Predict1:< But this is is the  .  .\n",
      "\n",
      "Input2:> 在 左边 你 看到 了 我们 的 飞秒 成像 相机\n",
      "\n",
      "Target2:= On the left , you see our UNK  . \n",
      "Predict2:< In the the , , we the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 15s (- -53m 46s), Epoch: [4/10], Step: [1420/3335], Train Loss: 3.4874107360839846, BLEU: 0.48817093657611754\n",
      "\n",
      "Input1:> 当 他们 遭受 海啸 袭击 之时 可想 可想而知 必然 四处 四处奔波 奔波 寻求 一线 一线生机 生机 自然 把 其他 东西 他们 的 所有 东西 西都 抛 诸 脑 后\n",
      "\n",
      "Target1:= As they had run from the wave , and for their lives , absolutely everything they had , everything had to be left behind  . \n",
      "Predict1:< When when when to to the , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 无法 想象 向日葵 有 多么 的 神奇 以及 它 将 对 人 产生 何种 影响\n",
      "\n",
      "Target2:= You just couldn t imagine how amazing a sunflower is and how it affects people  . \n",
      "Predict2:< You you you to to , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 20s (- -53m 41s), Epoch: [4/10], Step: [1440/3335], Train Loss: 3.3177995681762695, BLEU: 0.48944535062841876\n",
      "\n",
      "Input1:> 所以 像 狮子 这样 的 捕食 捕食者 食者 会 跟随 他们 这 就是 他们 的 所作 所作所为 所为\n",
      "\n",
      "Target1:= So predators like lions follow them , and this is what they do  . \n",
      "Predict1:< So , they , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 希望 你们 明白 的 是 这些 不是 我 自己 的 照片\n",
      "\n",
      "Target2:= And I hope what you are seeing is that these pictures are not pictures of me  . \n",
      "Predict2:< I am I I you you , , , , I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 26s (- -53m 35s), Epoch: [4/10], Step: [1460/3335], Train Loss: 3.462091255187988, BLEU: 0.5353327912452488\n",
      "\n",
      "Input1:> 议员 甚至 前来 拜访 表达 了 他 有 多么 赞赏 和 热爱 我们 所 做 的\n",
      "\n",
      "Target1:= My UNK even called in and said how they endorse and love what we are doing  . \n",
      "Predict1:< And is , , , , , , , and , and  .  .\n",
      "\n",
      "Input2:> 这 很 怪\n",
      "\n",
      "Target2:= That was awkward  . \n",
      "Predict2:< It is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 32s (- -53m 30s), Epoch: [4/10], Step: [1480/3335], Train Loss: 3.3060368418693544, BLEU: 0.5663346860650599\n",
      "\n",
      "Input1:> 这个 小男孩 男孩 8 岁 大\n",
      "\n",
      "Target1:= This young child is eight years old  . \n",
      "Predict1:< This is a the  .  .  .\n",
      "\n",
      "Input2:> 你 有 能力 制止 家庭 家庭暴力 暴力 只 需要 要点 点亮 星星 星星之火\n",
      "\n",
      "Target2:= You have the power to end domestic violence simply by shining a spotlight on it  . \n",
      "Predict2:< You you have to to , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 37s (- -53m 24s), Epoch: [4/10], Step: [1500/3335], Train Loss: 3.5655988216400147, BLEU: 0.5375685105357731\n",
      "\n",
      "Input1:> 我们 的 手工 清洗 活动 非常 顺利\n",
      "\n",
      "Target1:= Now , we were lucky with our UNK  . \n",
      "Predict1:< We are we to  .  .  .  .\n",
      "\n",
      "Input2:> UNK 今年 22 岁 又 高 又 帅\n",
      "\n",
      "Target2:= Remi is 22 , tall and very handsome  . \n",
      "Predict2:< And , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 43s (- -53m 19s), Epoch: [4/10], Step: [1520/3335], Train Loss: 3.498650300502777, BLEU: 0.5338660780288804\n",
      "\n",
      "Input1:> 整个 过程 非常 费时\n",
      "\n",
      "Target1:= It was very time-consuming  . \n",
      "Predict1:< It is very  .  .  .\n",
      "\n",
      "Input2:> 我 看起 看起来 起来 不 像是 典型 的 家庭 家庭暴力 暴力 受害 受害者\n",
      "\n",
      "Target2:= I don t look like a typical domestic violence survivor  . \n",
      "Predict2:< I I t to to , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 48s (- -53m 13s), Epoch: [4/10], Step: [1540/3335], Train Loss: 3.4553451895713807, BLEU: 0.5301751121638388\n",
      "\n",
      "Input1:> 我们 自己 也 变得 又 脏 又 臭 但是 我们 心里 很 舒坦\n",
      "\n",
      "Target1:= We got dirty , and we loved it  . \n",
      "Predict1:< We we t we , , we , we we  .  .  .\n",
      "\n",
      "Input2:> 相反 正如 爱因斯坦 因斯 坦 所说 让 一切 尽可 尽可能 可能 的 简单 而 不是 简化\n",
      "\n",
      "Target2:= Instead , as Einstein said , make everything as simple as possible , but no simpler  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 54s (- -53m 7s), Epoch: [4/10], Step: [1560/3335], Train Loss: 3.447783052921295, BLEU: 0.5145519259721283\n",
      "\n",
      "Input1:> 园艺 工作 就是 我 的 涂鸦 我 创造 自己 的 艺术\n",
      "\n",
      "Target1:= Gardening is my graffiti  . I grow my art  . \n",
      "Predict1:< And my my my I I I I I I\n",
      "\n",
      "Input2:> 记得 得当 当时 站 在 几乎 是 一片 黑暗 当中 我 刹那 刹那间 那间 感到 一阵 强烈 的 恐惧 那一刻 一刻 我 在 想 被困 在 这个 地狱 里 是 多么 的 恐怖\n",
      "\n",
      "Target2:= Standing in the near darkness , I remember feeling this quick , hot fear , and in that instant , I could only imagine what it must be like to be trapped in that hell  . \n",
      "Predict2:< And , , I , I , , I , , , , I , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 52m 59s (- -53m 2s), Epoch: [4/10], Step: [1580/3335], Train Loss: 3.4839892268180845, BLEU: 0.5393147513061198\n",
      "\n",
      "Input1:> 让 你们 的 想法 简单 单易 易懂 并不等于 不等 等于 简化 它\n",
      "\n",
      "Target1:= And making your ideas accessible is not the same as dumbing it down  . \n",
      "Predict1:< So you you to to is is is is is  .  .  .\n",
      "\n",
      "Input2:> 如果 孩子 们 种植 甘蓝 孩子 们 就 会 吃 甘蓝\n",
      "\n",
      "Target2:= If kids grow kale , kids eat kale  . \n",
      "Predict2:< If if if , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 5s (- -54m 56s), Epoch: [4/10], Step: [1600/3335], Train Loss: 3.3589912056922913, BLEU: 0.5386704856904996\n",
      "\n",
      "Input1:> 对于 阿富汗 我 意识 到 这 是 西方 方国 国家 也 不曾 意识 到 的 即 在 我们 成功 的 背后 是 一个 这样 的 父亲 他 能 认识 到 自己 女儿 的 价值 也 明白 女儿 的\n",
      "\n",
      "Target1:= What I ve come to realize about Afghanistan , and this is something that is often dismissed in the West , that behind most of us who succeed is a father who recognizes the value in his daughter \n",
      "Predict1:< And , I , , , , , , , , , , , , , , , , , , , , , , , , , , , the the the\n",
      "\n",
      "Input2:> 我心 心灰 心灰意冷\n",
      "\n",
      "Target2:= I lost all hope  . \n",
      "Predict2:< And is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 10s (- -54m 51s), Epoch: [4/10], Step: [1620/3335], Train Loss: 3.4118427276611327, BLEU: 0.5385758980613393\n",
      "\n",
      "Input1:> 众议 众议院 议院 不想 让 华盛 华盛顿 沉醉 沉醉于 权力\n",
      "\n",
      "Target1:= The House of Representatives didn t want Washington to get drunk on power  . \n",
      "Predict1:< And is t to to to to to  .  .\n",
      "\n",
      "Input2:> 我们 很 害怕 但是 我们 仍然 渴望 望去 学校\n",
      "\n",
      "Target2:= We were scared , but still , school was where we wanted to be  . \n",
      "Predict2:< We we we we we we we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 16s (- -54m 45s), Epoch: [4/10], Step: [1640/3335], Train Loss: 3.3149358153343202, BLEU: 0.5656832237980769\n",
      "\n",
      "Input1:> 他们 认为 是 个 称号 是 过去 用 过 并经 经过 考验 的\n",
      "\n",
      "Target1:= They thought it was tried and tested  . \n",
      "Predict1:< They are a a a , , , ,  .  .  .  .\n",
      "\n",
      "Input2:> 这 张 是 在 911 的 几周 后 拍 的 我 那时 尝试 解释 911 那天 发生 的 事 试图 采用 让 这个 五岁 孩子 明白 的 方式\n",
      "\n",
      "Target2:= This one was taken just weeks after 9 / 11 , and I found myself trying to explain what had happened that day in ways a five-year-old could understand  . \n",
      "Predict2:< This is a a , , , , , , , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 21s (- -54m 40s), Epoch: [4/10], Step: [1660/3335], Train Loss: 3.472547173500061, BLEU: 0.5406927344312414\n",
      "\n",
      "Input1:> 一切 都 准备 好 了\n",
      "\n",
      "Target1:= So I set up everything  . \n",
      "Predict1:< It , ,  .  .  .\n",
      "\n",
      "Input2:> 我 想要 我们 成为 环境 革命 的 叛徒 流氓 流氓 园丁\n",
      "\n",
      "Target2:= So I want us all to become UNK renegades , UNK , gangsta gardeners  . \n",
      "Predict2:< I I to to to to the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 27s (- -54m 34s), Epoch: [4/10], Step: [1680/3335], Train Loss: 3.4836886405944822, BLEU: 0.5854372077437064\n",
      "\n",
      "Input1:> 停电 也 变得 越来 越来越 频繁 到 了 晚上 我 周围 的 一切 都 变得 漆黑 漆黑一片 一片 除了 中国 的 灯火 灯火通明 通明 只要 穿过 我家 旁 的 河 就是\n",
      "\n",
      "Target1:= Power outages also became more and more frequent , so everything around me was completely dark at night except for the sea of lights in China , just across the river from my home  . \n",
      "Predict1:< And was , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但 如果 你 在 看 一下 这个 西红柿 你 就 会 注意 到 当 光 在 西红柿 上 走过 过时 它 一直 在 闪耀 它 并 没有 有变 变暗\n",
      "\n",
      "Target2:= But if you look at this tomato one more time , you will notice , as the light washes over the tomato , it continues to glow  . It doesn t become dark  . \n",
      "Predict2:< But if you you you , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 32s (- -54m 29s), Epoch: [4/10], Step: [1700/3335], Train Loss: 3.4115664839744566, BLEU: 0.5323728652227206\n",
      "\n",
      "Input1:> 当时 我 并不知道 不知 知道 引诱 和 迷惑 受害 受害者 是 家庭 家庭暴力 暴力 关系 开始 的 第一 第一步 一步\n",
      "\n",
      "Target1:= I didn t know that the first stage in any domestic violence relationship is to seduce and charm the victim  . \n",
      "Predict1:< I I , , , , , , , , , , , , , and , and\n",
      "\n",
      "Input2:> 他们 说 这个 真的 很 荒谬 你们 不能 叫 他 会议 主持\n",
      "\n",
      "Target2:= They said , that is ridiculous , you can t call him President  . \n",
      "Predict2:< And said , &quot; , , , , , , , &quot; ,  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 38s (- -54m 23s), Epoch: [4/10], Step: [1720/3335], Train Loss: 3.3055467128753664, BLEU: 0.53178872403723\n",
      "\n",
      "Input1:> 最重 重要 的 是 热情\n",
      "\n",
      "Target1:= The most important thing is passion  . \n",
      "Predict1:< The is is is is  .\n",
      "\n",
      "Input2:> 我们 正 努力 与 当地 的 居民 们 沟通\n",
      "\n",
      "Target2:= We are working with local communities  . \n",
      "Predict2:< We are we to the the  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 43s (- -54m 18s), Epoch: [4/10], Step: [1740/3335], Train Loss: 3.6405311942100527, BLEU: 0.5445401760440971\n",
      "\n",
      "Input1:> 在 50 UNK 摄氏 摄氏度 的 高温 中 男人 女人 小孩 事实 事实上 是 一整 整个 家庭 在 厚厚 厚厚的 灰尘 笼罩 下 机械 地 把 砖块 往 自己 的 头顶 顶上 上垒 每次 叠 上 18 块\n",
      "\n",
      "Target1:= UNK in temperatures of 130 degrees , men , women , children , entire families in fact , were cloaked in a heavy blanket of dust , while mechanically stacking bricks on their head , up to 18 \n",
      "Predict1:< In the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 我 翻看 那些 照片 片时 有些 甚至 有 逾 百年 的 历史 有些 仍 被 保存 存在 加工 实验 实验室 的 信封 封里 我 不禁 从 一个 修 图 师 的 角度 思考 或许 我 可以 帮\n",
      "\n",
      "Target2:= As I looked through the photos , there were some were over a hundred years old , some still in the envelope from the processing lab , I couldn t help but think as a UNK that I \n",
      "Predict2:< And I I to to a a , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 49s (- -54m 12s), Epoch: [4/10], Step: [1760/3335], Train Loss: 3.372882914543152, BLEU: 0.5282271649806163\n",
      "\n",
      "Input1:> 直至 现在 我 才 意识 到 丢失 了 这些 照片 肯定 是 这些 人 失去 的 一块 重大 大部 大部分 部分\n",
      "\n",
      "Target1:= Now , it wasn t until this point that I realized that these photos were such a huge part of the personal loss these people had felt  . \n",
      "Predict1:< Now , I I , I , , , , , , , , , the the the the\n",
      "\n",
      "Input2:> 今天 我 只要 要求 一件 事\n",
      "\n",
      "Target2:= Today I have just one request  . \n",
      "Predict2:< I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 53m 54s (- -54m 6s), Epoch: [4/10], Step: [1780/3335], Train Loss: 3.3078127980232237, BLEU: 0.5465637355270335\n",
      "\n",
      "Input1:> 但是 现在 你 知道 现在 有 多少 个 国家 有 总统 吗\n",
      "\n",
      "Target1:= But now , do you know how many nations have a president  ? \n",
      "Predict1:< But , you you you you you you you you  .\n",
      "\n",
      "Input2:> 你 可以 猜到 事情 是 怎么 发展 的\n",
      "\n",
      "Target2:= Well you can see where this is going  . \n",
      "Predict2:< You can can you you the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 0s (- -54m 1s), Epoch: [4/10], Step: [1800/3335], Train Loss: 3.394757568836212, BLEU: 0.5690190635387021\n",
      "\n",
      "Input1:> 现在 我 的 方法 已经 被 肯尼 肯尼亚 尼亚 的 所有 所有人 有人 使用 来 驱赶 其他 类似 似的 捕食 动物 像 鬣狗 美洲 美洲豹 等 它们 同样 被 用来 驱赶 大象 远离 离人 人类 的 农庄\n",
      "\n",
      "Target1:= And my idea is also being used now all over Kenya for scaring other predators like hyenas , leopards , and it is also being used to scare elephants away from people is farms  . \n",
      "Predict1:< Now , I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 现在 如果 我 用 一个 激光 光笔 并且 在 万亿 亿分 之一 一秒 内 开关 一次 也 就是 几 飞秒 我 就 会 制造 一些 光子 这些 光子 仅仅 是 几毫米 毫米 宽 并且 这些 光子 像 子弹\n",
      "\n",
      "Target2:= Now if I take a laser pointer and turn it on and off in one UNK of a second -- which is several femtoseconds -- I ll create a packet of photons barely a millimeter wide , and \n",
      "Predict2:< Now if I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 5s (- -55m 55s), Epoch: [4/10], Step: [1820/3335], Train Loss: 3.4305913925170897, BLEU: 0.5095205074761068\n",
      "\n",
      "Input1:> 所以 这 也 意味 意味着 人们 能 笑 人们 能用 手机 打进 你们 的 电话 然后 做 这项 测试 帕金森 帕金森病 病人 也 能 打通 通电 通电话 电话 然后 后记 记录 录下 声音 之后 他们 的 医生 生就\n",
      "\n",
      "Target1:= So what that means is that people will be able to — People will be able to call in from their mobile phones and do this test , and people with Parkinson is could call in , record \n",
      "Predict1:< So so is is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 另外 当 你们 描述 科学 原理 时 要 注意 术语 的 使用\n",
      "\n",
      "Target2:= And when you are describing your science , beware of jargon  . \n",
      "Predict2:< And , you you you you you you you the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 11s (- -55m 50s), Epoch: [4/10], Step: [1840/3335], Train Loss: 3.3741452217102053, BLEU: 0.5203206074587583\n",
      "\n",
      "Input1:> 开车 车行 行驶 在 加纳 的 一条 路上 与 Free the UNK 的 伙伴 一起 一个 骑 着 UNK 单车 的 同行 同行人 行人 突然 加速 靠近 我们 的 车 敲 着 车窗\n",
      "\n",
      "Target1:= Driving down a road in Ghana with partners of Free the Slaves , a fellow UNK on a UNK suddenly sped up to our UNK and tapped on the window  . \n",
      "Predict1:< And the the , , , , , , , , , , , , , , , , , , , , the the the the\n",
      "\n",
      "Input2:> 我 想要 种 一整 整个 街区 的 菜园 在 这个 街区 人们 能够 分享 食物\n",
      "\n",
      "Target2:= I want to plant a whole block of gardens where people can share in the food in the same block  . \n",
      "Predict2:< I I to to to the the the the , , the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 16s (- -55m 44s), Epoch: [4/10], Step: [1860/3335], Train Loss: 3.4968255877494814, BLEU: 0.48635909227106405\n",
      "\n",
      "Input1:> 由于 那里 里面 的 闷热 和 尘土 我 的 相机 烫得 没法 拿 也 无法 运转\n",
      "\n",
      "Target1:= So pervasive was the heat and the dust that my camera became too hot to even touch and ceased working  . \n",
      "Predict1:< And the , , , , , , , , and and and and and\n",
      "\n",
      "Input2:> 所以 我们 可以 通过 国际 社会 的 帮助 来 接受 受教 教育 英语 培训 工作 培训 等等\n",
      "\n",
      "Target2:= So we can benefit from the international community for education , English language training , job training , and more  . \n",
      "Predict2:< So we can can to to the , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 22s (- -55m 39s), Epoch: [4/10], Step: [1880/3335], Train Loss: 3.491457724571228, BLEU: 0.5435881027468812\n",
      "\n",
      "Input1:> 其中 中大 大部 大部分 部分 最终 回到 了 失主 手里 这 一点 也 异常 重要\n",
      "\n",
      "Target1:= a large majority of which did actually find their home again , importantly  . \n",
      "Predict1:< The the the of , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 我 甚至 将 我 的 朋友 带回 我 的 社区 帮助 他们 在 没有 赶 狮 灯 的 人家 家里 里安 安装 这个 我 教 他们 怎么 安装\n",
      "\n",
      "Target2:= I even took my friends back to my community , and we are installing the lights to the homes which don t have &#91; any &#93; , and I am teaching them how to put them  . \n",
      "Predict2:< I I I I to I , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 28s (- -55m 33s), Epoch: [4/10], Step: [1900/3335], Train Loss: 3.4007982134819033, BLEU: 0.4420099089921365\n",
      "\n",
      "Input1:> 秃鹫 还 曾经 被 迪斯尼 看上 像 高飞 狗 一样 被 人格 人格化 变成 了 麻木 的 愚蠢 的 动画 UNK 角色\n",
      "\n",
      "Target1:= They ve also be associated with Disney — — personified as goofy , dumb , stupid characters  . \n",
      "Predict1:< The is a , , , , , , , , , , the the\n",
      "\n",
      "Input2:> 但 我们 还是 会 得到 大量 来自 不同 环境 下 的 采样 数据 而且 重点 是 不同 的 环境 因为 这样 我们 才能 消除 除掉 那些 混淆 的 因素 寻求 这种 疾病 真正 的 标记\n",
      "\n",
      "Target2:= But we ll get a very large sample of data that is collected from all different circumstances , and it is getting it in different circumstances that matter because then we are looking at ironing out the confounding \n",
      "Predict2:< But we we we to to the , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 33s (- -55m 28s), Epoch: [4/10], Step: [1920/3335], Train Loss: 3.4522140979766847, BLEU: 0.5631242274727395\n",
      "\n",
      "Input1:> 这 是 沥青 就 像 我 说 的 石头 头中 中间 有 少量 的 粘合 粘合剂 合剂\n",
      "\n",
      "Target1:= It is a porous asphalt , like I said , so you have only a small amount of binder between the stones  . \n",
      "Predict1:< This is is a , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 朝鲜 朝鲜政府 政府 拦截 了 我 寄回 回家 的 部分 分钱 钱款 作为 惩罚 我 的 家人 要 被 强制 搬走 搬到 农村 的 一个 偏僻 的 地方 去\n",
      "\n",
      "Target2:= The North Korean authorities intercepted some money that I sent to my family , and , as a punishment , my family was going to be forcibly removed to a desolate location in the countryside  . \n",
      "Predict2:< And I , I , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 39s (- -55m 22s), Epoch: [4/10], Step: [1940/3335], Train Loss: 3.403877246379852, BLEU: 0.5265642079572187\n",
      "\n",
      "Input1:> 而是 由 一群 专业 专业人士 人士 制出 的 图象 发型 发型师 化装 UNK 摄影 摄影师 造型 UNK 以及 他们 的 助理 还有 前期 制作 后期 后期制作 制作 他们 制造 造出 了 这些 图象 而 不是 我\n",
      "\n",
      "Target1:= They are constructions , and they are constructions by a group of professionals , by UNK and makeup artists and photographers and stylists and all of their assistants and UNK and post-production , and they build this  . \n",
      "Predict1:< It is the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 记得 得当 当时 站 在 几乎 是 一片 黑暗 当中 我 刹那 刹那间 那间 感到 一阵 强烈 的 恐惧 那一刻 一刻 我 在 想 被困 在 这个 地狱 里 是 多么 的 恐怖\n",
      "\n",
      "Target2:= Standing in the near darkness , I remember feeling this quick , hot fear , and in that instant , I could only imagine what it must be like to be trapped in that hell  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 44s (- -55m 17s), Epoch: [4/10], Step: [1960/3335], Train Loss: 3.539729654788971, BLEU: 0.5183778503212477\n",
      "\n",
      "Input1:> 我们 都 知道 自己 正 冒 着 生命 的 危险 老师 学生 以及 家长 都 很 明白\n",
      "\n",
      "Target1:= We all knew we were risking our lives -- the teacher , the students and our parents  . \n",
      "Predict1:< We know know know of , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 就是 这样\n",
      "\n",
      "Target2:= Absolutely  . \n",
      "Predict2:< That is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 50s (- -55m 11s), Epoch: [4/10], Step: [1980/3335], Train Loss: 3.4245808362960815, BLEU: 0.5442621837395509\n",
      "\n",
      "Input1:> 错 他们 都 是 奴隶\n",
      "\n",
      "Target1:= Wrong  . They were all enslaved  . \n",
      "Predict1:< And they they they  .  .\n",
      "\n",
      "Input2:> 几千 几千人 成为 农奴 餐馆 UNK 家庭 UNK 还 不止 这些\n",
      "\n",
      "Target2:= Tens of hundreds of people are enslaved in agriculture , in restaurants , in domestic servitude , and the list can go on  . \n",
      "Predict2:< The is : , , , to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54m 55s (- -55m 5s), Epoch: [4/10], Step: [2000/3335], Train Loss: 3.5143196821212768, BLEU: 0.542459991555854\n",
      "\n",
      "Input1:> 据 保守 估计 当今 全世界 世界 有 超过 2700 万 人 被 奴役\n",
      "\n",
      "Target1:= A conservative estimate tells us there are more than 27 million people enslaved in the world today  . \n",
      "Predict1:< The of , , , , , , , the  .  .\n",
      "\n",
      "Input2:> 它 拥有 26 平方 平方公里 方公里 公里 的 空地\n",
      "\n",
      "Target2:= They own 26 square miles of vacant lots  . \n",
      "Predict2:< It is a the of ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 1s (- -55m 0s), Epoch: [4/10], Step: [2020/3335], Train Loss: 3.4482205271720887, BLEU: 0.56362320634103\n",
      "\n",
      "Input1:> 我 希望 这些 影像 能够 在 你们 这样 的 观者 心中 唤起 一股 力量 我 希望 这股 力量 的 星星 星星之火 能够 成为 被 奴役 者 的 希望 之 光 因为 若 没有 这 光亮 奴役 之 兽\n",
      "\n",
      "Target1:= I hope that these images awaken a force in those who view them , people like you , and I hope that force will ignite a fire , and that fire will shine a light on slavery , \n",
      "Predict1:< I I I to you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 无法 否认 家里 的确 确有 过 艰难 的 时候 我 多 想 他们 跟 我 一样\n",
      "\n",
      "Target2:= Now , I cannot deny that there have been some challenging moments in my family , moments where I ve wished that they were just like me  . \n",
      "Predict2:< I I t I I I I I I I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 6s (- -56m 54s), Epoch: [4/10], Step: [2040/3335], Train Loss: 3.2858118176460267, BLEU: 0.5364394590082021\n",
      "\n",
      "Input1:> 这 是 一个 意外 而 他 以后 不会 再 伤害 我\n",
      "\n",
      "Target1:= It was an isolated incident , and he was never going to hurt me again  . \n",
      "Predict1:< It is a a , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 甚至 几个 月\n",
      "\n",
      "Target2:= It could take months  . \n",
      "Predict2:< It is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 12s (- -56m 49s), Epoch: [4/10], Step: [2060/3335], Train Loss: 3.449970233440399, BLEU: 0.5513832182225276\n",
      "\n",
      "Input1:> 事实 事实上 这项 实验 得到 的 结论 是 如果 我们 每 四年 在 这段 路上 用 我们 的 加热 热机 机器 超大 大号 微波 微波炉 我们 要 把 这个 机器 搬到 公路 路上 用 我们 每 四年 上\n",
      "\n",
      "Target1:= So actually , the conclusion from this research is that if we go on the road every four years with our healing machine -- this is the big version we have made to go on the real road \n",
      "Predict1:< In fact , , , , , , , we we we we , we , we , we , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 必须 得到 诊所 去 而且 花费 相当 相当 大 这 也 意味 意味着 在 临床 临床试验 试验 之外 从来 从来没 没人 做 过 这个 测试 从来 从来没 没有\n",
      "\n",
      "Target2:= You have to go to the clinic to do it  . It is very , very costly , and that means that , outside the clinical trials , it is just never done  . It is never done \n",
      "Predict2:< You you t to , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 17s (- -56m 43s), Epoch: [4/10], Step: [2080/3335], Train Loss: 3.388121175765991, BLEU: 0.5796201003034395\n",
      "\n",
      "Input1:> 回到 UNK 我 忍不住 不住 想 哭 但是 身旁 的 废 奴 主义 同伴 迅速 地拉 拉住 了 我 他 说 Lisa 别哭 别 在 这里 哭\n",
      "\n",
      "Target1:= Back in the kilns , I wanted to cry , but the UNK next to me quickly grabbed me and he said , &quot; Lisa , don t do that  . Just don t do that here  . \n",
      "Predict1:< I I to to to , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 用 这个 技术 我们 可以 制造 能 看到 光路 拐角 的 照相 照相机 相机 能 看到 视野 之外 或者 不用 X 光 就 能 透视 我们 身体 的 照相 照相机 相机 这种 相机 挑战 了 我们 关于\n",
      "\n",
      "Target2:= And with that , we can create cameras that can look around corners , beyond line of sight or see inside our body without an X-ray , and really challenge what we mean by a camera  . \n",
      "Predict2:< So so can can we we can can we can can can , we can , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 23s (- -56m 38s), Epoch: [4/10], Step: [2100/3335], Train Loss: 3.348788821697235, BLEU: 0.5800050366093507\n",
      "\n",
      "Input1:> 所以 我们 带 着意 意大利 大利 的 种子 来到 了 赞比亚 比亚 南部 一个 壮丽 迷人 的 山谷 通向 UNK 比西 西河 我们 教 当地 当地人 人种 种植 意大利 大利 利西 西红柿 和 UNK 葫芦 等等\n",
      "\n",
      "Target1:= So we arrived there with Italian seeds in southern Zambia in this absolutely magnificent valley going down to the UNK River , and we taught the local people how to grow Italian tomatoes and UNK and  . . . \n",
      "Predict1:< So we we we , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> UNK 知道 什么 是 爱\n",
      "\n",
      "Target2:= Remi knows what love is  . \n",
      "Predict2:< What know , know is is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 28s (- -56m 32s), Epoch: [4/10], Step: [2120/3335], Train Loss: 3.365761947631836, BLEU: 0.5183555529932564\n",
      "\n",
      "Input1:> 而 如今 我 已经 是 一名 自豪 的 明德 学院 的 毕业 毕业生 业生\n",
      "\n",
      "Target1:= Instead , I stand here a proud graduate of UNK College  . \n",
      "Predict1:< And I I my my my  .  .  .\n",
      "\n",
      "Input2:> 他 出身 阿富汗 边远 边远地区 地区 有着 与 他人 不同 的 见解 他 坚持 让 他 的 女儿 我 的 母亲 去 上学 并 因此 被迫 与 他 的 父亲 断绝 父子 父子关系 子关 关系\n",
      "\n",
      "Target2:= A total maverick from a remote province of Afghanistan , he insisted that his daughter , my mom , go to school , and for that he was UNK by his father  . \n",
      "Predict2:< He he to to to to , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 34s (- -56m 27s), Epoch: [4/10], Step: [2140/3335], Train Loss: 3.4077020049095155, BLEU: 0.5695699670488209\n",
      "\n",
      "Input1:> 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "\n",
      "Target1:= It was the only way we both could be educated  . \n",
      "Predict1:< And is is is we we we we we  .  .\n",
      "\n",
      "Input2:> 我 只是 一个 有着 亲身 亲身经历 身经 经历 要 讲述 的 女人\n",
      "\n",
      "Target2:= I am just one woman with a story to tell  . \n",
      "Predict2:< I I a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 39s (- -56m 21s), Epoch: [4/10], Step: [2160/3335], Train Loss: 3.466093122959137, BLEU: 0.43860517574491564\n",
      "\n",
      "Input1:> 我们 还给 她 复原 照片 的 那天 又 恰好 是 她 小儿 小儿子 儿子 的 14 岁 生日\n",
      "\n",
      "Target1:= The day I gave her the photos also happened to be her youngest son is 14th birthday  . \n",
      "Predict1:< And we we a , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 他 说 小时 小时候 时候 的 精神 精神创伤 创伤 使得 他 需要 拥有 这些 枪\n",
      "\n",
      "Target2:= And he said that he needed those guns because of the trauma he d experienced as a young boy  . \n",
      "Predict2:< He he , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 45s (- -56m 16s), Epoch: [4/10], Step: [2180/3335], Train Loss: 3.3919060230255127, BLEU: 0.5413628055296726\n",
      "\n",
      "Input1:> 绿化 绿化带 归 政府 所有\n",
      "\n",
      "Target1:= Thing is , it is owned by the city  . \n",
      "Predict1:< It is is  .  .  .\n",
      "\n",
      "Input2:> 声音 颤动 虚弱 和 僵化\n",
      "\n",
      "Target2:= We see vocal tremor , weakness and rigidity  . \n",
      "Predict2:< The : :  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 50s (- -56m 10s), Epoch: [4/10], Step: [2200/3335], Train Loss: 3.3348267912864684, BLEU: 0.521508417118993\n",
      "\n",
      "Input1:> 它 是 沥青 做 的 沥青 是 一种 很 好 的 材料 在 上面 开车 很 好 但 也 不 总是 特别 是 像 今天 这样 的 天气 下 大雨 的 时候\n",
      "\n",
      "Target1:= It is made of asphalt , and asphalt is a very nice material to drive on , but not always , especially not on these days as today , when it is raining a lot  . \n",
      "Predict1:< It is is a is , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 而且 和 芭蕾 一样 这 也 需要 经过 特殊 地 训练\n",
      "\n",
      "Target2:= And like ballet , it takes an extraordinary level of training  . \n",
      "Predict2:< And it is , and , and and and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 55m 56s (- -56m 5s), Epoch: [4/10], Step: [2220/3335], Train Loss: 3.435918951034546, BLEU: 0.5905717018782777\n",
      "\n",
      "Input1:> 不到 一个 一个月 月前 他 和 他 女儿 在 从 学校 回 村里 的 路上 他们 差一点 一点 就 被 路边 炸弹 炸死\n",
      "\n",
      "Target1:= Less than a month ago , he and his daughter were on their way from UNK to their village , and they literally missed being killed by a roadside bomb by minutes  . \n",
      "Predict1:< And he a a , , , , , , , , , , and , and and and and and and and\n",
      "\n",
      "Input2:> 正好 相反 我 是 一个 很 强硬 的 女性 深爱 深爱着 这个 饱受 受困 困扰 的 男人 而且 我 是 这 世上 唯一 唯一一 一一 一个 可以 帮助 康 纳 面对 自己 心 魔 的 人\n",
      "\n",
      "Target2:= Instead , I was a very strong woman in love with a deeply troubled man , and I was the only person on Earth who could help UNK face his demons  . \n",
      "Predict2:< And , , , I a , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 1s (- -57m 59s), Epoch: [4/10], Step: [2240/3335], Train Loss: 3.430356514453888, BLEU: 0.5875400752341471\n",
      "\n",
      "Input1:> 尽管 适应 韩国 的 生活 并 不是 易事 但是 我 还是 决定\n",
      "\n",
      "Target1:= Even though adjusting to life in South Korea was not easy , I made a plan  . \n",
      "Predict1:< And , , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 不过 随着 对话 的 展开 我 的 经历 就 如 爱丽丝 一样 顺着 那个 兔子 洞 下去 看到 了 那 扇 通往 一个 全新 新世界 世界 的 大门\n",
      "\n",
      "Target2:= But as these conversations unfolded , I experienced what Alice must have when she went down that rabbit hole and saw that door to a whole new world  . \n",
      "Predict2:< But the the , , , , , , , , , , , , the the , the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 7s (- -57m 54s), Epoch: [4/10], Step: [2260/3335], Train Loss: 3.4795674324035644, BLEU: 0.5266537535240705\n",
      "\n",
      "Input1:> 这个 是 健康 状态 下 的 某人 在 发声 我们 把 自己 想 当作 是 声音 的 芭蕾 芭蕾舞 芭蕾舞者 舞者 因为 当 我们 发声 时 我们 得 去 协调 这些 发声 发声器 发声器官 器官 确实 这些 是\n",
      "\n",
      "Target1:= Now , this is healthy and this is somebody making speech sounds , and we can think of ourselves as vocal ballet dancers , because we have to coordinate all of these vocal organs when we make sounds \n",
      "Predict1:< And is is is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 不是 的\n",
      "\n",
      "Target2:= Not exactly  . \n",
      "Predict2:< It is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 12s (- -57m 48s), Epoch: [4/10], Step: [2280/3335], Train Loss: 3.333921563625336, BLEU: 0.5422349977821505\n",
      "\n",
      "Input1:> 她 两年 年前 退休 了 结果 却 把 我家 变成 了 一所 所学 学校 教育 UNK 社区 中 的 女孩 和 妇女\n",
      "\n",
      "Target1:= She retired two years ago , only to turn our house into a school for girls and women in our neighborhood  . \n",
      "Predict1:< She was the the , , , , and , and and and and and and and and  .\n",
      "\n",
      "Input2:> 所以 掌声 所以 政府 的 人 说 那 接着 做\n",
      "\n",
      "Target2:= So — — So the government says , &quot; Do it again  . &quot; \n",
      "Predict2:< So , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 18s (- -57m 42s), Epoch: [4/10], Step: [2300/3335], Train Loss: 3.4949191331863405, BLEU: 0.5587517519602044\n",
      "\n",
      "Input1:> 当 摄影 摄影师 要 我 弓起 背 摸 那个 男士 的 头发 时 我 非常 不自 不自在 自在\n",
      "\n",
      "Target1:= I was totally uncomfortable , and the photographer was telling me to arch my back and put my hand in that guy is hair  . \n",
      "Predict1:< When I I I , , , , , , , , I , I I  .\n",
      "\n",
      "Input2:> 我们 需要 去 和 整个 地区 的 政府 们 打交道 交道\n",
      "\n",
      "Target2:= We need to work with governments in regional levels  . \n",
      "Predict2:< We need to to to to and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 24s (- -57m 37s), Epoch: [4/10], Step: [2320/3335], Train Loss: 3.3929832935333253, BLEU: 0.5637808188137498\n",
      "\n",
      "Input1:> 因为 在家 家庭 家庭暴力 暴力 中 的 最后 一步 就是 杀掉 她\n",
      "\n",
      "Target1:= Because the final step in the domestic violence pattern is kill her  . \n",
      "Predict1:< Because she is , , , , , the  .  .\n",
      "\n",
      "Input2:> 1860 年 有 一批 专家 受到 邀请 去 讨论 纽约 纽约市 的 未来 是 怎样\n",
      "\n",
      "Target2:= There was a group of experts who were invited to discuss the future of the city of New York in 1860  . \n",
      "Predict2:< And is , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 29s (- -57m 31s), Epoch: [4/10], Step: [2340/3335], Train Loss: 3.4931127429008484, BLEU: 0.5363954570452962\n",
      "\n",
      "Input1:> 我 生活 在 洛杉矶 UNK\n",
      "\n",
      "Target1:= I live in South Central  . \n",
      "Predict1:< I I a in  .  .\n",
      "\n",
      "Input2:> 90 年代 中期 朝鲜 发生 了 一次 严重 的 饥荒\n",
      "\n",
      "Target2:= A huge famine hit North Korea in the mid-1990s  . \n",
      "Predict2:< The is , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 35s (- -57m 26s), Epoch: [4/10], Step: [2360/3335], Train Loss: 3.561202335357666, BLEU: 0.5523700743145262\n",
      "\n",
      "Input1:> 请 不要 要说 我 正常\n",
      "\n",
      "Target1:= Please -- don t tell me I am normal  . \n",
      "Predict1:< I I t I I  .  .\n",
      "\n",
      "Input2:> 在 喜马拉 喜马拉雅 马拉 马拉雅 我 见到 了 做 苦力 的 小孩 背着 石头 走上 几公里 公里 的 山路 到 路边 等 着 的 卡车 旁\n",
      "\n",
      "Target2:= In the Himalayas , I found children carrying stone for miles down UNK terrain to trucks waiting at roads below  . \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 40s (- -57m 20s), Epoch: [4/10], Step: [2380/3335], Train Loss: 3.3518301367759706, BLEU: 0.5857075253932615\n",
      "\n",
      "Input1:> 从 他 记事 开始 就 被迫 迫在 湖上 上工 工作\n",
      "\n",
      "Target1:= For as long as he can recall , he is been forced to work on the lake  . \n",
      "Predict1:< And he he to the the the the  .  .\n",
      "\n",
      "Input2:> 由于 风化 紫外 紫外线 外线 氧化 等 原因 这种 粘合 粘合剂 合剂 这种 沥青 集料 之间 的 黏胶 会 收缩 如果 它 收缩 就 会 有 小 裂纹 从 集 块 开始 会 出现 分层\n",
      "\n",
      "Target2:= Due to UNK , due to UNK light , due to oxidation , this binder , this bitumen , the glue between the aggregates is going to shrink , and if it shrinks , it gets UNK , \n",
      "Predict2:< And the the , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 46s (- -57m 15s), Epoch: [4/10], Step: [2400/3335], Train Loss: 3.5473344683647157, BLEU: 0.5496378320423152\n",
      "\n",
      "Input1:> 不仅 因为 它 听起 起来 很 不错\n",
      "\n",
      "Target1:= Just because it sounds so good  . \n",
      "Predict1:< It it not not it it it  .  .\n",
      "\n",
      "Input2:> 我们 鼓舞 各个 社区 进行 创业\n",
      "\n",
      "Target2:= We activate communities  . \n",
      "Predict2:< We are we to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 51s (- -57m 9s), Epoch: [4/10], Step: [2420/3335], Train Loss: 3.3808329820632936, BLEU: 0.5828994930696652\n",
      "\n",
      "Input1:> 一切 都 准备 好 了\n",
      "\n",
      "Target1:= So I set up everything  . \n",
      "Predict1:< And ,  .  .  .\n",
      "\n",
      "Input2:> 社区 会议 存在 一个 问题\n",
      "\n",
      "Target2:= There is a problem with community meetings  . \n",
      "Predict2:< There is a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 56m 57s (- -57m 4s), Epoch: [4/10], Step: [2440/3335], Train Loss: 3.4906386852264406, BLEU: 0.558767541782006\n",
      "\n",
      "Input1:> 噪音 也 会 消失 在 这些 毛孔 里\n",
      "\n",
      "Target1:= Also the noise will disappear in these pores  . \n",
      "Predict1:< And is are the the  .  .  .\n",
      "\n",
      "Input2:> 我 这个 年龄 的 女性 中 高中 高中毕业 毕业 的 人数 数不到 不到 6 如果 不是 我家 家人 的 坚持 我 也 会 像 绝大 绝大多数 大多 大多数 多数 数人 一样\n",
      "\n",
      "Target2:= Fewer than six percent of women my age have made it beyond high school , and had my family not been so committed to my education , I would be one of them  . \n",
      "Predict2:< I I t , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 2s (- -58m 58s), Epoch: [4/10], Step: [2460/3335], Train Loss: 3.469350516796112, BLEU: 0.5537516805282452\n",
      "\n",
      "Input1:> 当 我 还是 个 孩子 的 时候 我 认为 我 的 国家 是 世界 上 最好 的 我 大 一点 后学 学会 了 一首 叫做 无可 羡慕 的 歌\n",
      "\n",
      "Target1:= When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy  . &quot; \n",
      "Predict1:< When I I I I I I I , I , I , I , I , , I I , , I I I I\n",
      "\n",
      "Input2:> 奈 库 贝特 女神 在 古埃及 埃及 埃及文 文化 中 象征 着 监护 监护人 护人 和 母性 通常 和 眼镜 眼镜蛇 女神 维 阿 杰 特 一起 出现 象征 着上 埃及 和 下 埃及 的 团结 和 统一\n",
      "\n",
      "Target2:= UNK was the symbol of the protector and the motherhood , and together with the cobra , symbolized the unity between Upper and Lower Egypt  . \n",
      "Predict2:< The , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 8s (- -58m 53s), Epoch: [4/10], Step: [2480/3335], Train Loss: 3.378352952003479, BLEU: 0.5818534643989433\n",
      "\n",
      "Input1:> 我 的 手 滑 了 一下 立即 想到 数天 前 碰到 的 一名 矿工 他 一手 UNK 就 掉 下 了 无底 深井\n",
      "\n",
      "Target1:= When my hand slips , I suddenly remember a miner I had met days before who had lost his grip and fell countless feet down that shaft  . \n",
      "Predict1:< I was the the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 还 不算 毁坏 得 太严 严重 但是 被 水浸 浸没 的 地方 导致 照片 片中 中小 小女 小女孩 女孩 的 脸 UNK 了 我们 不得 不得不 精心 修复 做到 十分 精确\n",
      "\n",
      "Target2:= Not terribly damaged , but where the water had caused that UNK on the girl is face had to be repaired with such accuracy and delicacy  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 13s (- -58m 47s), Epoch: [4/10], Step: [2500/3335], Train Loss: 3.6331146717071534, BLEU: 0.5896258755204651\n",
      "\n",
      "Input1:> 每 一个 病例 的 情况 都 是 特殊 的 所以 UNK 与 Sam 的 情况 也 是 不同 的\n",
      "\n",
      "Target1:= It manifests in each individual differently , hence why Remi is so different from Sam  . \n",
      "Predict1:< The the is of of the , , , , , the the  .\n",
      "\n",
      "Input2:> 这个 指示 指示器 又 会 使 光线 闪动\n",
      "\n",
      "Target2:= And the indicator box makes the lights flash  . \n",
      "Predict2:< It is is a , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 19s (- -58m 42s), Epoch: [4/10], Step: [2520/3335], Train Loss: 3.497339153289795, BLEU: 0.631583751943667\n",
      "\n",
      "Input1:> 否则 照片 里 她 和 她 先生 的 脸部 就 无法 完全 修复 了 我们 也 因此 得以 把 所有 部分 整合 合到 一张 好 的 相片 里 然后 重新 新制 制作 这 张 照片\n",
      "\n",
      "Target1:= Areas like her husband and her face , which otherwise would have been completely impossible to fix , we could just put them together in one good photo , and remake the whole photo  . \n",
      "Predict1:< And she , , , , , , , , and , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 在 塔利 塔利班 控制 下 去 上学 的 女孩 曾经 只有 几百 几百人 百人 请 不要 忘 了 当时 是非 非法 的\n",
      "\n",
      "Target2:= Under the Taliban , girls who went to school numbered in the hundreds -- remember , it was illegal  . \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 24s (- -58m 36s), Epoch: [4/10], Step: [2540/3335], Train Loss: 3.4849547386169433, BLEU: 0.6209932122712345\n",
      "\n",
      "Input1:> 我 第一 第一次 一次 有 了 自己 的 房子 公寓 有 了 第一 第一张 一张 信用 信用卡 我 还有 一个 非常 大 的 秘密\n",
      "\n",
      "Target1:= I had my first apartment , my first little green American Express card , and I had a very big secret  . \n",
      "Predict1:< I was a a , , , , , , , , , , , , I a\n",
      "\n",
      "Input2:> 所以 我 又 有 了 个 主意\n",
      "\n",
      "Target2:= So I had an idea  . \n",
      "Predict2:< So I I a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 30s (- -58m 31s), Epoch: [4/10], Step: [2560/3335], Train Loss: 3.4277180194854737, BLEU: 0.5934851194279965\n",
      "\n",
      "Input1:> 我们 清理 了 大量 的 发臭 腐烂 的 鱼 尸骨 那些 鱼 都 是 当地 工厂 厂里 里加 加工 处理 的 鱼\n",
      "\n",
      "Target1:= We cleared tons and tons of stinking , rotting fish carcasses from the local fish processing plant  . \n",
      "Predict1:< We we a a of , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 确实 实相 相信 当 我们 把 他们 当做 同胞 就 会 很 难容 容忍 奴役 这样 的 暴行\n",
      "\n",
      "Target2:= I truly believe , if we can see one another as fellow human beings , then it becomes very difficult to tolerate atrocities like slavery  . \n",
      "Predict2:< I I I that that , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 35s (- -58m 25s), Epoch: [4/10], Step: [2580/3335], Train Loss: 3.354875552654266, BLEU: 0.6125198675757999\n",
      "\n",
      "Input1:> 他们 甚至 并 没有 延续 君主 君主制 他们 的 主意 是 你 可以 以是 选举 的 国王 有 一个 固定 的 任期\n",
      "\n",
      "Target1:= And they weren t even being UNK there , they had the idea that you could be elected King for a fixed term  . \n",
      "Predict1:< They they t a , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我 最后 爬出 了 矿井 得以 回家 而 这些 被 奴役 的 矿工 可能 永远 没有 回家 的 那 一天\n",
      "\n",
      "Target2:= I got to climb out of that hole , and I got to go home , but they likely never will , because they are trapped in slavery  . \n",
      "Predict2:< I I t the , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 41s (- -58m 20s), Epoch: [4/10], Step: [2600/3335], Train Loss: 3.3459847450256346, BLEU: 0.6443884616889636\n",
      "\n",
      "Input1:> 塔利 塔利班 走 了 父亲 大声 叫 着\n",
      "\n",
      "Target1:= &quot; The Taliban are gone  ! &quot; my father shouted  . \n",
      "Predict1:< The : : ,  .  .  .  .\n",
      "\n",
      "Input2:> 你 能 销售 售出 出去 吗 你 能 打理 资金 吗\n",
      "\n",
      "Target2:= Okay , can you sell it  ? Can you look after the money  ? &quot; \n",
      "Predict2:< You you you you ,  ?  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 46s (- -58m 14s), Epoch: [4/10], Step: [2620/3335], Train Loss: 3.374389576911926, BLEU: 0.5570054938747828\n",
      "\n",
      "Input1:> 他 表现 现出 的 深深 的 UNK\n",
      "\n",
      "Target1:= And he was very , very sorry  . \n",
      "Predict1:< He is a the  .  .  .  .\n",
      "\n",
      "Input2:> 我们 一 到 那里 我 就 去 看 了 个 大概\n",
      "\n",
      "Target2:= When we first arrived , I went to have a quick look  . \n",
      "Predict2:< We ve we to a , , I ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 52s (- -58m 8s), Epoch: [4/10], Step: [2640/3335], Train Loss: 3.4273247838020326, BLEU: 0.5809114321224691\n",
      "\n",
      "Input1:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target1:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict1:< He he he , , , , , , , , , , , he  .\n",
      "\n",
      "Input2:> 我 当时 非常 害怕 感觉 我 的 心 都 快 因 恐惧 而 爆炸 了\n",
      "\n",
      "Target2:= I was so scared , I thought my heart was going to explode  . \n",
      "Predict2:< I I t I I , I , I I  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57m 57s (- -58m 3s), Epoch: [4/10], Step: [2660/3335], Train Loss: 3.5380445599555967, BLEU: 0.5923218687195326\n",
      "\n",
      "Input1:> 我们 也 有机 机会 开始 寻找 这种 疾病 在 病入膏肓 膏肓 前 的 早期 期生 生物 标志 标志物\n",
      "\n",
      "Target1:= We have the opportunity to start to search for the early biomarkers of the disease before it is too late  . \n",
      "Predict1:< We we we to to the , , , , , , , the  .  .\n",
      "\n",
      "Input2:> 在 路 的 尽头 他 催促 我们 下车 让 司机 赶紧 走\n",
      "\n",
      "Target2:= At the end of the road , he urged us out of the car , and told the driver to quickly leave  . \n",
      "Predict2:< And the , , , , , we we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 3s (- -59m 57s), Epoch: [4/10], Step: [2680/3335], Train Loss: 3.3994518160820006, BLEU: 0.6004314136380448\n",
      "\n",
      "Input1:> 我们 试图 去 查明 秃鹫 与 周围 周围环境 环境 的 关系 生态 生态学 并且 了解 秃鹫 的 行踪\n",
      "\n",
      "Target1:= We are trying to determine their basic ecology , and see where they go  . \n",
      "Predict1:< We are to to to to the and and and and and and the the\n",
      "\n",
      "Input2:> 他 说 小时 小时候 时候 的 精神 精神创伤 创伤 使得 他 需要 拥有 这些 枪\n",
      "\n",
      "Target2:= And he said that he needed those guns because of the trauma he d experienced as a young boy  . \n",
      "Predict2:< He he , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 9s (- -59m 52s), Epoch: [4/10], Step: [2700/3335], Train Loss: 3.480313754081726, BLEU: 0.6129657275846343\n",
      "\n",
      "Input1:> 我 根本 不知 知道 身为 一个 朝鲜 的 难民 我 未来 的 生活 活会 怎样 但是 我 很快 快意 意识 到 那 生活 不但 充满 了 艰辛 甚至 无比 危险 因为 朝鲜 难民 在 中国 被 视为 非法\n",
      "\n",
      "Target1:= I had no idea what life was going to be like as a North Korean refugee , but I soon learned it is not only extremely difficult , it is also very dangerous , since North Korean refugees \n",
      "Predict1:< I I t know , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 即使 他们 学会 了 中文 找到 了 工作 他们 的 整个 世界 也 会 在 顷刻 顷刻间 颠覆\n",
      "\n",
      "Target2:= Even after learning a new language and getting a job , their whole world can be turned upside down in an instant  . \n",
      "Predict2:< And they they they they they they they they they they they they they they\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 14s (- -59m 46s), Epoch: [4/10], Step: [2720/3335], Train Loss: 3.3214148044586183, BLEU: 0.5979253631689954\n",
      "\n",
      "Input1:> 我们 的 手工 清洗 活动 非常 顺利\n",
      "\n",
      "Target1:= Now , we were lucky with our UNK  . \n",
      "Predict1:< Our is we ,  .  .  .  .\n",
      "\n",
      "Input2:> 正如 我 的 队长 维 恩 曾 说 这 就 像 在 对 人 进行 纹身 一样\n",
      "\n",
      "Target2:= As my team leader UNK once said , it is like doing a tattoo on someone  . \n",
      "Predict2:< And , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 20s (- -59m 41s), Epoch: [4/10], Step: [2740/3335], Train Loss: 3.5148215293884277, BLEU: 0.5887468750402998\n",
      "\n",
      "Input1:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target1:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict1:< We can we the the the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 做 过 调查 对 世界 上 100 家 标杆 企业 进行 研究 卡内 卡内基 培训 公司 西屋 公司 爱迪生 公司 福特 福特汽车公司 汽车 公司 以及 所有 这些 新兴 新兴产业 产业 的 公司 如 谷 歌 雅虎\n",
      "\n",
      "Target2:= We ve done the research , and we have looked at the 100 iconic companies of the world -- Carnegie , Westinghouse , Edison , Ford , all the new companies , Google , Yahoo  . \n",
      "Predict2:< We ve we the the the the , , , , , , , , , , , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 25s (- -59m 35s), Epoch: [4/10], Step: [2760/3335], Train Loss: 3.467152988910675, BLEU: 0.5999279395318586\n",
      "\n",
      "Input1:> 图象 象是 强大 的 但 同时 又 是 表面 UNK 的\n",
      "\n",
      "Target1:= Image is powerful , but also image is superficial  . \n",
      "Predict1:< The the is is is is is is  .  .\n",
      "\n",
      "Input2:> 所以 1860 年 他们 认为 这种 肮脏 的 科技 将 扼杀 纽约 的 生命\n",
      "\n",
      "Target2:= So 1860 , they are seeing this dirty technology that is going to choke the life out of New York  . \n",
      "Predict2:< So , , , the the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 31s (- -59m 30s), Epoch: [4/10], Step: [2780/3335], Train Loss: 3.4338281750679016, BLEU: 0.6077503943444392\n",
      "\n",
      "Input1:> 我 当时 非常 震惊\n",
      "\n",
      "Target1:= I was so shocked  . \n",
      "Predict1:< I was was  .  .  .\n",
      "\n",
      "Input2:> 你 会 好奇 是否 会 有效\n",
      "\n",
      "Target2:= And now you are of course curious if it also worked  . \n",
      "Predict2:< You you you to it  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 36s (- -59m 24s), Epoch: [4/10], Step: [2800/3335], Train Loss: 3.4276928901672363, BLEU: 0.598542265848167\n",
      "\n",
      "Input1:> 警报 拉 响 的 时候 他 不得 不得不 冲向 向海 海啸 最前 前线\n",
      "\n",
      "Target1:= He had to go towards the water as the sirens sounded  . \n",
      "Predict1:< He was , , , , , , , ,  .  .  .\n",
      "\n",
      "Input2:> 我 最后 爬出 了 矿井 得以 回家 而 这些 被 奴役 的 矿工 可能 永远 没有 回家 的 那 一天\n",
      "\n",
      "Target2:= I got to climb out of that hole , and I got to go home , but they likely never will , because they are trapped in slavery  . \n",
      "Predict2:< I I t the , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 42s (- -59m 19s), Epoch: [4/10], Step: [2820/3335], Train Loss: 3.4962231159210204, BLEU: 0.6058195286813919\n",
      "\n",
      "Input1:> 谢谢\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 我 在 那里 成长 我 在 那里 养育 我 的 儿子\n",
      "\n",
      "Target2:= I grew up there  . I raised my sons there  . \n",
      "Predict2:< I I my my I I I I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 47s (- -59m 13s), Epoch: [4/10], Step: [2840/3335], Train Loss: 3.3623656153678896, BLEU: 0.673649503206097\n",
      "\n",
      "Input1:> 现在 如果 你 用 普通 的 子弹 走 同样 的 路程 平 且 放慢 视频 同样 是 一百 一百万 一百万倍 百万 百万倍 万倍 你 知道 你 需要 等 多久 来看 看到 这个 全过程 过程 吗\n",
      "\n",
      "Target1:= Now , if you take an ordinary bullet and let it go the same distance and slow down the video again by a factor of 10 billion , do you know how long you ll have to sit \n",
      "Predict1:< Now if you you you you , , , , , , , , , , , , , , , , , , , , you , ,\n",
      "\n",
      "Input2:> 我们 苍白 UNK 无力 面如 UNK 死灰\n",
      "\n",
      "Target2:= We are pale , gray creatures  . \n",
      "Predict2:< We we to to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 53s (- -59m 8s), Epoch: [4/10], Step: [2860/3335], Train Loss: 3.380056345462799, BLEU: 0.6107574081067371\n",
      "\n",
      "Input1:> 在 我 交 了 罚金 金和 贿赂 用 的 钱 后 我 的 家人 一个 一个月 后 被 释放 了 但 不久 后 他们 在 老挝 的 首都 又 被 抓起 起来 了\n",
      "\n",
      "Target1:= After I paid the fine and bribe , my family was released in one month , but soon after , my family was arrested and jailed again in the capital of Laos  . \n",
      "Predict1:< And I I I a , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 要 知道 有些 人 可能 不会 打 给 我们 或者 没接 接通\n",
      "\n",
      "Target2:= You see , some people may not do it  . They may not get through it  . \n",
      "Predict2:< You don t t know , , , to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58m 58s (- -59m 2s), Epoch: [4/10], Step: [2880/3335], Train Loss: 3.505608558654785, BLEU: 0.5655906305325193\n",
      "\n",
      "Input1:> 你 可以 看到 这个 电灯 电灯泡 灯泡 是 朝外 的 因为 狮子 是从 那儿 过来 的\n",
      "\n",
      "Target1:= As you can see , the bulbs face outside , because that is where the lions come from  . \n",
      "Predict1:< You you see see see , , , , , , , is the  .\n",
      "\n",
      "Input2:> 147 个\n",
      "\n",
      "Target2:= A hundred and UNK  . \n",
      "Predict2:< :  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 4s (- -60m 56s), Epoch: [4/10], Step: [2900/3335], Train Loss: 3.44063777923584, BLEU: 0.6054079090976383\n",
      "\n",
      "Input1:> 我 朋友 跟 我 在 一起\n",
      "\n",
      "Target1:= My friend got to come with me  . \n",
      "Predict1:< I I I I  .  .  .  .\n",
      "\n",
      "Input2:> 我们 这么 做 的 时候 谁知 知道 我们 是否 会\n",
      "\n",
      "Target2:= And when we do this , who knows  ? \n",
      "Predict2:< We we we we we we we we we we  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 9s (- -60m 51s), Epoch: [4/10], Step: [2920/3335], Train Loss: 3.405148482322693, BLEU: 0.5538833846870006\n",
      "\n",
      "Input1:> 朝鲜 朝鲜政府 政府 拦截 了 我 寄回 回家 的 部分 分钱 钱款 作为 惩罚 我 的 家人 要 被 强制 搬走 搬到 农村 的 一个 偏僻 的 地方 去\n",
      "\n",
      "Target1:= The North Korean authorities intercepted some money that I sent to my family , and , as a punishment , my family was going to be forcibly removed to a desolate location in the countryside  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 到 时人 人们 将 会 被 UNK 淹没\n",
      "\n",
      "Target2:= They were already drowning in manure  . \n",
      "Predict2:< And people people to to to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 15s (- -60m 45s), Epoch: [4/10], Step: [2940/3335], Train Loss: 3.5540139436721803, BLEU: 0.60059402553179\n",
      "\n",
      "Input1:> 我们 自己 也 变得 又 脏 又 臭 但是 我们 心里 很 舒坦\n",
      "\n",
      "Target1:= We got dirty , and we loved it  . \n",
      "Predict1:< We we t to , , , , , we  .  .  .\n",
      "\n",
      "Input2:> 我们 清理 了 大量 的 发臭 腐烂 的 鱼 尸骨 那些 鱼 都 是 当地 工厂 厂里 里加 加工 处理 的 鱼\n",
      "\n",
      "Target2:= We cleared tons and tons of stinking , rotting fish carcasses from the local fish processing plant  . \n",
      "Predict2:< We we a a of , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 20s (- -60m 40s), Epoch: [4/10], Step: [2960/3335], Train Loss: 3.323410928249359, BLEU: 0.5406299497492677\n",
      "\n",
      "Input1:> 早先 我 在 战争 中长 长大 周末 在 朋友 朋友家 UNK 过夜 周六 看 足球 足球比赛 比赛 跟 种族 种族歧视 歧视 的 孩子 打架 以及 参加 宗教 UNK 民族 民族主义 主义 游行 在 人群 中大 大喊 打倒 共产\n",
      "\n",
      "Target1:= Long before that I had grown up with the war , but alongside weekend sleepovers and Saturday soccer games and UNK with racist children of the UNK and UNK demonstrations chanting , &quot; Down with communism and long \n",
      "Predict1:< And I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 认为 答案 已经 藏 在 问题 之中\n",
      "\n",
      "Target2:= So I figured that the problem is the solution  . \n",
      "Predict2:< I I I to to the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 26s (- -60m 34s), Epoch: [4/10], Step: [2980/3335], Train Loss: 3.410877764225006, BLEU: 0.5833110140922385\n",
      "\n",
      "Input1:> 今天 我 仍然 在 打破 着 沉默\n",
      "\n",
      "Target1:= I am still breaking the silence today  . \n",
      "Predict1:< I I I to to to  .  .\n",
      "\n",
      "Input2:> 我 只是 一个 有着 亲身 亲身经历 身经 经历 要 讲述 的 女人\n",
      "\n",
      "Target2:= I am just one woman with a story to tell  . \n",
      "Predict2:< I I a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 31s (- -60m 29s), Epoch: [4/10], Step: [3000/3335], Train Loss: 3.4977795243263246, BLEU: 0.5798388689856862\n",
      "\n",
      "Input1:> 这个 个数 数字 是 在 跨 大西 大西洋 西洋 洋奴 奴隶 贸易 中 从 非洲 被 贩卖 的 UNK 总数 的 两倍\n",
      "\n",
      "Target1:= That is double the amount of people taken from Africa during the entire trans-Atlantic slave trade  . \n",
      "Predict1:< This is the the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 对 我 来说 阿富汗 充满 了 希望 和 无限 的 可能 可能性 每 一天 在 UNK 读书 的 女孩 女孩儿 孩儿 都 这么 提醒 着 我\n",
      "\n",
      "Target2:= To me , Afghanistan is a country of hope and boundless possibilities , and every single day the girls of UNK remind me of that  . \n",
      "Predict2:< And , , , , , , and , and , , , and and , , , and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 37s (- -60m 23s), Epoch: [4/10], Step: [3020/3335], Train Loss: 3.4060057997703552, BLEU: 0.6325463221761235\n",
      "\n",
      "Input1:> 如今 芭蕾 需要 非凡 的 专业 专业知识 知识 和 杰出 的 专业 技巧 甚至 一定 的 初级 适应 适应性 这 可能 就是 所说 的 天赋\n",
      "\n",
      "Target1:= Now , ballet requires an extraordinary level of expertise and a high level of skill , and probably a level of initial suitability that may well have a genetic component to it  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 刚好 当地 的 疏散 中心 正在 收集 人们 丢失 的 照片\n",
      "\n",
      "Target2:= This happened to also be a place in the town where the evacuation center was collecting the photos  . \n",
      "Predict2:< The the the the the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 42s (- -60m 18s), Epoch: [4/10], Step: [3040/3335], Train Loss: 3.4845728039741517, BLEU: 0.629056828511746\n",
      "\n",
      "Input1:> 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "\n",
      "Target1:= It was the only way we both could be educated  . \n",
      "Predict1:< It is is we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 就 这个 问题 在 国会 会上 争论 了 很久 很久很久 很久\n",
      "\n",
      "Target2:= And this was debated in Congress for ages and ages  . \n",
      "Predict2:< And is is a , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 48s (- -60m 12s), Epoch: [4/10], Step: [3060/3335], Train Loss: 3.4720043182373046, BLEU: 0.5895060031351712\n",
      "\n",
      "Input1:> 我们 会 因 芭蕾 芭蕾舞 大师 和 踢踏 踢踏舞 舞蹈 者 而 感到 欣悦 稍后 你们 就 会 知道 了\n",
      "\n",
      "Target1:= We delight at ballet virtuosos and tap dancers you will see later on  . \n",
      "Predict1:< We we t that and and and , , , , , and , and and and\n",
      "\n",
      "Input2:> 这个 我 曾经 最爱 的 人 用 枪 指着 我 的 头 并 威胁 要 杀掉 我 我 已经 记不得 不得 对 我 做 了 多少 多少次\n",
      "\n",
      "Target2:= The man who I loved more than anybody on Earth held a gun to my head and threatened to kill me more times than I can even remember  . \n",
      "Predict2:< And I I I I I I I I I I I I I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 53s (- -60m 7s), Epoch: [4/10], Step: [3080/3335], Train Loss: 3.485418510437012, BLEU: 0.5840283052277104\n",
      "\n",
      "Input1:> 在 左边 你 看到 了 我们 的 飞秒 成像 相机\n",
      "\n",
      "Target1:= On the left , you see our UNK  . \n",
      "Predict1:< In the the , , , we the the  .  .\n",
      "\n",
      "Input2:> 第二 第二件 二件 事 你 可以 了解 到 当 美国 美国政府 国政 政府 宣称 这 只是 一项 临时 措施 时 它 会 持续 223 年 之久\n",
      "\n",
      "Target2:= Second thing you can learn is that when a government says that this is a temporary measure -- -- you can still be waiting UNK years later  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 59m 59s (- -60m 1s), Epoch: [4/10], Step: [3100/3335], Train Loss: 3.4770145058631896, BLEU: 0.554781917524576\n",
      "\n",
      "Input1:> 你 想 过为 为什么 什么 点 句式 符号 号叫 子弹 点儿 吗\n",
      "\n",
      "Target1:= Have you ever wondered why they are called bullet points  ? \n",
      "Predict1:< Do you you to to  ?  ?  ?  ?  ?  ?  ?\n",
      "\n",
      "Input2:> UNK 沉浸 在 他 自己 的 世界 里 应用 他 自己 制定 的 法则 他 从 细微 的 事物 中 寻找 找乐 乐趣 比如 绕 着 屋子 排列 他 的 玩具 玩具车 盯 着 洗衣 洗衣机 把 他\n",
      "\n",
      "Target2:= Remi lived and reigned in his own world , with his own rules , and he found pleasure in the smallest things , like lining up cars around the room and staring at the washing machine and eating \n",
      "Predict2:< And he the the the , , the , , the , , , the , , the , , the , the , the the , the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 5s (- -61m 56s), Epoch: [4/10], Step: [3120/3335], Train Loss: 3.3936642527580263, BLEU: 0.5508412583265341\n",
      "\n",
      "Input1:> 你 会 为 孩子 们 如何 被 影响 而 震惊\n",
      "\n",
      "Target1:= You d be surprised how kids are affected by this  . \n",
      "Predict1:< You you to to to to the the\n",
      "\n",
      "Input2:> 在 西非 一些 人 毫无 怜悯 的 叫卖 死去 的 秃鹫 来 满足 巫术 和 收藏 的 需求\n",
      "\n",
      "Target2:= In West Africa , there is a horrific trade of dead vultures to serve the witchcraft and the fetish market  . \n",
      "Predict2:< In the , , , , , , , , and the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 10s (- -61m 50s), Epoch: [4/10], Step: [3140/3335], Train Loss: 3.4301381349563598, BLEU: 0.5581633563904599\n",
      "\n",
      "Input1:> 秃鹫 是 我们 的 大自然 自然 垃圾 清道 清道夫 道夫\n",
      "\n",
      "Target1:= They are our natural garbage collectors  . \n",
      "Predict1:< The is the we we we the  .  .\n",
      "\n",
      "Input2:> 我们 在世 在世界上 世界 上 300 个 社区 开展 了 这个 项目\n",
      "\n",
      "Target2:= We ve done it in 300 communities around the world  . \n",
      "Predict2:< We ve a a the in the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 16s (- -61m 45s), Epoch: [4/10], Step: [3160/3335], Train Loss: 3.4191590905189515, BLEU: 0.5817634709992393\n",
      "\n",
      "Input1:> 许多 光子 最终 终到 到达 了 瓶盖 处 并且 向 四周 散去\n",
      "\n",
      "Target1:= Many of the photons eventually reach the cap and then they explode in various directions  . \n",
      "Predict1:< And is , , , , , , , , , the\n",
      "\n",
      "Input2:> 现在 超人 能 飞\n",
      "\n",
      "Target2:= So , Superman can fly  . \n",
      "Predict2:< Now , is is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 21s (- -61m 39s), Epoch: [4/10], Step: [3180/3335], Train Loss: 3.375771641731262, BLEU: 0.634972301892612\n",
      "\n",
      "Input1:> 从 1971 年 到 1977 年 我 看起 看起来 起来 很 年轻 但 其实 我 老 了 我 曾 在 赞比亚 比亚 肯尼 肯尼亚 尼亚 象牙 象牙海岸 海岸 阿尔及利亚 利亚 索马里 马里 参与 过 不少 与非 非洲 各国\n",
      "\n",
      "Target1:= From 1971 to 1977 -- I look young , but I am not — -- I worked in Zambia , Kenya , Ivory Coast , Algeria , Somalia , in projects of technical cooperation with African countries  . \n",
      "Predict1:< And years years , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这些 影像 非 关于 议题 而是 关于 人 真实 的 人 和 你 我 一样 都 值得 在 生命 里 获得 一样 的 权利 尊严 和 尊重\n",
      "\n",
      "Target2:= These images are not of issues  . They are of people , real people , like you and me , all deserving of the same rights , dignity and respect in their lives  . \n",
      "Predict2:< These are the of of , , , , , , , , , , , , and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 27s (- -61m 33s), Epoch: [4/10], Step: [3200/3335], Train Loss: 3.4445759773254396, BLEU: 0.5428394477595938\n",
      "\n",
      "Input1:> 因为 我 当时 是 个 小孩 小孩子 孩子 我 可以 一整 一整天 整天 不 干 别的 在 自己 的 房间 里 鼓捣 一天 我 甚至 把 我 妈妈 新 买 的 收音 收音机 拆 得 七零八 七零八落 那天\n",
      "\n",
      "Target1:= Since I was a small boy , I used to work in my room for the whole day , and I even took apart my mom is new radio , and that day she almost killed me , \n",
      "Predict1:< Because I I I I I I I I I I I I I , I , I , , I , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 又 走 了 两个 小时 蜿蜒 的 小径 戛然 戛然而止 然而 止 我们 面前 的 空地 地上 出现 了 一大 大片 坑洞 每个 都 有 一个 橄榄 橄榄球 橄榄球场 球场 那么 大 每个 坑洞 里 都 有 奴隶\n",
      "\n",
      "Target2:= After another two hours of hiking , the winding trail abruptly ended at a clearing , and before us was a mass of holes that could fit into the size of a football field , and all of \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 32s (- -61m 28s), Epoch: [4/10], Step: [3220/3335], Train Loss: 3.5173500657081602, BLEU: 0.5234989579558454\n",
      "\n",
      "Input1:> 但 我 转而 又 想到 他们 教 我 的 那些 事 关于 个性 交流 和 爱 我 意识 到 这些 东西 我 可不 愿意 用 正常 去 换\n",
      "\n",
      "Target1:= But I cast my mind back to the things that they ve taught me about individuality and communication and love , and I realize that these are things that I wouldn t want to change with normality  . \n",
      "Predict1:< But I I I I I I I I I and I and and I and and I and I and I I I\n",
      "\n",
      "Input2:> 蜜月 期间 我 又 被 打 了 两次\n",
      "\n",
      "Target2:= It happened twice more on the honeymoon  . \n",
      "Predict2:< I : : I , I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 38s (- -61m 22s), Epoch: [4/10], Step: [3240/3335], Train Loss: 3.4896990060806274, BLEU: 0.5782042174512624\n",
      "\n",
      "Input1:> 我 看见 她们 的 双亲 尤其 是 她们 的 父亲 就 像 我 父亲 那样 支持 她们 尽管 从 他们 脸上 也 看出 了 畏惧\n",
      "\n",
      "Target1:= And I see their parents and their fathers who , like my own , advocate for them , despite and even in the face of daunting opposition  . \n",
      "Predict1:< I I them , , , they they , they they , they , they , , they , , they , , ,\n",
      "\n",
      "Input2:> 我 不知 知道 你们 的 情况 但 我 个人 而言 除了 这 15 张 照片 我 不会 经常 经常出现 出现 现在 在家 家庭 照片 里面\n",
      "\n",
      "Target2:= I don t know about you , but aside from these 15 shots , I am not in many of the family photos  . \n",
      "Predict2:< I I t know know , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 43s (- -61m 17s), Epoch: [4/10], Step: [3260/3335], Train Loss: 3.4128295183181763, BLEU: 0.5920720365804819\n",
      "\n",
      "Input1:> 掌声 谢谢\n",
      "\n",
      "Target1:= Thanks  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 这 是 我 帮助 其他 的 受害 受害者 的 方式 同时 也 是 我 对 你们 最后 的 请求\n",
      "\n",
      "Target2:= It is my way of helping other victims , and it is my final request of you  . \n",
      "Predict2:< This is is my , I I I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 49s (- -61m 11s), Epoch: [4/10], Step: [3280/3335], Train Loss: 3.509320068359375, BLEU: 0.5852548856801951\n",
      "\n",
      "Input1:> 我们 要 改写 流氓 的 定义\n",
      "\n",
      "Target1:= We gotta flip the script on what a gangsta is  . \n",
      "Predict1:< We we to to to  .  .\n",
      "\n",
      "Input2:> 会 发生 什么 呢\n",
      "\n",
      "Target2:= So what happened  ? \n",
      "Predict2:< What what  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 60m 54s (- -61m 6s), Epoch: [4/10], Step: [3300/3335], Train Loss: 3.5107047080993654, BLEU: 0.5859164887653756\n",
      "\n",
      "Input1:> 我们 总 呆 在 漆黑 漆黑一片 一片 不见 UNK 见天 见天日 天日 的 屋子 屋子里 工作 工作室 没有 窗 主要 要是 为了 抵御 阳光\n",
      "\n",
      "Target1:= We hide in dark , windowless rooms , and generally avoid sunlight  . \n",
      "Predict1:< We we the to the , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 就是 其中 一只 在 夜晚 被 杀害 的 奶牛 我 在 早晨 醒来 醒来时 来时 就 发现 它 已经 死 了 我 的 心情 槽 透 了 因为 这 是 我家 家里 唯一 的 公牛\n",
      "\n",
      "Target2:= This is one of the cows which was killed at night , and I just woke up in the morning and I found it dead , and I felt so bad , because it was the only bull \n",
      "Predict2:< This is is the the , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 0s (- -61m 0s), Epoch: [4/10], Step: [3320/3335], Train Loss: 3.3052249431610106, BLEU: 0.5952942491659006\n",
      "\n",
      "Input1:> 但是 总 需要 有人 维护 它\n",
      "\n",
      "Target1:= But you have to maintain it  . \n",
      "Predict1:< But it is it to  .  .\n",
      "\n",
      "Input2:> 这些 井 有 差不多 不多 有 90 多米 深 他们 抬出 抬出来 出来 装满 UNK 的 袋子 这些 石头 被 运送 送到 另外 一个 区域 被 捣碎 以便 提取 金子\n",
      "\n",
      "Target2:= The shafts are up to 300 feet deep , and they carry out heavy bags of stone that later will be transported to another area , where the stone will be pounded so that they can extract the \n",
      "Predict2:< These are are of , , , , , , , , , , , , , , , , , , , ,\n",
      "[3.5677786588668825, 3.4476536870002747, 3.416226696968079, 3.4205126404762267, 3.4793741703033447, 3.5294612526893614, 3.636067545413971, 3.355557107925415, 3.5787582874298094, 3.505247974395752, 3.5563385486602783, 3.3896560430526734, 3.5448543310165403, 3.5006566882133483, 3.4503707885742188, 3.343601655960083, 3.4381697297096254, 3.4732531189918516, 3.414517319202423, 3.389195239543915, 3.428777503967285, 3.4067443728446962, 3.3800053238868712, 3.4830841779708863, 3.406867277622223, 3.492591905593872, 3.5263291597366333, 3.3748878121376036, 3.5263229966163636, 3.4019383072853087, 3.5312399744987486, 3.348922276496887, 3.4544426679611204, 3.4722309350967406, 3.461645805835724, 3.411879849433899, 3.505241572856903, 3.467104208469391, 3.429835557937622, 3.368838059902191, 3.4459815859794616, 3.5019674062728883, 3.3941235899925233, 3.3908248782157897, 3.516837203502655, 3.5200705528259277, 3.4943707942962647, 3.4361815571784975, 3.391366970539093, 3.4163920164108275, 3.528168869018555, 3.5310001492500307, 3.4917668223381044, 3.528818166255951, 3.443132221698761, 3.4801522135734557, 3.402263379096985, 3.4849227547645567, 3.4427913546562197, 3.423172318935394, 3.5301421642303468, 3.518061661720276, 3.4389845371246337, 3.426754415035248, 3.3650981783866882, 3.5698448538780214, 3.540112292766571, 3.596217942237854, 3.4451733827590942, 3.4383995175361632, 3.4874107360839846, 3.3177995681762695, 3.462091255187988, 3.3060368418693544, 3.5655988216400147, 3.498650300502777, 3.4553451895713807, 3.447783052921295, 3.4839892268180845, 3.3589912056922913, 3.4118427276611327, 3.3149358153343202, 3.472547173500061, 3.4836886405944822, 3.4115664839744566, 3.3055467128753664, 3.6405311942100527, 3.372882914543152, 3.3078127980232237, 3.394757568836212, 3.4305913925170897, 3.3741452217102053, 3.4968255877494814, 3.491457724571228, 3.4007982134819033, 3.4522140979766847, 3.403877246379852, 3.539729654788971, 3.4245808362960815, 3.5143196821212768, 3.4482205271720887, 3.2858118176460267, 3.449970233440399, 3.388121175765991, 3.348788821697235, 3.365761947631836, 3.4077020049095155, 3.466093122959137, 3.3919060230255127, 3.3348267912864684, 3.435918951034546, 3.430356514453888, 3.4795674324035644, 3.333921563625336, 3.4949191331863405, 3.3929832935333253, 3.4931127429008484, 3.561202335357666, 3.3518301367759706, 3.5473344683647157, 3.3808329820632936, 3.4906386852264406, 3.469350516796112, 3.378352952003479, 3.6331146717071534, 3.497339153289795, 3.4849547386169433, 3.4277180194854737, 3.354875552654266, 3.3459847450256346, 3.374389576911926, 3.4273247838020326, 3.5380445599555967, 3.3994518160820006, 3.480313754081726, 3.3214148044586183, 3.5148215293884277, 3.467152988910675, 3.4338281750679016, 3.4276928901672363, 3.4962231159210204, 3.3623656153678896, 3.380056345462799, 3.505608558654785, 3.44063777923584, 3.405148482322693, 3.5540139436721803, 3.323410928249359, 3.410877764225006, 3.4977795243263246, 3.4060057997703552, 3.4845728039741517, 3.4720043182373046, 3.485418510437012, 3.4770145058631896, 3.3936642527580263, 3.4301381349563598, 3.4191590905189515, 3.375771641731262, 3.4445759773254396, 3.5173500657081602, 3.4896990060806274, 3.4128295183181763, 3.509320068359375, 3.5107047080993654, 3.3052249431610106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 7s (- -59m 55s), Epoch: [5/10], Step: [20/3335], Train Loss: 3.566279423236847, BLEU: 0.6000769582045992\n",
      "\n",
      "Input1:> 为了 改变 世界 我们 迫切 迫切需要 需要 从 我们 的 科学 科学家 学家 和 工程 工程师 那里 得到 良好 的 沟通\n",
      "\n",
      "Target1:= We desperately need great communication from our scientists and engineers in order to change the world  . \n",
      "Predict1:< And we , to to to and and and and and and and our our our\n",
      "\n",
      "Input2:> 孩子 们 从 家里 被 带出 出来 被 贩卖 被迫 迫在 船上 无休 无休止 休止 的 工作 他们 在 湖上 上工 工作 甚至 还 不会 游泳\n",
      "\n",
      "Target2:= Children are taken from their families and trafficked and vanished , and they are forced to work endless hours on these boats on the lake , even though they do not know how to swim  . \n",
      "Predict2:< They are the to , , , , , , , , , , , , , they , , , they , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 13s (- -60m 18s), Epoch: [5/10], Step: [40/3335], Train Loss: 3.4070756435394287, BLEU: 0.5873590294813604\n",
      "\n",
      "Input1:> 我 不 反对 对开 开发 风能 但是 我们 应该 呼吁 政府 重新 重新考虑 考虑 这项 决定 因为 风力 涡轮 涡轮机 轮机 会 对 鸟类 造成 威胁 高速 的 扇 叶 会 把 鸟儿 切 成 两半\n",
      "\n",
      "Target1:= I am not against wind energy , but we need to work with the governments , because wind turbines do this to birds  . They slice them in half  . \n",
      "Predict1:< I I t t to , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 可以 向 客户 提出 你 的 点子\n",
      "\n",
      "Target2:= You can give somebody an idea  . \n",
      "Predict2:< You can can you your your  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 18s (- -61m 42s), Epoch: [5/10], Step: [60/3335], Train Loss: 3.394347310066223, BLEU: 0.5557228177932535\n",
      "\n",
      "Input1:> 他 非常 害怕 主人 不敢 逃跑 由于 他 从小 就 被 残酷 对待 他 也 用 同样 的 方法 对待 比 他 小 的 奴隶\n",
      "\n",
      "Target1:= Terrified of his master , he will not run away , and since he is been treated with cruelty all his life , he passes that down to the younger slaves that he manages  . \n",
      "Predict1:< He he he he , , , , , , , he , , , , , he , , , he ,\n",
      "\n",
      "Input2:> 我 在 一个 UNK 遇见 了 康 纳 那时 是 一月 一月份 月份 天 很 冷\n",
      "\n",
      "Target2:= I met UNK on a cold , rainy January night  . \n",
      "Predict2:< I I a a a a , , , , , a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 24s (- -61m 21s), Epoch: [5/10], Step: [80/3335], Train Loss: 3.294837462902069, BLEU: 0.6325683969982133\n",
      "\n",
      "Input1:> CA 会 是 一场 UNK Richard UNK 你 与众不同 不同\n",
      "\n",
      "Target1:= In the trenches  . Richard UNK , you are something else  . \n",
      "Predict1:< And is is you you you  .  .  .\n",
      "\n",
      "Input2:> 要 知道 有些 人 可能 不会 打 给 我们 或者 没接 接通\n",
      "\n",
      "Target2:= You see , some people may not do it  . They may not get through it  . \n",
      "Predict2:< You don t know , , , , t t  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 29s (- -61m 6s), Epoch: [5/10], Step: [100/3335], Train Loss: 3.447178316116333, BLEU: 0.5900569871755204\n",
      "\n",
      "Input1:> 我 想 介绍 给 你们 认识 我 的 两个 兄弟\n",
      "\n",
      "Target1:= Now I d like to introduce you to my brothers  . \n",
      "Predict1:< I I to to you you to to to  .  .\n",
      "\n",
      "Input2:> 冬天 很 舒服 但 夏天 却 极热\n",
      "\n",
      "Target2:= It was cozy in winter but extremely hot in summer  . \n",
      "Predict2:< And , is , ,  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 35s (- -62m 55s), Epoch: [5/10], Step: [120/3335], Train Loss: 3.6949307918548584, BLEU: 0.5847401267833531\n",
      "\n",
      "Input1:> 这些 事 就 发生 生在 我们 身边 我们 只是 不知 知道 罢了\n",
      "\n",
      "Target1:= It is all around us  . We just don t see it  . \n",
      "Predict1:< And are are we , , , we , we we we we  .  .  .\n",
      "\n",
      "Input2:> 相反 正如 爱因斯坦 因斯 坦 所说 让 一切 尽可 尽可能 可能 的 简单 而 不是 简化\n",
      "\n",
      "Target2:= Instead , as Einstein said , make everything as simple as possible , but no simpler  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 41s (- -62m 45s), Epoch: [5/10], Step: [140/3335], Train Loss: 3.5530288934707643, BLEU: 0.6021705276350081\n",
      "\n",
      "Input1:> 谁 能 发明 这种 科技 来 实现 绿色 绿色革命 革命 大学 吗 别 想 了\n",
      "\n",
      "Target1:= Who is going to invent the technology for the green revolution  ? Universities  ? Forget about it  ! \n",
      "Predict1:< And is , , , , , , , ,  ?  ?  ?\n",
      "\n",
      "Input2:> 同时 我 在 考虑 如果 没有 途径 去 得到 健康 的 食物 你 的 感受 是 什么 如果 每次 你 从 家里 出来 看到 在 你 的 社区 现有 的 食物 系统 的 不良 不良影响 影响 你 的\n",
      "\n",
      "Target2:= And I was wondering , how would you feel if you had no access to healthy food , if every time you walk out your door you see the ill effects that the present food system has on \n",
      "Predict2:< And I I you you you you you you you you you , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 46s (- -62m 36s), Epoch: [5/10], Step: [160/3335], Train Loss: 3.5061386704444883, BLEU: 0.5972715327573397\n",
      "\n",
      "Input1:> 我 在 为 这次 演讲 做 调查 时 发现 现在 美国 13 岁 女孩 女孩儿 孩儿 群体 中 百分 百分之 53 不 喜欢 她们 的 身体 到 了 17 岁 这 一比 比例 上升 升至 百分 百分之 78\n",
      "\n",
      "Target1:= When I was researching this talk , I found out that of the 13-year-old girls in the United States , 53 percent don t like their bodies , and that number goes to 78 percent by the time \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 那 是 日本 UNK UNK 的 一个 小 渔村 人口 大约 只有 5 万 人 是 当时 第一 第一波 受灾 的 地方\n",
      "\n",
      "Target2:= It is a small fishing town in UNK Prefecture , about 50,000 people , one of the first that was hit by the wave  . \n",
      "Predict2:< It is a a of , , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 52s (- -62m 28s), Epoch: [5/10], Step: [180/3335], Train Loss: 3.580853283405304, BLEU: 0.612179735679407\n",
      "\n",
      "Input1:> 谢谢 掌声\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 比如 艾哈迈 艾哈迈德 哈迈德 迈德 这 不是 真名 我 也 不能 让 你们 看见 他 的 长相 他 是 我 一个 学生 的 父亲\n",
      "\n",
      "Target2:= Like Ahmed  . That is not his real name , and I cannot show you his face , but Ahmed is the father of one of my students  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 61m 57s (- -62m 20s), Epoch: [5/10], Step: [200/3335], Train Loss: 3.400308334827423, BLEU: 0.5696371007592602\n",
      "\n",
      "Input1:> 我们 可以 多次 多次重 多次重复 重复\n",
      "\n",
      "Target1:= Several times we can repeat that  . \n",
      "Predict1:< We can can to the  .  .  .\n",
      "\n",
      "Input2:> 这 就是 这个 项目 目的 全部 为了 恢复 人性 中 的 一点 小 碎片 挽回 UNK 人们 与 过去 的 连接 连接点 接点\n",
      "\n",
      "Target2:= That is all this project was about , about restoring those little bits of humanity , giving someone that connection back  . \n",
      "Predict2:< This is is the the the the the the the the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 3s (- -62m 13s), Epoch: [5/10], Step: [220/3335], Train Loss: 3.653374123573303, BLEU: 0.6036912721312759\n",
      "\n",
      "Input1:> 我 读 过 一个 当时 可怜 议员 的 日记 里面 持续 的 记录 着 竟然 又 是 这个 议题\n",
      "\n",
      "Target1:= I read a diary of this poor senator , who just keeps coming back , &quot; Still on this subject  . &quot; \n",
      "Predict1:< I I a a a a , , , , , , a  .\n",
      "\n",
      "Input2:> 你们 中 的 一些 人 可能 还 不 太 了解 孤独 孤独症 这个 术语 它 是 由 复杂 的 大脑 功能 紊乱 而 导致 的 社交 障碍 学习 障碍 有时 伴有 运动 功能 功能障碍 障碍\n",
      "\n",
      "Target2:= Now , for you who may be less familiar with the term &quot; autism , &quot; it is a complex brain disorder that affects social communication , learning and sometimes physical skills  . \n",
      "Predict2:< You you you you you , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 8s (- -62m 6s), Epoch: [5/10], Step: [240/3335], Train Loss: 3.4086325764656067, BLEU: 0.5885026459773182\n",
      "\n",
      "Input1:> 而且 仅仅 几年 年后 我们 有 了 第一 第一次 一次 记录 使用 短语 as UNK as UNK UNK 敢作 敢作敢为\n",
      "\n",
      "Target1:= And it was only a few years later that we have the first recorded use of the phrase &quot; as bold as brass  . &quot; \n",
      "Predict1:< And , , , , , , , , , we , , , we , , ,  .  .\n",
      "\n",
      "Input2:> 我 想要 我们 成为 环境 革命 的 叛徒 流氓 流氓 园丁\n",
      "\n",
      "Target2:= So I want us all to become UNK renegades , UNK , gangsta gardeners  . \n",
      "Predict2:< I I to to to to to the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 14s (- -62m 0s), Epoch: [5/10], Step: [260/3335], Train Loss: 3.4760347247123717, BLEU: 0.543691269862891\n",
      "\n",
      "Input1:> 我们 聊起 了 奴隶 奴隶制 奴隶制度 制度 而 此时 我 才 开始 真正 了解 奴役 的 现实 虽然 我 一直 都 知道 世上 奴隶 奴隶制 尚存 但 完全 不知 知道 事态 的 严重 严重性\n",
      "\n",
      "Target1:= We started talking about slavery , and really , I started learning about slavery , for I had certainly known it existed in the world , but not to such a degree  . \n",
      "Predict1:< We know t that , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 从 声音 里 我们 可以 追寻 寻到 声 UNK 颤动 时 的 位置 正如 帕金森 能 影响 我们 四肢 它 也 能 影响 我们 发声 发声器 发声器官 器官\n",
      "\n",
      "Target2:= From the sound , we can actually track the vocal fold position as it vibrates , and just as the limbs are affected in Parkinson is , so too are the vocal organs  . \n",
      "Predict2:< And can we we we , we , , , we , , , , , we , , , , , we , we ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 19s (- -63m 53s), Epoch: [5/10], Step: [280/3335], Train Loss: 3.4496815323829653, BLEU: 0.5990358182364477\n",
      "\n",
      "Input1:> 一个 奄奄 奄奄一息 一息 的 女人 正 躺 在 地上 怀里 抱 着 一个 虚弱 的 孩子 那个 孩子 只能 绝望 地 望 着 他 母亲 的 脸\n",
      "\n",
      "Target1:= A lifeless woman was lying on the ground , while an emaciated child in her arms just stared helplessly at his mother is face  . \n",
      "Predict1:< One a a a , , , , , , , , , , , , , , , a\n",
      "\n",
      "Input2:> 我 也 很 幸运 幸运地 因为 这项 项发明 发明 而 获得 了 奖学 奖学金 这 是 肯尼 肯尼亚 尼亚 最好 的 学校 布鲁克 豪斯 国际 学校 所 颁发 的 我 对此 感到 十分 的 兴奋\n",
      "\n",
      "Target2:= Because of this invention , I was lucky to get a scholarship in one of the best schools in Kenya , UNK International School , and I am really excited about this  . \n",
      "Predict2:< I I was that was , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 25s (- -63m 47s), Epoch: [5/10], Step: [300/3335], Train Loss: 3.4562154412269592, BLEU: 0.5932047212283409\n",
      "\n",
      "Input1:> 奈 库 贝特 女神 在 古埃及 埃及 埃及文 文化 中 象征 着 监护 监护人 护人 和 母性 通常 和 眼镜 眼镜蛇 女神 维 阿 杰 特 一起 出现 象征 着上 埃及 和 下 埃及 的 团结 和 统一\n",
      "\n",
      "Target1:= UNK was the symbol of the protector and the motherhood , and together with the cobra , symbolized the unity between Upper and Lower Egypt  . \n",
      "Predict1:< The , , , the , , , , , , , , , , , , , , , , , , , , , , , , and\n",
      "\n",
      "Input2:> 已经 远远 远不止 不止 了\n",
      "\n",
      "Target2:= It is much better than that  . \n",
      "Predict2:< It is is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 30s (- -63m 40s), Epoch: [5/10], Step: [320/3335], Train Loss: 3.4839292407035827, BLEU: 0.6120245103831173\n",
      "\n",
      "Input1:> 你 可能 会 震惊 土地 所能 做 的 如果 你 把 它 当成 你 的 画布\n",
      "\n",
      "Target1:= You d be surprised what the soil could do if you let it be your canvas  . \n",
      "Predict1:< You you you you you you you you you you you you  .  .\n",
      "\n",
      "Input2:> 湖面 下 浸 着 些 树 的 枝干 经常 常会 挂住 渔网 这些 疲惫 害怕 的 孩子 们 就 被 扔到 水中 去 解开 渔网\n",
      "\n",
      "Target2:= The skeletal tree limbs submerged in Lake Volta often catch the fishing nets , and weary , frightened children are thrown into the water to untether the lines  . \n",
      "Predict2:< And the , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 36s (- -63m 34s), Epoch: [5/10], Step: [340/3335], Train Loss: 3.467835986614227, BLEU: 0.6062011340352733\n",
      "\n",
      "Input1:> 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定 女孩 上学 是 违法 的\n",
      "\n",
      "Target1:= You see , I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school  . \n",
      "Predict1:< I I , , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 所以 当天 傍晚 我 就 登陆 脸谱 网 问 了 一些 修 图 师 朋友 还 没到 第二 第二天 二天 早上 就 有 很多 很多很多 很多 回复 消息 说 他们 乐意 帮忙 我 知道 我们 应该 该行 行动\n",
      "\n",
      "Target2:= So that evening , I just reached out on Facebook and asked a few of them , and by morning the response had been so overwhelming and so positive , I knew we had to give it a \n",
      "Predict2:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 41s (- -63m 28s), Epoch: [5/10], Step: [360/3335], Train Loss: 3.4456197142601015, BLEU: 0.6642422480268944\n",
      "\n",
      "Input1:> 即使 他们 学会 了 中文 找到 了 工作 他们 的 整个 世界 也 会 在 顷刻 顷刻间 颠覆\n",
      "\n",
      "Target1:= Even after learning a new language and getting a job , their whole world can be turned upside down in an instant  . \n",
      "Predict1:< And they they they they they they they they they they they they they they they\n",
      "\n",
      "Input2:> 回到 我 的 问题 为什么 什么 我 要 留下 留下来 下来 了\n",
      "\n",
      "Target2:= Back to my question : Why did I stay  ? \n",
      "Predict2:< So , I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 47s (- -63m 22s), Epoch: [5/10], Step: [380/3335], Train Loss: 3.427717089653015, BLEU: 0.6444490885171847\n",
      "\n",
      "Input1:> 在 沥青 沥青路 路上 上会 溅 起 很多 水\n",
      "\n",
      "Target1:= Then you can have a lot of splash water in the asphalt  . \n",
      "Predict1:< In is , , , , ,  .  .\n",
      "\n",
      "Input2:> 现在 在 这里 有 很多 事 会 发生 所以 让 我 一件 一件 的 展示\n",
      "\n",
      "Target2:= Now , there is a lot going on in this movie , so let me break this down and show you what is going on  . \n",
      "Predict2:< Now , is a I I I I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 52s (- -63m 16s), Epoch: [5/10], Step: [400/3335], Train Loss: 3.3074670314788817, BLEU: 0.608709094798837\n",
      "\n",
      "Input1:> 神经 神经学 经学 测试 是 无 创 性 的 而且 他们 都 使用 现有 的 基础 设备\n",
      "\n",
      "Target1:= The neurologist is test is non-invasive  . They both use existing infrastructure  . \n",
      "Predict1:< The is , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 你 再 需要 一个 像 这样 的 机器 一个 感应 UNK 电机 它 可以 用来 做饭\n",
      "\n",
      "Target2:= Then you need a machine , like you see here , that you can use for cooking -- an induction machine  . \n",
      "Predict2:< You can can to a a a a a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 62m 58s (- -63m 10s), Epoch: [5/10], Step: [420/3335], Train Loss: 3.283340561389923, BLEU: 0.6449027951151359\n",
      "\n",
      "Input1:> 我 有 信心 你们 将 会 看到 越来 越来越 越多 的 朝鲜 朝鲜人 在世 世界 的 舞台 台上 取得 取得成功 得成 成功 包括 TED 这个 舞台\n",
      "\n",
      "Target1:= I am confident that you will see more and more North Koreans succeeding all over the world , including the TED stage  . \n",
      "Predict1:< I I you you you you you you you you the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 那么 我 在 麻省 麻省理工 麻省理工学院 理工 理工学 理工学院 工学 工学院 学院 的 团队 是 怎么 做出 这个 相机 的 呢\n",
      "\n",
      "Target2:= So how did my team at MIT create this camera  ? \n",
      "Predict2:< So what I what the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 4s (- -63m 4s), Epoch: [5/10], Step: [440/3335], Train Loss: 3.454622805118561, BLEU: 0.651405052647167\n",
      "\n",
      "Input1:> 种 一棵 一棵树 能够 收到 一千 甚至 一万 万颗 种子\n",
      "\n",
      "Target1:= Growing one plant will give you 1,000 , 10,000 seeds  . \n",
      "Predict1:< And is the the the the the the  .  .\n",
      "\n",
      "Input2:> 一年 年前 我 只是 一个 热带 带大 草原 上 帮 爸爸 UNK 的 普通 男孩 我 常常 看着 飞机 飞过 并且 告诉 自己 总有一天 一天 我 也 会 在 里面\n",
      "\n",
      "Target2:= So one year ago , I was just a boy in the savanna grassland herding my father is cows , and I used to see planes flying over , and I told myself that one day , I \n",
      "Predict2:< So I , , , I , I , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 9s (- -64m 58s), Epoch: [5/10], Step: [460/3335], Train Loss: 3.3726407289505005, BLEU: 0.6072263779612336\n",
      "\n",
      "Input1:> 食物 是 问题 食物 也 是 解决 解决方案 方案\n",
      "\n",
      "Target1:= Food is the problem and food is the solution  . \n",
      "Predict1:< The is is is is is is is  .\n",
      "\n",
      "Input2:> 另外 当 你们 描述 科学 原理 时 要 注意 术语 的 使用\n",
      "\n",
      "Target2:= And when you are describing your science , beware of jargon  . \n",
      "Predict2:< And , , you you you you you you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 15s (- -64m 52s), Epoch: [5/10], Step: [480/3335], Train Loss: 3.2930323600769045, BLEU: 0.6153406855014856\n",
      "\n",
      "Input1:> 你 可能 会 震惊 土地 所能 做 的 如果 你 把 它 当成 你 的 画布\n",
      "\n",
      "Target1:= You d be surprised what the soil could do if you let it be your canvas  . \n",
      "Predict1:< You you you you you you you you you you you you you you  .  .\n",
      "\n",
      "Input2:> 我们 的 手工 清洗 活动 非常 顺利\n",
      "\n",
      "Target2:= Now , we were lucky with our UNK  . \n",
      "Predict2:< Our is our our  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 20s (- -64m 46s), Epoch: [5/10], Step: [500/3335], Train Loss: 3.450244355201721, BLEU: 0.5983935507974962\n",
      "\n",
      "Input1:> 这 为 我们 营造 了 一个 反省 和 思考 的 空间 也 提醒 我们 在 不断 断成 成长 改变 的 过程 中 什么 才 是 最为 为重 重要 的\n",
      "\n",
      "Target1:= It is about making space for reflection and contemplation , and remembering what really matters most to us as we grow and change  . \n",
      "Predict1:< And is is a we we we and and and and and and and and and and and and and and and\n",
      "\n",
      "Input2:> 要 知道 有些 人 可能 不会 打 给 我们 或者 没接 接通\n",
      "\n",
      "Target2:= You see , some people may not do it  . They may not get through it  . \n",
      "Predict2:< You know t know , , , , , t  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 26s (- -64m 41s), Epoch: [5/10], Step: [520/3335], Train Loss: 3.5237938523292542, BLEU: 0.5989442869238414\n",
      "\n",
      "Input1:> 你 再 需要 一个 像 这样 的 机器 一个 感应 UNK 电机 它 可以 用来 做饭\n",
      "\n",
      "Target1:= Then you need a machine , like you see here , that you can use for cooking -- an induction machine  . \n",
      "Predict1:< You can to to a a a a a a a a a a  .  .\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢 肯尼 肯尼亚 尼亚 UNK 亚语 在 我 的 语言 里 它 的 意思 是 非常 非常感谢 感谢\n",
      "\n",
      "Target2:= UNK UNK  . It means in my language , thank you very much  . \n",
      "Predict2:< Thank you very very , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 31s (- -64m 35s), Epoch: [5/10], Step: [540/3335], Train Loss: 3.4599188208580016, BLEU: 0.6616060513840613\n",
      "\n",
      "Input1:> 但是 今天 阿富汗 接受 受教 教育 的 女孩 已经 超过 300 万\n",
      "\n",
      "Target1:= But today , more than three million girls are in school in Afghanistan  . \n",
      "Predict1:< But the , , the the the the the the the\n",
      "\n",
      "Input2:> 之后 经过 三周 的 辩论 最后 参议 参议院 议院 也 没有 屈服\n",
      "\n",
      "Target2:= And after three weeks of debate , in the end the Senate did not cave in  . \n",
      "Predict2:< And the the , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 37s (- -64m 29s), Epoch: [5/10], Step: [560/3335], Train Loss: 3.329887795448303, BLEU: 0.6584424668424527\n",
      "\n",
      "Input1:> 我们 将 为 你 在 成长 中 的 每 一步 喝彩 朋友\n",
      "\n",
      "Target1:= We are going to cheer you on every step of the way , my friend  . \n",
      "Predict1:< We are going to to you to the  .  .  .\n",
      "\n",
      "Input2:> 你 甚至 都 无从 结识 社区 区里 最 聪明 的 那些 人 因为 他们 根本 不 参与 你们 的 公众 集会\n",
      "\n",
      "Target2:= The smartest people in your community you don t even know , because they don t come to your public meetings  . \n",
      "Predict2:< You you you you , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 42s (- -64m 23s), Epoch: [5/10], Step: [580/3335], Train Loss: 3.4016898036003114, BLEU: 0.6961865701774926\n",
      "\n",
      "Input1:> 而 最近 如果 你 留意 过 肯尼 肯尼亚 尼亚 的 新闻 掌声 欢呼 肯尼 肯尼亚 尼亚 的 国会 国会议员 会议 会议员 议员 们 被 讽刺 为 像 秃鹫 一般 贪婪 和 愚蠢 但是 我 想 UNK 它\n",
      "\n",
      "Target1:= More recently , if you ve been following the Kenyan press — — these are the attributes that they associated the Kenyan MPs with  . But I want to challenge that  . \n",
      "Predict1:< And if you , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 48s (- -64m 18s), Epoch: [5/10], Step: [600/3335], Train Loss: 3.4488552093505858, BLEU: 0.6702964805927474\n",
      "\n",
      "Input1:> 开车 车行 行驶 在 加纳 的 一条 路上 与 Free the UNK 的 伙伴 一起 一个 骑 着 UNK 单车 的 同行 同行人 行人 突然 加速 靠近 我们 的 车 敲 着 车窗\n",
      "\n",
      "Target1:= Driving down a road in Ghana with partners of Free the Slaves , a fellow UNK on a UNK suddenly sped up to our UNK and tapped on the window  . \n",
      "Predict1:< And the the the the , , , , , , , , , , , , , , , , the the the the the the\n",
      "\n",
      "Input2:> 掌声 我 的 家人 很 信任 我\n",
      "\n",
      "Target2:= My family believes in me  . \n",
      "Predict2:< I my my my my  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 53s (- -64m 12s), Epoch: [5/10], Step: [620/3335], Train Loss: 3.5067548036575316, BLEU: 0.6496304530853287\n",
      "\n",
      "Input1:> 人们 把 收集 到 的 照片 送到 那个 公社 温泉 当时 我 十分 有幸 幸得 得到 了 他们 的 信任 让 我 开始 清洗 还原 那些 照片\n",
      "\n",
      "Target1:= This is where people were handing them in , and I was honored that day that they actually trusted me to help them start UNK them  . \n",
      "Predict1:< And I to to to , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 然后 他 指着 着地 地上 一排 很 不明 明显 的 脚印 说 就是 这里 就是 这条 路 走\n",
      "\n",
      "Target2:= Then he pointed toward this barely visible UNK , and said , &quot; This is the path , this is the path  . Go  . &quot; \n",
      "Predict2:< And then he , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 59s (- -64m 6s), Epoch: [5/10], Step: [640/3335], Train Loss: 3.424999940395355, BLEU: 0.6218994557943914\n",
      "\n",
      "Input1:> 他 说 小时 小时候 时候 的 精神 精神创伤 创伤 使得 他 需要 拥有 这些 枪\n",
      "\n",
      "Target1:= And he said that he needed those guns because of the trauma he d experienced as a young boy  . \n",
      "Predict1:< He he , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 一旦 价格 价格低 价格低廉 低廉 它 也 就 能 被 大规 大规模 规模 地 扩展\n",
      "\n",
      "Target2:= When something becomes ultra-low cost , it becomes massively scalable  . \n",
      "Predict2:< And it , , , , , , it , it  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 4s (- -64m 1s), Epoch: [5/10], Step: [660/3335], Train Loss: 3.3604604482650755, BLEU: 0.6224206710107862\n",
      "\n",
      "Input1:> 现在 着 整个 事情 鼓掌\n",
      "\n",
      "Target1:= Now , the whole event \n",
      "Predict1:< Now , is is  .\n",
      "\n",
      "Input2:> 现在 就 像 爱 哲 顿 博士 一个 科学 科学家 学家 科学 变成 了 艺术 一种 高速 摄影 摄影艺术 艺术 而且 我 意识 到 我们 每次 收集 到 的 大量 资料 不仅 是 科学 学成 成像 而且 还是\n",
      "\n",
      "Target2:= Now , like Doc Edgerton , a scientist himself , science became art , an art of UNK photography , and I realized that all the gigabytes of data that we are collecting every time is not just \n",
      "Predict2:< Now , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 10s (- -65m 55s), Epoch: [5/10], Step: [680/3335], Train Loss: 3.421999680995941, BLEU: 0.6291177074461552\n",
      "\n",
      "Input1:> 当然 了 你们 可以 说 空间 空间性 与 瞬时 瞬时性 可是 为什么 什么 不 说 空间 与 时间 呢 这 对 我们 来说 容易 理解 多 了\n",
      "\n",
      "Target1:= Sure , you can say &quot; spatial and temporal , &quot; but why not just say &quot; space and time , &quot; which is so much more accessible to us  ? \n",
      "Predict1:< And , course , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 在 剑桥 剑桥市 市长 长大 有 一次 我 走进 一家 家商 商店 却 忘 了 UNK 他们 送 了 我 一条 裙子\n",
      "\n",
      "Target2:= I grew up in Cambridge , and one time I went into a store and I forgot my money and they gave me the dress for free  . \n",
      "Predict2:< I I I , I , I , I , , , , , , , , , , I , , I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 15s (- -65m 49s), Epoch: [5/10], Step: [700/3335], Train Loss: 3.537403094768524, BLEU: 0.6434415108161651\n",
      "\n",
      "Input1:> 我们 开始 交往 他 喜欢 关于 我 的 一切 例如 我 的 聪明 我 在 哈佛 的 求学 经历 我 给予 青少 青少年 少年 女性 的 热情 UNK 帮助 以及 我 的 工作\n",
      "\n",
      "Target1:= We started dating , and he loved everything about me , that I was smart , that I d gone to Harvard , that I was passionate about helping teenage girls , and my job  . \n",
      "Predict1:< We had to to to my , my , , my , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 纺织 纺织业 也 是 我们 听到 奴役 劳动 时经 经常 联想 想到 的\n",
      "\n",
      "Target2:= The textile industry is another one we often think of when we hear about slave labor  . \n",
      "Predict2:< And we we we , we , we we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 21s (- -65m 43s), Epoch: [5/10], Step: [720/3335], Train Loss: 3.3405981779098513, BLEU: 0.7096638217288358\n",
      "\n",
      "Input1:> 这 是 UNK 当 他 父亲 死 的 时候 他 的 叔叔 把 他 卖到 自己 在 工作 的 这个 矿场\n",
      "\n",
      "Target1:= This is UNK  . When his father died , his uncle trafficked him to work with him in the mines  . \n",
      "Predict1:< He is the the , he , he , he , he , he the the he  .\n",
      "\n",
      "Input2:> 你 能 做 什么 你 能 生产 产出 出来 吗\n",
      "\n",
      "Target2:= What can you do  ? Can you make it  ? \n",
      "Predict2:< Can you you you do  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 26s (- -65m 38s), Epoch: [5/10], Step: [740/3335], Train Loss: 3.4492794394493105, BLEU: 0.5956653762697188\n",
      "\n",
      "Input1:> 有时 学校 校会 突然 停课 一周 因为 被 塔利 塔利班 怀疑 了\n",
      "\n",
      "Target1:= From time to time , the school would suddenly be canceled for a week because Taliban were suspicious  . \n",
      "Predict1:< And , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 为了 让 我 的 家人 获得 自由 倾尽 所能 然而 就 在 快要 成功 的 时候 家人 却 又 要 经历 牢狱 牢狱之灾 之灾 并且 这个 监狱 离 朝鲜 大使 大使馆 使馆 并 不远\n",
      "\n",
      "Target2:= I did everything to get my family to freedom , and we came so close , but my family was thrown in jail just a short distance from the South Korean embassy  . \n",
      "Predict2:< I I to to to to to , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 32s (- -65m 32s), Epoch: [5/10], Step: [760/3335], Train Loss: 3.339508032798767, BLEU: 0.6698293083968152\n",
      "\n",
      "Input1:> 但 现在 五十 五十年 十年 年后 我们 可以 再快 一百 一百万 一百万倍 百万 百万倍 万倍 也 就是 不仅 在 一百 一百万 百万 百万分之一 万分 万分之 万分之一 之一 一秒 或者 十亿 十亿分 亿分 之一 一秒 而是 在 万亿 亿分\n",
      "\n",
      "Target1:= But now , 50 years later , we can go a million times faster and see the world not at a million , or a billion , but one trillion frames per second  . \n",
      "Predict1:< But , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 任何 健康 或者 患有 帕金森 疾病 的 人 都 能 打进 进来 很 便宜 留下 录音 每次 只要 几分 分钱 我 很 高兴 宣布 我们 仅 在 8 小时 内 就 达到 了 我们 目标 标的 6\n",
      "\n",
      "Target2:= Anyone healthy or with Parkinson is can call in , cheaply , and leave recordings , a few cents each , and I am really happy to announce that we ve already hit six percent of our target \n",
      "Predict2:< It is a , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 37s (- -65m 26s), Epoch: [5/10], Step: [780/3335], Train Loss: 3.499672758579254, BLEU: 0.6171879663939336\n",
      "\n",
      "Input1:> 我们 用 我们 的 双手 获得 获得成功 得成 成功\n",
      "\n",
      "Target1:= We had a victory on our hands  . \n",
      "Predict1:< We we we we , , ,  .  .  .\n",
      "\n",
      "Input2:> 首先 为什么 什么 他们 会 给 我们 留下 这种 坏 印象 呢\n",
      "\n",
      "Target2:= First of all , why do they have such a bad press  ? \n",
      "Predict2:< Why , , we we we we we we to  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 43s (- -65m 21s), Epoch: [5/10], Step: [800/3335], Train Loss: 3.450148856639862, BLEU: 0.6281289895257735\n",
      "\n",
      "Input1:> 居高临下 意味 意味着 我 把 另 一文 文化 文化背景 背景 的 人 当做 我 的 仆人 对待\n",
      "\n",
      "Target1:= UNK , I treat everybody from another culture as if they were my servants  . \n",
      "Predict1:< And I I I I , I , I , I I to  .  .\n",
      "\n",
      "Input2:> 让 铲子 成为 你 选择 的 武器 吧\n",
      "\n",
      "Target2:= And let that be your weapon of choice  . \n",
      "Predict2:< Let is to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 49s (- -65m 15s), Epoch: [5/10], Step: [820/3335], Train Loss: 3.4489348769187926, BLEU: 0.6270855518680273\n",
      "\n",
      "Input1:> 自从 我 足够 大拿 拿起 相机 以来 我 一直 热衷 摄影 但 我 今天 想 和 你们 分享 15 张 我 最 珍爱 的 照片 它们 没有 一张 是 我 拍 的\n",
      "\n",
      "Target1:= Photography has been my passion ever since I was old enough to pick up a camera , but today I want to share with you the 15 most treasured photos of mine , and I didn t take \n",
      "Predict1:< I , I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 我 朋友 跟 我 在 一起\n",
      "\n",
      "Target2:= My friend got to come with me  . \n",
      "Predict2:< I I I I  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 64m 54s (- -65m 10s), Epoch: [5/10], Step: [840/3335], Train Loss: 3.3742238521575927, BLEU: 0.6135610404852604\n",
      "\n",
      "Input1:> 目睹 这样 UNK 的 场景 是 一件 极其 痛苦 的 事\n",
      "\n",
      "Target1:= It is difficult to witness something so overwhelming  . \n",
      "Predict1:< And is is is is is is is  .  .\n",
      "\n",
      "Input2:> 用 你们 研究 的 科学 减去 你们 的 点 句式 和 术语 除以 相关 UNK 内容 就是 要 分享 和 观众 有关 的 内容 再 乘以 你 对 自己 正在 从事 的 这份 神奇 工作 的 激情 它\n",
      "\n",
      "Target2:= Take your science , subtract your bullet points and your jargon , divide by relevance , meaning share what is relevant to the audience , and multiply it by the passion that you have for this incredible work \n",
      "Predict2:< And you you you you you you , and you , and , , , , and , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 0s (- -65m 4s), Epoch: [5/10], Step: [860/3335], Train Loss: 3.4247326135635374, BLEU: 0.6558515999350951\n",
      "\n",
      "Input1:> 我 的 父亲 在 用 他 的 灰色 小 收音 收音机 听 BBC 新闻\n",
      "\n",
      "Target1:= My father was listening to BBC News on his small , gray radio  . \n",
      "Predict1:< My father my , , , , , ,  .  .\n",
      "\n",
      "Input2:> 相反 正如 爱因斯坦 因斯 坦 所说 让 一切 尽可 尽可能 可能 的 简单 而 不是 简化\n",
      "\n",
      "Target2:= Instead , as Einstein said , make everything as simple as possible , but no simpler  . \n",
      "Predict2:< And , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 5s (- -66m 58s), Epoch: [5/10], Step: [880/3335], Train Loss: 3.3251610159873963, BLEU: 0.6324590748530365\n",
      "\n",
      "Input1:> 让 你们 的 想法 简单 单易 易懂 并不等于 不等 等于 简化 它\n",
      "\n",
      "Target1:= And making your ideas accessible is not the same as dumbing it down  . \n",
      "Predict1:< And you you you to is is , is is is  .  .\n",
      "\n",
      "Input2:> 我们 与 客户 在 餐馆 和 酒吧 见面\n",
      "\n",
      "Target2:= We meet at the cafe  . We meet at the pub  . \n",
      "Predict2:< We are the the and and and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 11s (- -66m 53s), Epoch: [5/10], Step: [900/3335], Train Loss: 3.3889285922050476, BLEU: 0.6173630460187637\n",
      "\n",
      "Input1:> 但 不幸 的 是 我 得 告诉 你们 在 2007 年时 一名 纽约 纽约大学 大学 的 博士 在 启发 之下 统计 了 所有 在职 模特 的 人数 在 UNK 名 正式 受雇 模特 中 只有 27 名 也\n",
      "\n",
      "Target1:= But unfortunately I have to inform you that in 2007 , a very inspired NYU Ph .D . student counted all the models on the runway , every single one that was hired , and of the UNK models that \n",
      "Predict1:< But I , you you you , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 今天 我 想 聊 一个 令人 令人不安 不安 的 问题 这个 问题 的 答案 同样 令人 烦扰\n",
      "\n",
      "Target2:= I am here today to talk about a disturbing question , which has an equally disturbing answer  . \n",
      "Predict2:< I I want to to to question question question a a a a a a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 16s (- -66m 47s), Epoch: [5/10], Step: [920/3335], Train Loss: 3.4824897170066835, BLEU: 0.6146305805115853\n",
      "\n",
      "Input1:> 噪音 也 会 消失 在 这些 毛孔 里\n",
      "\n",
      "Target1:= Also the noise will disappear in these pores  . \n",
      "Predict1:< And is are in in  .  .\n",
      "\n",
      "Input2:> 我 现时 住 在 新奥尔良 奥尔良 并 深深 深深地 爱 上 了 这 座 城市\n",
      "\n",
      "Target2:= Now , I live in New Orleans , and I am in love with New Orleans  . \n",
      "Predict2:< I I in in in in in in in in  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 22s (- -66m 41s), Epoch: [5/10], Step: [940/3335], Train Loss: 3.4587595582008364, BLEU: 0.6211085714312984\n",
      "\n",
      "Input1:> 尽管 这 只是 一个 19 世纪 的 报纸 编辑 给出 的 一个 很 好 的 定义 他 的 原话 UNK 是 一个 这样 的 家伙 他 会 为了 谋求 高位 而 不在 不在乎 在乎 任何 政党 任何 舞台\n",
      "\n",
      "Target1:= Although there was a 19th-century newspaper editor who defined it rather better when he said , &quot; A UNK is a fellow who seeks office regardless of party , platform or principle , and who , when he \n",
      "Predict1:< And he is a a a a , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 的 父亲 在 用 他 的 灰色 小 收音 收音机 听 BBC 新闻\n",
      "\n",
      "Target2:= My father was listening to BBC News on his small , gray radio  . \n",
      "Predict2:< My father , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 27s (- -66m 36s), Epoch: [5/10], Step: [960/3335], Train Loss: 3.443846845626831, BLEU: 0.6085096234658252\n",
      "\n",
      "Input1:> 此时 此时此刻 此刻 你 可能 在 想 哇 这 才 神奇 了 或者 喔 她 真 蠢 但是 在 整个 过程 中 事实 事实上 我 在 谈论 的 是 你\n",
      "\n",
      "Target1:= Right now , maybe you are thinking , &quot; Wow , this is fascinating , &quot; or , &quot; Wow , how stupid was she , &quot; but this whole time , I ve actually been talking about \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 之后 经过 三周 的 辩论 最后 参议 参议院 议院 也 没有 屈服\n",
      "\n",
      "Target2:= And after three weeks of debate , in the end the Senate did not cave in  . \n",
      "Predict2:< And the , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 33s (- -66m 30s), Epoch: [5/10], Step: [980/3335], Train Loss: 3.4136322379112243, BLEU: 0.6990799898009937\n",
      "\n",
      "Input1:> 这里 本身 不是 一间 妓院\n",
      "\n",
      "Target1:= This wasn t a brothel , per se  . \n",
      "Predict1:< It is not is  .  .  .\n",
      "\n",
      "Input2:> 他们 也 都 具有 精确 精确性 虽然 这么 说 但是 基于 语音 的 测试 却 不是 专家 UNK 级别 别的\n",
      "\n",
      "Target2:= And they are both accurate  . Okay , but in addition , UNK tests are UNK  . \n",
      "Predict2:< They they t , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 38s (- -66m 25s), Epoch: [5/10], Step: [1000/3335], Train Loss: 3.4292887568473818, BLEU: 0.655427491691162\n",
      "\n",
      "Input1:> 一些 朝鲜 朝鲜人 在 中国 到 外国 国大 大使 大使馆 使馆 去 寻求 庇护 但是 很多 人 被 中国 警察 抓住 并 被 送回 朝鲜\n",
      "\n",
      "Target1:= Some North Koreans in China seek asylum in foreign embassies , but many can be caught by the Chinese police and UNK  . \n",
      "Predict1:< And people , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢 肯尼 肯尼亚 尼亚 UNK 亚语 在 我 的 语言 里 它 的 意思 是 非常 非常感谢 感谢\n",
      "\n",
      "Target2:= UNK UNK  . It means in my language , thank you very much  . \n",
      "Predict2:< Thank you very very very , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 44s (- -66m 19s), Epoch: [5/10], Step: [1020/3335], Train Loss: 3.449284327030182, BLEU: 0.6639071739861475\n",
      "\n",
      "Input1:> 我 在 剑桥 剑桥市 市长 长大 有 一次 我 走进 一家 家商 商店 却 忘 了 UNK 他们 送 了 我 一条 裙子\n",
      "\n",
      "Target1:= I grew up in Cambridge , and one time I went into a store and I forgot my money and they gave me the dress for free  . \n",
      "Predict1:< I I I a I I , I , I , I , , I , I , I I I\n",
      "\n",
      "Input2:> 在 我 的 身后 是 我 父亲 的 奶牛 在 奶牛 的 后边 就是 内罗毕 国家 公园\n",
      "\n",
      "Target2:= Those are my dad is cows at the back , and behind the cows , that is the Nairobi National Park  . \n",
      "Predict2:< And my my my my my my , my , my the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 49s (- -66m 13s), Epoch: [5/10], Step: [1040/3335], Train Loss: 3.299820303916931, BLEU: 0.6489556372482832\n",
      "\n",
      "Input1:> 今天 我 想 聊 一个 令人 令人不安 不安 的 问题 这个 问题 的 答案 同样 令人 烦扰\n",
      "\n",
      "Target1:= I am here today to talk about a disturbing question , which has an equally disturbing answer  . \n",
      "Predict1:< I I I to to to to to a a a a a a problem\n",
      "\n",
      "Input2:> 而且 即使 我们 过 了 这一关 我 的 家人 还是 因 非法 入境 被捕 入狱 了\n",
      "\n",
      "Target2:= But even after we got past the border , my family was arrested and jailed for illegal border crossing  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 65m 55s (- -66m 8s), Epoch: [5/10], Step: [1060/3335], Train Loss: 3.424681544303894, BLEU: 0.6263977210829942\n",
      "\n",
      "Input1:> 中途 不能 休息 不能 吃饭 不能 喝水 由于 极度 的 缺水 他们 基本 基本上 也 不 需要 上 厕所\n",
      "\n",
      "Target1:= There were no breaks for food , no water breaks , and the severe dehydration made urinating pretty much inconsequential  . \n",
      "Predict1:< No , t t t , , , , , , t , t t t t  .  .\n",
      "\n",
      "Input2:> 这 是 南 洛杉矶 笑\n",
      "\n",
      "Target2:= This is South Los Angeles  . \n",
      "Predict2:< This is a  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 0s (- -66m 2s), Epoch: [5/10], Step: [1080/3335], Train Loss: 3.5131307244300842, BLEU: 0.688688280993376\n",
      "\n",
      "Input1:> 我们 试图 去 查明 秃鹫 与 周围 周围环境 环境 的 关系 生态 生态学 并且 了解 秃鹫 的 行踪\n",
      "\n",
      "Target1:= We are trying to determine their basic ecology , and see where they go  . \n",
      "Predict1:< We need to to to the the the , , , , , and the to the\n",
      "\n",
      "Input2:> 你 可以 向 客户 提出 你 的 点子\n",
      "\n",
      "Target2:= You can give somebody an idea  . \n",
      "Predict2:< You can can to the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 6s (- -67m 57s), Epoch: [5/10], Step: [1100/3335], Train Loss: 3.487703490257263, BLEU: 0.7471771680368625\n",
      "\n",
      "Input1:> 我 知道 你们 中 有些 人 可能 对 这个 观点 存疑 你们 可能 会 举出 一些 时尚 圈 人物 例如 Naomi UNK Joan UNK 和 UNK UNK\n",
      "\n",
      "Target1:= And I know there are people in the audience who are skeptical at this point , and maybe there are some fashionistas who are , like , &quot; Wait  . Naomi  . UNK  . Joan UNK  . Liu \n",
      "Predict1:< I think you you you you you you you you , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 由于 感应 UNK 电机 太大 了 拿 不 上来 我 在 这里 就 用 微波 微波炉 演示 了\n",
      "\n",
      "Target2:= Today I use a microwave because I cannot take the big induction machine here onstage  . \n",
      "Predict2:< And , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 12s (- -67m 51s), Epoch: [5/10], Step: [1120/3335], Train Loss: 3.4621851444244385, BLEU: 0.6602056473569701\n",
      "\n",
      "Input1:> 你 有 能力 制止 家庭 家庭暴力 暴力 只 需要 要点 点亮 星星 星星之火\n",
      "\n",
      "Target1:= You have the power to end domestic violence simply by shining a spotlight on it  . \n",
      "Predict1:< You have have to to , , , , , ,  .  .\n",
      "\n",
      "Input2:> 笑 如果 他们 种植 番茄 他们 就 会 吃 番茄 掌声\n",
      "\n",
      "Target2:= If they grow tomatoes , they eat tomatoes  . \n",
      "Predict2:< And they they they they they they they they they  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 17s (- -67m 45s), Epoch: [5/10], Step: [1140/3335], Train Loss: 3.499940884113312, BLEU: 0.6765888106644609\n",
      "\n",
      "Input1:> 我 并 不是 精神 精神病 医生 社会 社会工作 社会工作者 工作 工作者 作者 也 不是 家庭 家庭暴力 暴力 方面 的 专家\n",
      "\n",
      "Target1:= I am not a psychiatrist , a social worker or an expert in domestic violence  . \n",
      "Predict1:< I don not not not , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 我 厌倦 了 这些 现实\n",
      "\n",
      "Target2:= I got tired of seeing this happening  . \n",
      "Predict2:< I ve these  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 23s (- -67m 40s), Epoch: [5/10], Step: [1160/3335], Train Loss: 3.4203924179077148, BLEU: 0.6624838241110024\n",
      "\n",
      "Input1:> 因此 我 做 的 就是 在 我 的 房子 前 种 上 食物 森林\n",
      "\n",
      "Target1:= So what I did , I planted a food forest in front of my house  . \n",
      "Predict1:< So I I my my the the the the the the the the the  .\n",
      "\n",
      "Input2:> 计划 是 给 创业 帮 了 倒忙\n",
      "\n",
      "Target2:= Planning is the kiss of death of entrepreneurship  . \n",
      "Predict2:< It is is ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 28s (- -67m 34s), Epoch: [5/10], Step: [1180/3335], Train Loss: 3.401089978218079, BLEU: 0.6509575759530626\n",
      "\n",
      "Input1:> 据估计 估计 有 超过 4000 名儿 儿童 在 沃尔特 尔特 湖水 水库 被 奴役 那里 是 世界 上 最大 的 人工 人工湖\n",
      "\n",
      "Target1:= It is estimated that more than 4,000 children are enslaved on Lake Volta , the largest man-made lake in the world  . \n",
      "Predict1:< There is is , , , , , , , , , , , , in in\n",
      "\n",
      "Input2:> 我 朋友 跟 我 在 一起\n",
      "\n",
      "Target2:= My friend got to come with me  . \n",
      "Predict2:< I I I I  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 34s (- -67m 29s), Epoch: [5/10], Step: [1200/3335], Train Loss: 3.3221269726753233, BLEU: 0.6699063664763159\n",
      "\n",
      "Input1:> 我们 所 做 的 是 跟 他们 交朋友 朋友 并发 发现 他们 的 兴趣 和 意向\n",
      "\n",
      "Target1:= And what we do , we become friends , and we find out what that person wants to do  . \n",
      "Predict1:< And we we them them and and and and and and and and  .\n",
      "\n",
      "Input2:> 五年 年前 我 曾 有 过 一个 有如 漫游 爱丽丝 仙境 的 经历\n",
      "\n",
      "Target2:= Five years ago , I experienced a bit of what it must have been like to be Alice in Wonderland  . \n",
      "Predict2:< I was a a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 39s (- -67m 23s), Epoch: [5/10], Step: [1220/3335], Train Loss: 3.3250633001327516, BLEU: 0.6207536172971339\n",
      "\n",
      "Input1:> 讲话 会 变得 小声 而且 之后 伴随 更多 气息 音 这些 都 是 症状 的 例子\n",
      "\n",
      "Target1:= The speech actually becomes quieter and more UNK after a while , and that is one of the example symptoms of it  . \n",
      "Predict1:< And are are the , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 当 那个 中国 警官 快 检查 查到 我家 家人 时 我 本能 地 站起 起来 告诉 他 这 几位 是 UNK UNK 我 是 他们 的 监护 监护人 护人\n",
      "\n",
      "Target2:= As the Chinese officer approached my family , I UNK stood up , and I told him that these are deaf and dumb people that I was UNK  . \n",
      "Predict2:< So when when , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 45s (- -67m 18s), Epoch: [5/10], Step: [1240/3335], Train Loss: 3.483081543445587, BLEU: 0.6180124661448333\n",
      "\n",
      "Input1:> 由此 开始 了 我 深入 现代 奴隶 奴隶制 奴隶制度 制度 的 旅程\n",
      "\n",
      "Target1:= Thus began my journey into modern day slavery  . \n",
      "Predict1:< And then I I , I , I  .  .\n",
      "\n",
      "Input2:> 我 这儿 有些 现在 能 拿出 出来 来说 的 伟大 的 目标\n",
      "\n",
      "Target2:= So here are some amazing goals that I think we can deal with now  . \n",
      "Predict2:< I I I to to to the , , the  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 50s (- -67m 12s), Epoch: [5/10], Step: [1260/3335], Train Loss: 3.4559252738952635, BLEU: 0.6686304473526731\n",
      "\n",
      "Input1:> 或许 它 正 发生 生在 你 的 身上\n",
      "\n",
      "Target1:= It may even be your story  . \n",
      "Predict1:< And is is you  .  .  .\n",
      "\n",
      "Input2:> 然后 你 要为 他们 提供 专注 而 热情 的 服务\n",
      "\n",
      "Target2:= Then you have to offer them absolute , dedicated , passionate service to them  . \n",
      "Predict2:< And you you to to to the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 66m 56s (- -67m 6s), Epoch: [5/10], Step: [1280/3335], Train Loss: 3.4436222314834595, BLEU: 0.62565970380578\n",
      "\n",
      "Input1:> 为了 改变 世界 我们 迫切 迫切需要 需要 从 我们 的 科学 科学家 学家 和 工程 工程师 那里 得到 良好 的 沟通\n",
      "\n",
      "Target1:= We desperately need great communication from our scientists and engineers in order to change the world  . \n",
      "Predict1:< And , , to to to and and and and and and and and and and\n",
      "\n",
      "Input2:> 因为 它 有 很多 空洞 相互 抵消 了 噪音 所以 路 就 很 安静\n",
      "\n",
      "Target2:= Because it is very hollow , all the noise will disappear , so it is a very silent road  . \n",
      "Predict2:< Because it is a a , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 1s (- -67m 1s), Epoch: [5/10], Step: [1300/3335], Train Loss: 3.4073214292526246, BLEU: 0.6324965045245764\n",
      "\n",
      "Input1:> 我们 能 减少 和 病人 有关 的 后勤 问题\n",
      "\n",
      "Target1:= We can reduce logistical difficulties with patients  . \n",
      "Predict1:< We can we to the the  .  .\n",
      "\n",
      "Input2:> 但 让 我 坚定 坚定信念 信念 的 是 即使 现实 如此 就算 他们 不 被 当作 正常 正常人 常人 看 这 也 只 说明 了 一件 事 他们 是非 非凡 的 虽然 患有 自闭 自闭症 但 非凡 无比\n",
      "\n",
      "Target2:= But what lifted my heart and strengthened my soul was that even though this was the case , although they were not seen as ordinary , this could only mean one thing : that they were extraordinary -- \n",
      "Predict2:< But I I is is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 7s (- -68m 55s), Epoch: [5/10], Step: [1320/3335], Train Loss: 3.4071653485298157, BLEU: 0.5983279608771045\n",
      "\n",
      "Input1:> 从 1971 年 到 1977 年 我 看起 看起来 起来 很 年轻 但 其实 我 老 了 我 曾 在 赞比亚 比亚 肯尼 肯尼亚 尼亚 象牙 象牙海岸 海岸 阿尔及利亚 利亚 索马里 马里 参与 过 不少 与非 非洲 各国\n",
      "\n",
      "Target1:= From 1971 to 1977 -- I look young , but I am not — -- I worked in Zambia , Kenya , Ivory Coast , Algeria , Somalia , in projects of technical cooperation with African countries  . \n",
      "Predict1:< And years years , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 为了 让 我 的 家人 获得 自由 倾尽 所能 然而 就 在 快要 成功 的 时候 家人 却 又 要 经历 牢狱 牢狱之灾 之灾 并且 这个 监狱 离 朝鲜 大使 大使馆 使馆 并 不远\n",
      "\n",
      "Target2:= I did everything to get my family to freedom , and we came so close , but my family was thrown in jail just a short distance from the South Korean embassy  . \n",
      "Predict2:< I I to to to to the the , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 12s (- -68m 50s), Epoch: [5/10], Step: [1340/3335], Train Loss: 3.578267824649811, BLEU: 0.5929706820852775\n",
      "\n",
      "Input1:> 我 不知 知道 那 意味 意味着 什么 但是 我 能 看出 父亲 非常 非常 非常高兴 高兴\n",
      "\n",
      "Target1:= I didn t know what it meant , but I could see that my father was very , very happy  . \n",
      "Predict1:< I I t know that I I , , , , , , it , it  .\n",
      "\n",
      "Input2:> 我 经常 思考 为什么 什么 他们 有 灯火 而 我们 没有\n",
      "\n",
      "Target2:= I always wondered why they had lights but we didn t  . \n",
      "Predict2:< I I t , , , , we , we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 18s (- -68m 44s), Epoch: [5/10], Step: [1360/3335], Train Loss: 3.56934015750885, BLEU: 0.6765340761331577\n",
      "\n",
      "Input1:> 我 当时 也 不知 知道 第二 第二步 二步 是 孤立 受害 受害者\n",
      "\n",
      "Target1:= I also didn t know that the second step is to isolate the victim  . \n",
      "Predict1:< I I t know , , , , , , ,  .\n",
      "\n",
      "Input2:> 这 是 一家 一家人 家人 的 肖像\n",
      "\n",
      "Target2:= This is a family portrait  . \n",
      "Predict2:< This is a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 23s (- -68m 39s), Epoch: [5/10], Step: [1380/3335], Train Loss: 3.425290524959564, BLEU: 0.6740989911074736\n",
      "\n",
      "Input1:> 单调 调和 疲惫 消磨 着 他们 他们 安静 着 劳动 着 一遍 又 一遍 的 干 着 同样 的 活 一天 16 到 17 个 小时\n",
      "\n",
      "Target1:= UNK by monotony and exhaustion , they work silently , doing this task over and over for 16 or 17 hours a day  . \n",
      "Predict1:< The , , , , , , , , , , , , , , , , , , the the\n",
      "\n",
      "Input2:> 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 29s (- -68m 33s), Epoch: [5/10], Step: [1400/3335], Train Loss: 3.3850125074386597, BLEU: 0.6729634753233761\n",
      "\n",
      "Input1:> 当 狮子 晚上 走近 的 时候 看到 的 就是 这个 样子\n",
      "\n",
      "Target1:= And that is how it looks to lions when they come at night  . \n",
      "Predict1:< And when when when the the the the the the the the  .\n",
      "\n",
      "Input2:> 而且 这 也 是 为什么 什么 参议 参议院 议院 反对 这个 个头 头衔\n",
      "\n",
      "Target2:= And that is why the Senate objected to it  . \n",
      "Predict2:< And this is is the the , , the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 34s (- -68m 27s), Epoch: [5/10], Step: [1420/3335], Train Loss: 3.219499480724335, BLEU: 0.6490699610760907\n",
      "\n",
      "Input1:> 我 读 过 一个 当时 可怜 议员 的 日记 里面 持续 的 记录 着 竟然 又 是 这个 议题\n",
      "\n",
      "Target1:= I read a diary of this poor senator , who just keeps coming back , &quot; Still on this subject  . &quot; \n",
      "Predict1:< I I a a a a , , , , , , , a  .\n",
      "\n",
      "Input2:> 他们 甚至 并 没有 延续 君主 君主制 他们 的 主意 是 你 可以 以是 选举 的 国王 有 一个 固定 的 任期\n",
      "\n",
      "Target2:= And they weren t even being UNK there , they had the idea that you could be elected King for a fixed term  . \n",
      "Predict2:< They they t t a , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 40s (- -68m 22s), Epoch: [5/10], Step: [1440/3335], Train Loss: 3.524367356300354, BLEU: 0.65308140933285\n",
      "\n",
      "Input1:> 我 并不知道 不知 知道 他 是 在 虐待 我\n",
      "\n",
      "Target1:= I didn t know he was abusing me  . \n",
      "Predict1:< I I t know know , I  .  .  .  .\n",
      "\n",
      "Input2:> 我 真心 心地 感谢 他 并 问道 你 为什么 什么 要 帮助 我\n",
      "\n",
      "Target2:= I thanked him with all my heart , and I asked him , &quot; Why are you helping me  ? &quot; \n",
      "Predict2:< I I to , , , , , , you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 46s (- -68m 16s), Epoch: [5/10], Step: [1460/3335], Train Loss: 3.442322278022766, BLEU: 0.6273468822396928\n",
      "\n",
      "Input1:> 在 死 之前 我 想 在 上百 上百万 百万 的 观众 面前 唱歌\n",
      "\n",
      "Target1:= &quot; Before I die , I want to sing for millions  . &quot; \n",
      "Predict1:< In , , I I I I , , I to to\n",
      "\n",
      "Input2:> 所以 我 找到 了 一个 个旧 的 车用 蓄电 蓄电池 电池 一个 指示 指示器 这 是 一个 从 摩托 摩托车 托车 中 找到 的 小 装备 用来 控制 摩托 摩托车 托车 的 转向 转向灯 能够 让 灯 闪烁\n",
      "\n",
      "Target2:= So I got an old car battery , an indicator box  . It is a small device found in a motorcycle , and it helps motorists when they want to turn right or left  . It blinks  . \n",
      "Predict2:< So I I to a a a a a a , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 51s (- -68m 11s), Epoch: [5/10], Step: [1480/3335], Train Loss: 3.4001275300979614, BLEU: 0.6483624800178782\n",
      "\n",
      "Input1:> 当 你 走出 这个 房间 的 时候 你们 就是 秃鹫 达人 了 不但 但要 告诉 你 的 家人 人们 还要 告诉 你们 的 邻居 邻居们 有关 秃鹫 的 故事\n",
      "\n",
      "Target1:= When you walk out of this room , you will be informed about vultures , but speak to your families , to your children , to your neighbors about vultures  . \n",
      "Predict1:< When you you you you you you , , , , , , you , , you , , you , you you you\n",
      "\n",
      "Input2:> 我们 聊起 了 奴隶 奴隶制 奴隶制度 制度 而 此时 我 才 开始 真正 了解 奴役 的 现实 虽然 我 一直 都 知道 世上 奴隶 奴隶制 尚存 但 完全 不知 知道 事态 的 严重 严重性\n",
      "\n",
      "Target2:= We started talking about slavery , and really , I started learning about slavery , for I had certainly known it existed in the world , but not to such a degree  . \n",
      "Predict2:< We know t the the , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 67m 57s (- -68m 5s), Epoch: [5/10], Step: [1500/3335], Train Loss: 3.34035187959671, BLEU: 0.662278064353662\n",
      "\n",
      "Input1:> 我 甚至 将 我 的 朋友 带回 我 的 社区 帮助 他们 在 没有 赶 狮 灯 的 人家 家里 里安 安装 这个 我 教 他们 怎么 安装\n",
      "\n",
      "Target1:= I even took my friends back to my community , and we are installing the lights to the homes which don t have &#91; any &#93; , and I am teaching them how to put them  . \n",
      "Predict1:< I I my to my , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 谁 还 会 认真 认真对待 真对 对待 他 如果 他 只有 一个 这样 愚蠢 的 小头 头衔 比如 美利 美利坚 联邦 联邦政府 邦政府 政府 主持 UNK of the United UNK of America\n",
      "\n",
      "Target2:= And who is going to take him seriously if he is got a silly little title like President of the United States of America  ? \n",
      "Predict2:< And he he to , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 2s (- -68m 0s), Epoch: [5/10], Step: [1520/3335], Train Loss: 3.4071396827697753, BLEU: 0.6521779737547969\n",
      "\n",
      "Input1:> 事实 事实上 我 的 学生 UNK 我 得 提 一下 他 因为 他 做出 一些 出色 的 成果 现在 他 已经 证明 了 在 移动 移动电话 电话 电话网 网上 也 能 进行 这个 项目 所以 现在 我们 接近\n",
      "\n",
      "Target1:= Actually , my student UNK , I have to plug him , because he is done some fantastic work , and now he has proved that it works over the mobile telephone network as well , which enables \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 拒绝 成为 由 别人 人为 我 打造 的 这种 现实 的 一部 一部分 部分 我 要 自己 创造 自己 的 现实\n",
      "\n",
      "Target2:= And I refuse to be a part of this manufactured reality that was manufactured for me by some other people , and I am manufacturing my own reality  . \n",
      "Predict2:< I I I to to I I I I I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 8s (- -69m 54s), Epoch: [5/10], Step: [1540/3335], Train Loss: 3.371313285827637, BLEU: 0.6732247399327592\n",
      "\n",
      "Input1:> 我们 很 害怕 但是 我们 仍然 渴望 望去 学校\n",
      "\n",
      "Target1:= We were scared , but still , school was where we wanted to be  . \n",
      "Predict1:< We we we we we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 只是 说明 对 有 不同 同种 种类\n",
      "\n",
      "Target2:= It just means that there is a different kind of right  . \n",
      "Predict2:< And is a a a  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 13s (- -69m 48s), Epoch: [5/10], Step: [1560/3335], Train Loss: 3.5011670708656313, BLEU: 0.7590929435768247\n",
      "\n",
      "Input1:> 我 想到 的 第一 第一个 一个 办法 是 使用 火 因为 我 知道 狮子 们 都 惧怕 火\n",
      "\n",
      "Target1:= And the first idea I got was to use fire , because I thought lions were scared of fire  . \n",
      "Predict1:< I I was , I , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 27 岁 时 我 做出 一个 决定 只 基于 人们 的 需求 提供 提供援助 援助 我 创立 了 一套 工作 系统 叫 企业 梦工厂 工厂 我们 不 做 创始 创始人 也 不 激励 任何 任何人 何人 而是 为\n",
      "\n",
      "Target2:= I decided when I was 27 years old to only respond to people , and I invented a system called Enterprise UNK , where you never initiate anything , you never motivate anybody , but you become a \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 19s (- -69m 43s), Epoch: [5/10], Step: [1580/3335], Train Loss: 3.452900528907776, BLEU: 0.7163947849807633\n",
      "\n",
      "Input1:> 有时 有时候 时候 路面 面会 出现 坑 槽\n",
      "\n",
      "Target1:= Sometimes you can create potholes with that  . \n",
      "Predict1:< And , , , , ,  .  .\n",
      "\n",
      "Input2:> 然后 我 想 酷 就 这样 吧\n",
      "\n",
      "Target2:= And I was like , &quot; Cool  . Bring it  . &quot; \n",
      "Predict2:< And I I to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 24s (- -69m 37s), Epoch: [5/10], Step: [1600/3335], Train Loss: 3.297061097621918, BLEU: 0.6107758392085006\n",
      "\n",
      "Input1:> 而 我 永远 不会 再 拥有 的 永远 是 一个 嘴里 说 爱 我 的 人 用 上膛 了 的 枪 指着 我 的 脑袋\n",
      "\n",
      "Target1:= What I will never have again , ever , is a loaded gun held to my head by someone who says that he loves me  . \n",
      "Predict1:< And I I I , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 你 必须 得到 诊所 去 而且 花费 相当 相当 大 这 也 意味 意味着 在 临床 临床试验 试验 之外 从来 从来没 没人 做 过 这个 测试 从来 从来没 没有\n",
      "\n",
      "Target2:= You have to go to the clinic to do it  . It is very , very costly , and that means that , outside the clinical trials , it is just never done  . It is never done \n",
      "Predict2:< You you t to to , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 30s (- -69m 32s), Epoch: [5/10], Step: [1620/3335], Train Loss: 3.4645060896873474, BLEU: 0.676663903628593\n",
      "\n",
      "Input1:> 不过 随着 对话 的 展开 我 的 经历 就 如 爱丽丝 一样 顺着 那个 兔子 洞 下去 看到 了 那 扇 通往 一个 全新 新世界 世界 的 大门\n",
      "\n",
      "Target1:= But as these conversations unfolded , I experienced what Alice must have when she went down that rabbit hole and saw that door to a whole new world  . \n",
      "Predict1:< But the the the , , , , , the the , the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 在 初次 交往 中 康 纳 做 的 最 聪明 的 事情 是 让 我 相信 在 这段 感情 中 我 是 强势 的 一方\n",
      "\n",
      "Target2:= One of the smartest things UNK did , from the very beginning , was to create the illusion that I was the dominant partner in the relationship  . \n",
      "Predict2:< In the the the , the , I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 35s (- -69m 26s), Epoch: [5/10], Step: [1640/3335], Train Loss: 3.3940698742866515, BLEU: 0.6563812134606907\n",
      "\n",
      "Input1:> 回到 UNK 我 忍不住 不住 想 哭 但是 身旁 的 废 奴 主义 同伴 迅速 地拉 拉住 了 我 他 说 Lisa 别哭 别 在 这里 哭\n",
      "\n",
      "Target1:= Back in the kilns , I wanted to cry , but the UNK next to me quickly grabbed me and he said , &quot; Lisa , don t do that  . Just don t do that here  . \n",
      "Predict1:< And I , to to , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 看到 似乎 是 一家 一家人 家人 在 船上 捕鱼 两个 哥哥 几个 小点 的 孩子 挺 像 的 吧\n",
      "\n",
      "Target2:= I saw what seemed to be a family fishing on a boat , two older brothers , some younger kids , makes sense right  ? \n",
      "Predict2:< I I was a a , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 41s (- -69m 21s), Epoch: [5/10], Step: [1660/3335], Train Loss: 3.5006507515907286, BLEU: 0.6687622313351383\n",
      "\n",
      "Input1:> 今天 的 奴隶 奴隶制 奴隶制度 制度 关乎 商业 商业利益 利益 奴隶 奴隶制 制造 的 商品 具有 价值 但是 创造 造价 价值 的 人 却是 被 随意 处置 的\n",
      "\n",
      "Target1:= Today is slavery is about commerce , so the goods that enslaved people produce have value , but the people producing them are disposable  . \n",
      "Predict1:< Today , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 而且 即使 我们 过 了 这一关 我 的 家人 还是 因 非法 入境 被捕 入狱 了\n",
      "\n",
      "Target2:= But even after we got past the border , my family was arrested and jailed for illegal border crossing  . \n",
      "Predict2:< And we we , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 46s (- -69m 15s), Epoch: [5/10], Step: [1680/3335], Train Loss: 3.4272173047065735, BLEU: 0.7462860190548702\n",
      "\n",
      "Input1:> 内罗毕 国家 公园 的 南部 并 没有 全部 围起 栅栏 这 就 意味 意味着 像 斑马 这样 的 野生 野生动物 生动 动物 可以 自由 地 在 公园 园外 UNK 移动\n",
      "\n",
      "Target1:= Nairobi National Park is not UNK in the south widely , which means wild animals like zebras migrate out of the park freely  . \n",
      "Predict1:< The the the the the , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 的 意思 是 当然 啦 他们 怎么 能 不 这么 做 呢\n",
      "\n",
      "Target2:= I mean , come on , why wouldn t they  ? \n",
      "Predict2:< I mean , , , they , they they they they  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 52s (- -69m 10s), Epoch: [5/10], Step: [1700/3335], Train Loss: 3.4793603897094725, BLEU: 0.6849966064229248\n",
      "\n",
      "Input1:> 爱 哲 顿 博士 用 这 张 子弹 击穿 苹果 的 照片 激起 了 我们 的 敬佩 和 好奇 这 张 照片 的 曝光 时间 只有 一百 一百万 百万 百万分之一 万分 万分之 万分之一 之一 一秒\n",
      "\n",
      "Target1:= Doc Edgerton inspired us with awe and curiosity with this photo of a bullet piercing through an apple , and exposure just a millionth of a second  . \n",
      "Predict1:< The is the , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 147 个\n",
      "\n",
      "Target2:= A hundred and UNK  . \n",
      "Predict2:< It :  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68m 57s (- -69m 4s), Epoch: [5/10], Step: [1720/3335], Train Loss: 3.4092456102371216, BLEU: 0.652002965745161\n",
      "\n",
      "Input1:> 让 人 震惊 的 是 奴隶 在 全球 产生 的 利润 每年 超过 130 亿美金 美金\n",
      "\n",
      "Target1:= Astonishingly , slavery generates profits of more than $ 13 billion worldwide each year  . \n",
      "Predict1:< The is is the the the the the the the the the the the\n",
      "\n",
      "Input2:> 但 如果 你 在 看 一下 这个 西红柿 你 就 会 注意 到 当 光 在 西红柿 上 走过 过时 它 一直 在 闪耀 它 并 没有 有变 变暗\n",
      "\n",
      "Target2:= But if you look at this tomato one more time , you will notice , as the light washes over the tomato , it continues to glow  . It doesn t become dark  . \n",
      "Predict2:< But if you you you , , , , , , , , , , , , , , , , , , , it ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 3s (- -70m 59s), Epoch: [5/10], Step: [1740/3335], Train Loss: 3.416345810890198, BLEU: 0.6982046350730878\n",
      "\n",
      "Input1:> 尽管 她们 被 抓住 了 但是 UNK 巨大 的 国际 UNK 舆论 舆论压力 压力 她们 最终 被 释放 了\n",
      "\n",
      "Target1:= Even though they were caught , they were eventually released after heavy international pressure  . \n",
      "Predict1:< And they they , , , , , , , , , , , they , they  .\n",
      "\n",
      "Input2:> 我 并 不是 在 帮助 你 他 回答\n",
      "\n",
      "Target2:= &quot; I am not helping you , &quot; he said  . \n",
      "Predict2:< I don t t t you  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 8s (- -70m 53s), Epoch: [5/10], Step: [1760/3335], Train Loss: 3.3518608808517456, BLEU: 0.6917839910127637\n",
      "\n",
      "Input1:> 我们 的 手工 清洗 活动 非常 顺利\n",
      "\n",
      "Target1:= Now , we were lucky with our UNK  . \n",
      "Predict1:< Our is our our  .  .  .\n",
      "\n",
      "Input2:> 正如 我 的 队长 维 恩 曾 说 这 就 像 在 对 人 进行 纹身 一样\n",
      "\n",
      "Target2:= As my team leader UNK once said , it is like doing a tattoo on someone  . \n",
      "Predict2:< And , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 14s (- -70m 47s), Epoch: [5/10], Step: [1780/3335], Train Loss: 3.446413815021515, BLEU: 0.6650722537919312\n",
      "\n",
      "Input1:> 在 喜马拉 喜马拉雅 马拉 马拉雅 我 见到 了 做 苦力 的 小孩 背着 石头 走上 几公里 公里 的 山路 到 路边 等 着 的 卡车 旁\n",
      "\n",
      "Target1:= In the Himalayas , I found children carrying stone for miles down UNK terrain to trucks waiting at roads below  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当 我们 处于 第一 第一次 一次 工业 工业革命 革命 的 末期 时 我们 使用 不可 可再生 再生 矿物 燃料 发展 制造 制造业 但 我们 忽然 意识 到 这种 发展 是 不可 持续 的\n",
      "\n",
      "Target2:= We are at the end of the first industrial revolution -- UNK fossil fuels , manufacturing -- and all of a sudden , we have systems which are not sustainable  . \n",
      "Predict2:< When we we we the the , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 19s (- -70m 42s), Epoch: [5/10], Step: [1800/3335], Train Loss: 3.427246558666229, BLEU: 0.6423303413410918\n",
      "\n",
      "Input1:> 我们 做 过 调查 对 世界 上 100 家 标杆 企业 进行 研究 卡内 卡内基 培训 公司 西屋 公司 爱迪生 公司 福特 福特汽车公司 汽车 公司 以及 所有 这些 新兴 新兴产业 产业 的 公司 如 谷 歌 雅虎\n",
      "\n",
      "Target1:= We ve done the research , and we have looked at the 100 iconic companies of the world -- Carnegie , Westinghouse , Edison , Ford , all the new companies , Google , Yahoo  . \n",
      "Predict1:< We ve we the the the the the , , , , , , , , , , , , , , , , , , , , and , , the\n",
      "\n",
      "Input2:> 这 不是 所有 所有人 有人 都 能 做 得到 的\n",
      "\n",
      "Target2:= Not everybody gets to do that  . \n",
      "Predict2:< It is not the of  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 25s (- -70m 36s), Epoch: [5/10], Step: [1820/3335], Train Loss: 3.4387010455131533, BLEU: 0.6209813253872584\n",
      "\n",
      "Input1:> 当 他 唱起 我们 儿时 的 歌谣 唱出 就 连 我 都 忘记 了 的 歌词 他 提醒 了 我 我们 对 精神 知之甚少 甚少 而 未知 的 事物 是 如此 迷人\n",
      "\n",
      "Target1:= When he sings songs from our childhood , attempting words that not even I could remember , he reminds me of one thing : how little we know about the mind , and how wonderful the unknown must \n",
      "Predict1:< And when when when , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 如果 摄影 摄影师 就 在 这里 灯光 就 在 这里 比如 漂亮 的 水银 UNK UNK 客户 说 Cameron 我 想要 一张 走路 时 的 快门 照 这条 修长 的 美腿 先 迈步 这条 手臂 放在 后面\n",
      "\n",
      "Target2:= So if the photographer is right there and the light is right there , like a nice UNK , and the client says , &quot; Cameron , we want a walking shot , &quot; well then this leg \n",
      "Predict2:< So if if if , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 30s (- -70m 31s), Epoch: [5/10], Step: [1840/3335], Train Loss: 3.3751016736030577, BLEU: 0.6984304894996357\n",
      "\n",
      "Input1:> 你们 能 做 些 什么 呢 你们 可以 行动 动起 起来 制造 声势 你们 可以 写信 写信给 你们 的 政府 并且 告诉 他们 我们 需要 去 关注 这些 被 严重 误解 的 生物 花 些 时间 告诉 你\n",
      "\n",
      "Target1:= How can you help  ? You can become active , make noise  . You can write a letter to your government and tell them that we need to focus on these very misunderstood creatures  . UNK your time \n",
      "Predict1:< You can can you can , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 掌声 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 36s (- -70m 25s), Epoch: [5/10], Step: [1860/3335], Train Loss: 3.293307197093964, BLEU: 0.6696296682068678\n",
      "\n",
      "Input1:> 我 本 以为 只有 我们 意大利 意大利人 大利 利人 在 非洲 栽 了 跟头 但 当 我 知道 美国 国人 的 情况 英国 英国人 国人 的 情况 法国 法国人 国人 的 情况 当 看 了 他们 的 所作\n",
      "\n",
      "Target1:= I thought it was only us Italians blundering around Africa , but then I saw what the Americans were doing , what the English were doing , what the French were doing , and after seeing what they \n",
      "Predict1:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 告诉 你们 一个 秘密\n",
      "\n",
      "Target2:= Let me tell you a secret  . \n",
      "Predict2:< I I you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 42s (- -70m 20s), Epoch: [5/10], Step: [1880/3335], Train Loss: 3.3833961367607115, BLEU: 0.6514456461062561\n",
      "\n",
      "Input1:> 你 不能 有 任何 UNK 否则 结果 将 一团 一团糟\n",
      "\n",
      "Target1:= You don t get a chance to mess it up  . \n",
      "Predict1:< You have t to to you , ,  .  .\n",
      "\n",
      "Input2:> 我 想 通过 一个 叫做 回扣 的 作品 来看 看看 在 阿富汗 的 腐败 并且 模拟 一个 警察 警察局 官员 在 喀布尔 布尔 的 大街 街上 建立 一个 假 的 检查 检查站 然后 把 那些 车 停下 停下来\n",
      "\n",
      "Target2:= I want to look into corruption in Afghanistan through a work called &quot; UNK &quot; and impersonate a police officer , set up a fake UNK on the street of Kabul and stop cars , but instead of \n",
      "Predict2:< I I to to to a a a a a a a a a a a a a , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 47s (- -70m 14s), Epoch: [5/10], Step: [1900/3335], Train Loss: 3.462352466583252, BLEU: 0.6263958025983591\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 全世界 世界 都 将 会 有 很大 的 损失 如果 失去 了 这些 奇妙 的 物种\n",
      "\n",
      "Target1:= Kenya , this world , will be much poorer without these wonderful species  . \n",
      "Predict1:< And the , , , world the world the world the world the world world the\n",
      "\n",
      "Input2:> 谢谢 你 Tom 掌声\n",
      "\n",
      "Target2:= Thanks , Tom  . \n",
      "Predict2:< Thank you you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 53s (- -70m 9s), Epoch: [5/10], Step: [1920/3335], Train Loss: 3.413485527038574, BLEU: 0.6631202038399593\n",
      "\n",
      "Input1:> 我 嫁给 我 的 第二 UNK 丈夫 UNK 差不多 不多 20 年 了 并 育 有 3 个 孩子\n",
      "\n",
      "Target1:= I ve been married for almost 20 years to my second husband and we have three kids together  . \n",
      "Predict1:< I I my , I , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 如果 你 想 现在 就 可以 杀 了 我 他 说 但 我 不会 因 你 落后 的 思想 而 毁 了 我 女儿 的 未来\n",
      "\n",
      "Target2:= &quot; Kill me now , if you wish , &quot; he said , &quot; but I will not ruin my daughter is future because of your old and backward ideas  . &quot; \n",
      "Predict2:< If you you to to , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 69m 58s (- -70m 3s), Epoch: [5/10], Step: [1940/3335], Train Loss: 3.4228247880935667, BLEU: 0.7058692752179859\n",
      "\n",
      "Input1:> 我们 很 害怕 但是 我们 仍然 渴望 望去 学校\n",
      "\n",
      "Target1:= We were scared , but still , school was where we wanted to be  . \n",
      "Predict1:< We we t we we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 我们 是 一个 自由 的 组织 由来 来自 不同 同行 同行业 行业 不同 同城 城市 的 园丁 组成 大家 全都 是 志愿 志愿者 愿者 我们 所 做 的 一切 一切都是 义务 的\n",
      "\n",
      "Target2:= What we do , we are a UNK kind of group , where it is composed of gardeners from all walks of life , from all over the city , and it is completely volunteer , and everything \n",
      "Predict2:< We are a a a of , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 4s (- -71m 57s), Epoch: [5/10], Step: [1960/3335], Train Loss: 3.4957590818405153, BLEU: 0.6565807676023308\n",
      "\n",
      "Input1:> 当 狮子 晚上 走近 的 时候 看到 的 就是 这个 样子\n",
      "\n",
      "Target1:= And that is how it looks to lions when they come at night  . \n",
      "Predict1:< When when when the the the the , , is is  .  .\n",
      "\n",
      "Input2:> 为什么 什么 因为 他们 看着 图表 上 的 曲线 说 如果 人口 继续 以 这种 速度 增长 纽约 的 人们 出行 将 需要 六百 六百万 百万 UNK 马 而 六百 六百万 百万 UNK 马 所 排泄 的 粪便\n",
      "\n",
      "Target2:= Why  ? Because they looked at the curve and said , if the population keeps growing at this rate , to move the population of New York around , they would have needed six million horses , and \n",
      "Predict2:< Because if , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 9s (- -71m 52s), Epoch: [5/10], Step: [1980/3335], Train Loss: 3.380850911140442, BLEU: 0.7115855259112083\n",
      "\n",
      "Input1:> 接近 女儿 三岁 生日 时 妻子 说 嘿 你 为什么 什么 不 带 萨比 娜 回 纽约 来 个 父女 之 旅 继续 这个 小传 传统\n",
      "\n",
      "Target1:= Approaching my daughter is third birthday , my wife said , &quot; Hey , why don t you take UNK back to New York and make it a father-daughter trip , and continue the ritual  ? &quot; \n",
      "Predict1:< Now , , , , , , , , , , , , , , , , , , , , , , , , , &quot;\n",
      "\n",
      "Input2:> 我 搬到 了 纽约 开始 了 自己 第一 第一份 一份 工作 在 UNK 杂志 杂志社 当 编辑 和 撰稿 撰稿人\n",
      "\n",
      "Target2:= I had moved to New York City for my first job as a writer and editor at Seventeen magazine  . \n",
      "Predict2:< I went went the and , and , and , , , and , and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 15s (- -71m 46s), Epoch: [5/10], Step: [2000/3335], Train Loss: 3.4685551285743714, BLEU: 0.6350859041243699\n",
      "\n",
      "Input1:> 顺便 提下 如果 去 神经 神经科 诊所 做 的话 这个 费用 要 300 美金\n",
      "\n",
      "Target1:= Takes about $ 300 , by the way , in the neurologist is clinic to do it  . \n",
      "Predict1:< And if , , , , , , , to to to to\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢 肯尼 肯尼亚 尼亚 UNK 亚语 在 我 的 语言 里 它 的 意思 是 非常 非常感谢 感谢\n",
      "\n",
      "Target2:= UNK UNK  . It means in my language , thank you very much  . \n",
      "Predict2:< Thank you very very , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 20s (- -71m 41s), Epoch: [5/10], Step: [2020/3335], Train Loss: 3.4198631763458254, BLEU: 0.6712383397798127\n",
      "\n",
      "Input1:> 如果 我们 能 让 这种 材料 自我 修复 就 有 可能 有 解决 解决方案 方案\n",
      "\n",
      "Target1:= If we can make this material UNK , then probably we have a solution  . \n",
      "Predict1:< If we can to to , , , , , , ,\n",
      "\n",
      "Input2:> 大家 都 想 问 的 另 一个 问题 是 为什么 什么 她 不离 离开\n",
      "\n",
      "Target2:= The other question everybody asks is , why doesn t she just leave  ? \n",
      "Predict2:< How , , , , , , ,  ?  ?  ?  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 26s (- -71m 35s), Epoch: [5/10], Step: [2040/3335], Train Loss: 3.4045624852180483, BLEU: 0.6856106543302076\n",
      "\n",
      "Input1:> 他 出身 阿富汗 边远 边远地区 地区 有着 与 他人 不同 的 见解 他 坚持 让 他 的 女儿 我 的 母亲 去 上学 并 因此 被迫 与 他 的 父亲 断绝 父子 父子关系 子关 关系\n",
      "\n",
      "Target1:= A total maverick from a remote province of Afghanistan , he insisted that his daughter , my mom , go to school , and for that he was UNK by his father  . \n",
      "Predict1:< He had to to to , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 第一 第一次 一次 有 了 自己 的 房子 公寓 有 了 第一 第一张 一张 信用 信用卡 我 还有 一个 非常 大 的 秘密\n",
      "\n",
      "Target2:= I had my first apartment , my first little green American Express card , and I had a very big secret  . \n",
      "Predict2:< I was a a a , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 31s (- -71m 30s), Epoch: [5/10], Step: [2060/3335], Train Loss: 3.37219694852829, BLEU: 0.6905906327305207\n",
      "\n",
      "Input1:> 有人 指控 我 是 朝鲜 朝鲜人 所以 警察 测试 了 我 的 汉语 能力 并且 问 了 我 很多 问题\n",
      "\n",
      "Target1:= Someone had accused me of being North Korean , so they tested my Chinese language abilities and asked me tons of questions  . \n",
      "Predict1:< I , , I , I I I I I I , I I I , I I I I  .\n",
      "\n",
      "Input2:> 这 就是 我 的 遗产 这 就是 我 一直 在 兑现 的 遗产\n",
      "\n",
      "Target2:= And this is a legacy that was built for me , and it is a legacy that I ve been UNK out on  . \n",
      "Predict2:< This is is my my I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 37s (- -71m 24s), Epoch: [5/10], Step: [2080/3335], Train Loss: 3.457511568069458, BLEU: 0.6333994601940084\n",
      "\n",
      "Input1:> 这个 本来 被 遗忘 的 建筑 变成 了 一个 极具 建设 建设性 的 地方 这些 人 的 希望 和 梦想 让 我 放声 大笑 也 黯然 落泪 也 曾 在 我 经历 困境 的 时候 给 我 安慰\n",
      "\n",
      "Target1:= So this neglected space became a constructive one , and people is hopes and dreams made me laugh out loud , tear up , and they consoled me during my own tough times  . \n",
      "Predict1:< And this a a , , , , and , and , and , , and , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 他们 又 跳进 跳进来 进来 杀死 了 动物\n",
      "\n",
      "Target2:= So he jumps in and kills the animals  . \n",
      "Predict2:< So they they to to the the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 42s (- -71m 19s), Epoch: [5/10], Step: [2100/3335], Train Loss: 3.3808371782302857, BLEU: 0.6731239413560942\n",
      "\n",
      "Input1:> 政府 吗 别 想 了\n",
      "\n",
      "Target1:= Government  ? Forget about it  ! \n",
      "Predict1:< So , , , ,  .\n",
      "\n",
      "Input2:> 这 就 像 在 水池 池里 扔 一块 块石 石头\n",
      "\n",
      "Target2:= It is like throwing a stone in a pond of water  . \n",
      "Predict2:< This is a the , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 48s (- -71m 13s), Epoch: [5/10], Step: [2120/3335], Train Loss: 3.460650658607483, BLEU: 0.6777213750931638\n",
      "\n",
      "Input1:> 或许 它 正 发生 生在 你 的 身上\n",
      "\n",
      "Target1:= It may even be your story  . \n",
      "Predict1:< And is is you  .  .  .\n",
      "\n",
      "Input2:> 我们 还给 她 复原 照片 的 那天 又 恰好 是 她 小儿 小儿子 儿子 的 14 岁 生日\n",
      "\n",
      "Target2:= The day I gave her the photos also happened to be her youngest son is 14th birthday  . \n",
      "Predict2:< And we we her her , , , , , , , her , her her  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 53s (- -71m 8s), Epoch: [5/10], Step: [2140/3335], Train Loss: 3.5170281171798705, BLEU: 0.6989663025186788\n",
      "\n",
      "Input1:> 所以 我们 可以 通过 国际 社会 的 帮助 来 接受 受教 教育 英语 培训 工作 培训 等等\n",
      "\n",
      "Target1:= So we can benefit from the international community for education , English language training , job training , and more  . \n",
      "Predict1:< So we can can to to , , , , , , , , , and  .\n",
      "\n",
      "Input2:> 那么 最后 一个 问题 是 当 模特 是 什么 感觉\n",
      "\n",
      "Target2:= So the last question people ask me is , &quot; What is it like to be a model  ? &quot; \n",
      "Predict2:< So , question question question question question is is is is  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70m 59s (- -71m 2s), Epoch: [5/10], Step: [2160/3335], Train Loss: 3.5263235330581666, BLEU: 0.6432545490498601\n",
      "\n",
      "Input1:> 照片 上 他 正在 UNK 洗澡 把 一桶 一桶桶 水 往 头上 倒 好消息 消息 是 此时 此时此刻 此刻 UNK 已经 和 家人 团聚 了 更让人 欣喜 的 是 他们 一家 已 能够 自己 谋生 并 确保 孩子\n",
      "\n",
      "Target1:= Here he is seen taking a bath at the well , pouring big buckets of water over his head , and the wonderful news is , as you and I are sitting here talking today , Kofi has \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 让 我们 携起手来 起手 来 让 我们 的 床 我们 餐桌 和 家庭 成为 它们 应该 成为 的 安全 和平 的 绿洲\n",
      "\n",
      "Target2:= Together we can make our beds , our dinner tables and our families the safe and peaceful oases they should be  . \n",
      "Predict2:< Let is us to to to to to and and and and and and and and and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 4s (- -72m 56s), Epoch: [5/10], Step: [2180/3335], Train Loss: 3.4196643233299255, BLEU: 0.6550885487607363\n",
      "\n",
      "Input1:> 突然 间 我 似乎 无法 骄傲 骄傲地 称 任何 一个 国家 为 我 自己 的 国家\n",
      "\n",
      "Target1:= Suddenly , there was no country I could proudly call my own  . \n",
      "Predict1:< And I I I I I I I I I I I I I I I  .\n",
      "\n",
      "Input2:> 朝鲜 朝鲜人 通往 自由 的 道路 是 如此 遥不可及 不可 及\n",
      "\n",
      "Target2:= North Koreans have to travel incredible distances on the path to freedom  . \n",
      "Predict2:< The : : , , , , is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 10s (- -72m 51s), Epoch: [5/10], Step: [2200/3335], Train Loss: 3.4837419271469114, BLEU: 0.7188098655527229\n",
      "\n",
      "Input1:> 我 告诉 你们 一个 秘密\n",
      "\n",
      "Target1:= Let me tell you a secret  . \n",
      "Predict1:< I I you you  .  .  .\n",
      "\n",
      "Input2:> 并且 我们 把 所有 的 原始 原始数据 数据 到 算进 算进去 进去 时 进行 很 有趣 的 处理\n",
      "\n",
      "Target2:= And we can take all that raw data and treat it in very interesting ways  . \n",
      "Predict2:< And we we that the of the of the , , , , the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 16s (- -72m 45s), Epoch: [5/10], Step: [2220/3335], Train Loss: 3.444309186935425, BLEU: 0.6718063286998027\n",
      "\n",
      "Input1:> 不久 久之 之后 我 参加 做 志愿 志愿者 愿者 的 一个 一个组 组织 携手 志愿 志愿者 愿者 团队 马上 就 赶赴 了 现场 仅仅 在 几天 之内 在 当地 开展 应急 急救 救援 活动\n",
      "\n",
      "Target1:= Soon after , an organization I volunteer with , All Hands Volunteers , were on the ground , within days , working as part of the response efforts  . \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 就 在 这时 我 听见 一个 个人 问 我 怎么 了\n",
      "\n",
      "Target2:= At that moment , I heard one man is voice ask me , &quot; What is wrong  ? &quot; \n",
      "Predict2:< And , , , , , I , I I  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 21s (- -72m 40s), Epoch: [5/10], Step: [2240/3335], Train Loss: 3.422720181941986, BLEU: 0.6782198617193753\n",
      "\n",
      "Input1:> 它 随处 随处可见 可见\n",
      "\n",
      "Target1:= It is everywhere  . \n",
      "Predict1:< It is it  .  .\n",
      "\n",
      "Input2:> 它们 帮 我们 记录 保存 回忆 和 我们 的 过去 照片 是 紧急 时刻 最 容易 被 我们 忽视 的 但有 有时 UNK UNK 过天 UNK 后 我们 第一 第一个 一个 想 找回 回来 的 东西\n",
      "\n",
      "Target2:= They are our UNK and our histories , the last thing we would grab and the first thing you d go back to look for  . \n",
      "Predict2:< And we we the , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 27s (- -72m 34s), Epoch: [5/10], Step: [2260/3335], Train Loss: 3.3428240537643434, BLEU: 0.6255110561340904\n",
      "\n",
      "Input1:> 但 这 记忆 记忆力 是 有 选择 选择性 的\n",
      "\n",
      "Target1:= He has a selective one , though  . \n",
      "Predict1:< But that is is is  .  .  .\n",
      "\n",
      "Input2:> 当 他们 遭受 海啸 袭击 之时 可想 可想而知 必然 四处 四处奔波 奔波 寻求 一线 一线生机 生机 自然 把 其他 东西 他们 的 所有 东西 西都 抛 诸 脑 后\n",
      "\n",
      "Target2:= As they had run from the wave , and for their lives , absolutely everything they had , everything had to be left behind  . \n",
      "Predict2:< When they they to the , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 32s (- -72m 29s), Epoch: [5/10], Step: [2280/3335], Train Loss: 3.307387042045593, BLEU: 0.6478205381783595\n",
      "\n",
      "Input1:> 照片 片中 中美 美丽 的 和服 必须 通过 过手 手绘 处理 或者 一点 一点一点 一点 拼接 而 成 再配 配上 掉 了 的 颜色 把 那些 没有 被 水冲 UNK 的 地方 进行 细节 处理\n",
      "\n",
      "Target1:= The kimono in this shot pretty much had to be hand-drawn , or pieced together , picking out the remaining parts of color and detail that the water hadn t damaged  . \n",
      "Predict1:< The the , the , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 政客 们 小心 小心翼翼 翼翼 地 选择 用词 并 利用 用词 词汇 塑造 和 控制 现实 但是 事实 事实上 现实 改变 词汇 远远 远超过 超过 词汇 能 改变 的 现实\n",
      "\n",
      "Target2:= Politicians try to pick words and use words to shape reality and control reality , but in fact , reality changes words far more than words can ever change reality  . \n",
      "Predict2:< And can , , , , , , , , , , , , , , , , , , , , , , and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 38s (- -72m 23s), Epoch: [5/10], Step: [2300/3335], Train Loss: 3.422113132476807, BLEU: 0.6634732934560654\n",
      "\n",
      "Input1:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target1:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict1:< And is is you your , your , you your your your your your your\n",
      "\n",
      "Input2:> 这些 年 他 接受 受过 很多 治疗 也 有 很大 的 进步\n",
      "\n",
      "Target2:= So great progress and treatment has been made over the years  . \n",
      "Predict2:< And are , of of , , , of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 43s (- -72m 18s), Epoch: [5/10], Step: [2320/3335], Train Loss: 3.4632876396179197, BLEU: 0.636685243843305\n",
      "\n",
      "Input1:> 为什么 什么 因为 他们 看着 图表 上 的 曲线 说 如果 人口 继续 以 这种 速度 增长 纽约 的 人们 出行 将 需要 六百 六百万 百万 UNK 马 而 六百 六百万 百万 UNK 马 所 排泄 的 粪便\n",
      "\n",
      "Target1:= Why  ? Because they looked at the curve and said , if the population keeps growing at this rate , to move the population of New York around , they would have needed six million horses , and \n",
      "Predict1:< Why , , , , , , , , , , , , , , , , , , , , , , , , , , the , the the the the\n",
      "\n",
      "Input2:> 你们 中 的 一些 人 可能 还 不 太 了解 孤独 孤独症 这个 术语 它 是 由 复杂 的 大脑 功能 紊乱 而 导致 的 社交 障碍 学习 障碍 有时 伴有 运动 功能 功能障碍 障碍\n",
      "\n",
      "Target2:= Now , for you who may be less familiar with the term &quot; autism , &quot; it is a complex brain disorder that affects social communication , learning and sometimes physical skills  . \n",
      "Predict2:< You you you you of , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 49s (- -72m 12s), Epoch: [5/10], Step: [2340/3335], Train Loss: 3.3917952299118044, BLEU: 0.604351099969309\n",
      "\n",
      "Input1:> 我 现时 住 在 新奥尔良 奥尔良 并 深深 深深地 爱 上 了 这 座 城市\n",
      "\n",
      "Target1:= Now , I live in New Orleans , and I am in love with New Orleans  . \n",
      "Predict1:< I I in in in in , , , in  .  .\n",
      "\n",
      "Input2:> 结果 是 因为 我们 在 以 接近 光速 来 录 这些 东西 我们 有 一些 奇怪 的 现象 爱因斯坦 因斯 坦 会 相当 高兴 看到 这些 图片\n",
      "\n",
      "Target2:= It turns out , because we are recording nearly at the speed of light , we have strange effects , and Einstein would have loved to see this picture  . \n",
      "Predict2:< And , we we we we , we , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 71m 54s (- -72m 7s), Epoch: [5/10], Step: [2360/3335], Train Loss: 3.391256844997406, BLEU: 0.644976554300203\n",
      "\n",
      "Input1:> 但 我 转而 又 想到 他们 教 我 的 那些 事 关于 个性 交流 和 爱 我 意识 到 这些 东西 我 可不 愿意 用 正常 去 换\n",
      "\n",
      "Target1:= But I cast my mind back to the things that they ve taught me about individuality and communication and love , and I realize that these are things that I wouldn t want to change with normality  . \n",
      "Predict1:< But I I I I I I I and I and and and I and and I and I and I I I to I\n",
      "\n",
      "Input2:> 他们 甚至 并 没有 延续 君主 君主制 他们 的 主意 是 你 可以 以是 选举 的 国王 有 一个 固定 的 任期\n",
      "\n",
      "Target2:= And they weren t even being UNK there , they had the idea that you could be elected King for a fixed term  . \n",
      "Predict2:< They they t t , , , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 0s (- -72m 1s), Epoch: [5/10], Step: [2380/3335], Train Loss: 3.2749462485313416, BLEU: 0.7008963343973565\n",
      "\n",
      "Input1:> 笑 这 就是 我 要 告诉 你 的 第一 第一个 一个 事实 家庭 家庭暴力 暴力 可能 发生 生在 每个 个人 人身 身上 无关 你 的 种族 信仰 收入 和 教育 水平\n",
      "\n",
      "Target1:= So my first message for you is that domestic violence happens to everyone -- all races , all religions , all income and education levels  . \n",
      "Predict1:< And , is is you you you you you you you you you , , , , , , , , , , you\n",
      "\n",
      "Input2:> 有点 点子 的 人 未必 懂得 相关 知识 但 我们 可以 提供 支持\n",
      "\n",
      "Target2:= The person with the idea may not have the knowledge , but the knowledge is available  . \n",
      "Predict2:< It is a , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 5s (- -73m 55s), Epoch: [5/10], Step: [2400/3335], Train Loss: 3.42664794921875, BLEU: 0.68581762831232\n",
      "\n",
      "Input1:> 这些 项目 都 源自 于 我 自己 的 一些 疑问 如 我 的 邻居 得 付 多少 房租\n",
      "\n",
      "Target1:= And these projects came from questions I had , like , how much are my neighbors paying for their apartments  ? \n",
      "Predict1:< These are these of of , I I I I I I I I I I\n",
      "\n",
      "Input2:> 我 在读 一本 本书 时 如 遭 当头 UNK UNK 这本 本书 叫 小 即是 美 作者 是 UNK 马赫 英国 经济 经济学 经济学家 济学 学家 他 说 经济 发展 最重 重要 的 一点 是 如果 人们 不想\n",
      "\n",
      "Target2:= I was given a slap in the face reading a book , &quot; Small is Beautiful , &quot; written by UNK , who said , above all in economic development , if people do not wish to be \n",
      "Predict2:< I I the to , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 11s (- -73m 50s), Epoch: [5/10], Step: [2420/3335], Train Loss: 3.324041783809662, BLEU: 0.6239224290512414\n",
      "\n",
      "Input1:> 图象 象是 强大 的 但 同时 又 是 表面 UNK 的\n",
      "\n",
      "Target1:= Image is powerful , but also image is superficial  . \n",
      "Predict1:< The the is is is , is  .  .  .\n",
      "\n",
      "Input2:> 我们 在 纽约 的 地铁 上 碰巧 坐在 一起 他 先 跟 我 聊 了 起来\n",
      "\n",
      "Target2:= He sat next to me on the New York City subway , and he started chatting me up  . \n",
      "Predict2:< We went the in , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 16s (- -73m 44s), Epoch: [5/10], Step: [2440/3335], Train Loss: 3.403178858757019, BLEU: 0.6578289619819209\n",
      "\n",
      "Input1:> 掌声 我 的 家人 很 信任 我\n",
      "\n",
      "Target1:= My family believes in me  . \n",
      "Predict1:< I my my my  .  .\n",
      "\n",
      "Input2:> 所以 事实 事实上 我 不是 个例\n",
      "\n",
      "Target2:= So actually , I was in very good company  . \n",
      "Predict2:< So , , not not I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 22s (- -73m 39s), Epoch: [5/10], Step: [2460/3335], Train Loss: 3.430593752861023, BLEU: 0.6671837917316996\n",
      "\n",
      "Input1:> 所以 不管 是 看到 拐角 还是 为 下一代 一代 制造 健康 影像 还是 增加 新 的 可视 可视化 自从 我们 的 发明 我们 就 已经 把 所有 的 资料 和 细节 放到 了 我们 的 网上 并且 我们\n",
      "\n",
      "Target1:= So whether it is for photography around corners , or creating the next generation of health imaging , or creating new visualizations , since our invention , we have open-sourced all the data and details on our website \n",
      "Predict1:< So , is the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 正在 做 什么 呢 我们 正在 研究 这些 秃鹫 我们 在 它们 身上 安装 定位 定位器\n",
      "\n",
      "Target2:= So what is being done  ? Well , we are conducting research on these birds  . We are putting transmitters on them  . \n",
      "Predict2:< We we we we we we we we we we we we we we we we  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 27s (- -73m 33s), Epoch: [5/10], Step: [2480/3335], Train Loss: 3.403286361694336, BLEU: 0.7030191514832284\n",
      "\n",
      "Input1:> 怎么 会 有人 人愿 愿意 和 一个 一个打 她 的 男人 继续 住在一起 一起\n",
      "\n",
      "Target1:= Why would anyone stay with a man who beats her  ? \n",
      "Predict1:< And is a to and and and and and and and  ?\n",
      "\n",
      "Input2:> 我们 的 UNK 号码 能 接通 地球 上 7 亿 5000 万 人 的 来电\n",
      "\n",
      "Target2:= We have local numbers accessible to three quarters of a billion people on the planet  . \n",
      "Predict2:< We we we the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 33s (- -73m 28s), Epoch: [5/10], Step: [2500/3335], Train Loss: 3.4299074053764342, BLEU: 0.6555965458199497\n",
      "\n",
      "Input1:> 然后 他 指着 着地 地上 一排 很 不明 明显 的 脚印 说 就是 这里 就是 这条 路 走\n",
      "\n",
      "Target1:= Then he pointed toward this barely visible UNK , and said , &quot; This is the path , this is the path  . Go  . &quot; \n",
      "Predict1:< And then he , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 世界 上 所有 有成 成功 的 企业 只有 一个 共同 共同点 同点 只有 一个 没有 一个 企业 是 靠 一 人 之 力 创建 的\n",
      "\n",
      "Target2:= There is only one thing that all the successful companies in the world have in common , only one : None were started by one person  . \n",
      "Predict2:< The is a of of , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 39s (- -73m 22s), Epoch: [5/10], Step: [2520/3335], Train Loss: 3.2163798213005066, BLEU: 0.7611126487093625\n",
      "\n",
      "Input1:> 为了 改变 世界 我们 迫切 迫切需要 需要 从 我们 的 科学 科学家 学家 和 工程 工程师 那里 得到 良好 的 沟通\n",
      "\n",
      "Target1:= We desperately need great communication from our scientists and engineers in order to change the world  . \n",
      "Predict1:< And , , to to to to and and and and and and and to to\n",
      "\n",
      "Input2:> 我 这儿 有些 现在 能 拿出 出来 来说 的 伟大 的 目标\n",
      "\n",
      "Target2:= So here are some amazing goals that I think we can deal with now  . \n",
      "Predict2:< I I I to to the of , of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 44s (- -73m 17s), Epoch: [5/10], Step: [2540/3335], Train Loss: 3.495960462093353, BLEU: 0.6963381419955965\n",
      "\n",
      "Input1:> 人们 把 收集 到 的 照片 送到 那个 公社 温泉 当时 我 十分 有幸 幸得 得到 了 他们 的 信任 让 我 开始 清洗 还原 那些 照片\n",
      "\n",
      "Target1:= This is where people were handing them in , and I was honored that day that they actually trusted me to help them start UNK them  . \n",
      "Predict1:< And I the to to the , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 一家 一家人 家人 的 肖像\n",
      "\n",
      "Target2:= This is a family portrait  . \n",
      "Predict2:< This is a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 50s (- -73m 11s), Epoch: [5/10], Step: [2560/3335], Train Loss: 3.48674658536911, BLEU: 0.7104490832884353\n",
      "\n",
      "Input1:> 掌声 随着 时间 时间推移 推移 越来 越来越 越多 需要 修复 的 照片 被 运送 送过来 过来 我们 也 需要 更多 修 图 师 来 帮忙 处理 这些 照片 于是 我 又 登陆 脸谱 网 和 职业 链接 网站\n",
      "\n",
      "Target1:= Over time , more photos came in , thankfully , and more retouchers were needed , and so I reached out again on Facebook and LinkedIn , and within five days , 80 people wanted to help from \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 新一代 一代 的 创业 创业者 业者 们 皆 因 闭门 闭门造车 而 一事 一事无成 无成\n",
      "\n",
      "Target2:= There is a new generation of entrepreneurs who are dying of solitude  . \n",
      "Predict2:< The is : , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72m 55s (- -73m 6s), Epoch: [5/10], Step: [2580/3335], Train Loss: 3.3887919783592224, BLEU: 0.6342126640127129\n",
      "\n",
      "Input1:> 因为 在家 家庭 家庭暴力 暴力 中 的 最后 一步 就是 杀掉 她\n",
      "\n",
      "Target1:= Because the final step in the domestic violence pattern is kill her  . \n",
      "Predict1:< Because the is is , , , the the  .  .\n",
      "\n",
      "Input2:> 这 是 我 第一 第一张 一张 模特 照片 也 是 我 第一 第一次 一次 穿 比基 比基尼 基尼 我 基 至 还 没有 过 月经\n",
      "\n",
      "Target2:= This picture is the very first picture that I ever took , and it is also the very first time that I had worn a bikini , and I didn t even have my period yet  . \n",
      "Predict2:< This is is , first , I I I I I I , I I , I I , I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 1s (- -73m 0s), Epoch: [5/10], Step: [2600/3335], Train Loss: 3.464010846614838, BLEU: 0.7774954731150482\n",
      "\n",
      "Input1:> 那双 被 染黑 了 的 手 属于 这家 的 父亲 蓝色 和 红色 的 手 属于 两个 个儿 儿子\n",
      "\n",
      "Target1:= The dyed black hands are the father , while the blue and red hands are his sons  . \n",
      "Predict1:< It is a a , , , , , , , , and and and and and and and\n",
      "\n",
      "Input2:> 我们 每天 要 走 不同 的 路线 这样 才 没有 有人 会 怀疑 我们 要 去 哪里\n",
      "\n",
      "Target2:= Each day , we took a different route so that no one would suspect where we were going  . \n",
      "Predict2:< We we to to to to to , , , , , to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 6s (- -74m 54s), Epoch: [5/10], Step: [2620/3335], Train Loss: 3.4152575969696044, BLEU: 0.7424548140494456\n",
      "\n",
      "Input1:> 甚至 几个 月\n",
      "\n",
      "Target1:= It could take months  . \n",
      "Predict1:< It is a  .  .  .\n",
      "\n",
      "Input2:> 由于 风化 紫外 紫外线 外线 氧化 等 原因 这种 粘合 粘合剂 合剂 这种 沥青 集料 之间 的 黏胶 会 收缩 如果 它 收缩 就 会 有 小 裂纹 从 集 块 开始 会 出现 分层\n",
      "\n",
      "Target2:= Due to UNK , due to UNK light , due to oxidation , this binder , this bitumen , the glue between the aggregates is going to shrink , and if it shrinks , it gets UNK , \n",
      "Predict2:< And the the the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 12s (- -74m 49s), Epoch: [5/10], Step: [2640/3335], Train Loss: 3.3129146337509154, BLEU: 0.6589582810522797\n",
      "\n",
      "Input1:> 我们 看 这个 样品 现在 还有 有点 烫\n",
      "\n",
      "Target1:= So we still have the specimen here  . It is quite warm  . \n",
      "Predict1:< We we we this this this this this this  .  .  .\n",
      "\n",
      "Input2:> 希望 不 像 中间 那 张 那么 怪\n",
      "\n",
      "Target2:= Hopefully less awkward than that one in the middle  . \n",
      "Predict2:< It , t , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 17s (- -74m 43s), Epoch: [5/10], Step: [2660/3335], Train Loss: 3.4865137577056884, BLEU: 0.6487661422709716\n",
      "\n",
      "Input1:> 两周 内 我 共 得到 150 位 志愿 志愿者 愿者 的 答复 他们 表示 愿意 加入 我们 的 行列\n",
      "\n",
      "Target1:= Within two weeks , I had 150 people wanting to join in  . \n",
      "Predict1:< And , , , , , , , , , , , the the the  .\n",
      "\n",
      "Input2:> 事实 事实上 每 三个 美国 女性 中 就 有 一个 曾 是 家庭 家庭暴力 暴力 受害 受害者 或 潜在 的 目标 而 CDC 的 报告 称 每年 有 1500 万 的 儿童 遭受 受虐 虐待 1500 万\n",
      "\n",
      "Target2:= One in three American women experiences domestic violence or stalking at some point in her life , and the CDC reports that 15 million children are abused every year , 15 million  . \n",
      "Predict2:< In fact , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 23s (- -74m 38s), Epoch: [5/10], Step: [2680/3335], Train Loss: 3.330206263065338, BLEU: 0.7465740333134675\n",
      "\n",
      "Input1:> 他 让 我们 跟 他 改 走 一条 土路 路通 通向 丛林\n",
      "\n",
      "Target1:= He told us to follow him down a dirt road into the jungle  . \n",
      "Predict1:< He he to to to he , , the  .  .  .\n",
      "\n",
      "Input2:> 就 好像 涂鸦 艺术 艺术家 术家 一样 他们 美化 墙壁 而 我 美化 草坪 绿化 绿化带\n",
      "\n",
      "Target2:= Just like a graffiti artist , where they UNK walls , me , I beautiful lawns , UNK  . \n",
      "Predict2:< It is , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 28s (- -74m 32s), Epoch: [5/10], Step: [2700/3335], Train Loss: 3.276122677326202, BLEU: 0.7117270988312546\n",
      "\n",
      "Input1:> 你 要 告诉 他们 创业 创业精神 精神 的 真谛\n",
      "\n",
      "Target1:= And then you have to tell them the truth about entrepreneurship  . \n",
      "Predict1:< You you you to to to to  .  .\n",
      "\n",
      "Input2:> 在 喜马拉 喜马拉雅 马拉 马拉雅 我 见到 了 做 苦力 的 小孩 背着 石头 走上 几公里 公里 的 山路 到 路边 等 着 的 卡车 旁\n",
      "\n",
      "Target2:= In the Himalayas , I found children carrying stone for miles down UNK terrain to trucks waiting at roads below  . \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 34s (- -74m 27s), Epoch: [5/10], Step: [2720/3335], Train Loss: 3.4345097064971926, BLEU: 0.7106344970966014\n",
      "\n",
      "Input1:> 肯尼 肯尼亚 尼亚 生长 着 八种 秃鹫 其中 的 六种 正 濒临 濒临灭绝 灭绝\n",
      "\n",
      "Target1:= We have eight species of vultures that occur in Kenya , of which six are highly threatened with extinction  . \n",
      "Predict1:< The : : , , , , the the  .\n",
      "\n",
      "Input2:> 我 是 在 帮助 朝鲜 的 人们\n",
      "\n",
      "Target2:= &quot; I am helping the North Korean people  . &quot; \n",
      "Predict2:< I am the in in  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 39s (- -74m 21s), Epoch: [5/10], Step: [2740/3335], Train Loss: 3.430942106246948, BLEU: 0.6707678459399126\n",
      "\n",
      "Input1:> 现在 如果 我 用 一个 激光 光笔 并且 在 万亿 亿分 之一 一秒 内 开关 一次 也 就是 几 飞秒 我 就 会 制造 一些 光子 这些 光子 仅仅 是 几毫米 毫米 宽 并且 这些 光子 像 子弹\n",
      "\n",
      "Target1:= Now if I take a laser pointer and turn it on and off in one UNK of a second -- which is several femtoseconds -- I ll create a packet of photons barely a millimeter wide , and \n",
      "Predict1:< Now , I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 尽管 她们 被 抓住 了 但是 UNK 巨大 的 国际 UNK 舆论 舆论压力 压力 她们 最终 被 释放 了\n",
      "\n",
      "Target2:= Even though they were caught , they were eventually released after heavy international pressure  . \n",
      "Predict2:< And they they , , , , , , , , , , , they ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 45s (- -74m 16s), Epoch: [5/10], Step: [2760/3335], Train Loss: 3.374164354801178, BLEU: 0.6519010551387677\n",
      "\n",
      "Input1:> 孩子 们 从 家里 被 带出 出来 被 贩卖 被迫 迫在 船上 无休 无休止 休止 的 工作 他们 在 湖上 上工 工作 甚至 还 不会 游泳\n",
      "\n",
      "Target1:= Children are taken from their families and trafficked and vanished , and they are forced to work endless hours on these boats on the lake , even though they do not know how to swim  . \n",
      "Predict1:< They were the to , , , , , , , , , , , , , , they , , , ,\n",
      "\n",
      "Input2:> 我们 已经 有 大约 50 人参 参与 到 我们 的 团队 中 来 他们 全都 是 志愿 志愿者 愿者\n",
      "\n",
      "Target2:= We ve had , like , 50 people come to our UNK and participate , and it is all volunteers  . \n",
      "Predict2:< We ve we we the , we , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 51s (- -74m 10s), Epoch: [5/10], Step: [2780/3335], Train Loss: 3.3557626605033875, BLEU: 0.7041081787992926\n",
      "\n",
      "Input1:> 这 是 我 的 信条 我 告诉 人们 种植 你们 自己 的 食物 吧\n",
      "\n",
      "Target1:= It is my gospel , when I am telling people , grow your own food  . \n",
      "Predict1:< This is is my I I I I I I I  .  .  .\n",
      "\n",
      "Input2:> 他们 第一 第一天 一天 来时 看到 了 稻草 稻草人 草人 然后 就 回去 了 但是 第二 第二天 二天 他们 会 再来 并且 且说 这个 东西 从没 没动 过 他 一直 呆 在 这儿\n",
      "\n",
      "Target2:= They will come the first day and they see the scarecrow , and they go back , but the second day , they ll come and they say , this thing is not moving here , it is \n",
      "Predict2:< They they went , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 73m 56s (- -74m 5s), Epoch: [5/10], Step: [2800/3335], Train Loss: 3.3650839805603026, BLEU: 0.6559402089764246\n",
      "\n",
      "Input1:> 对于 阿富汗 我 意识 到 这 是 西方 方国 国家 也 不曾 意识 到 的 即 在 我们 成功 的 背后 是 一个 这样 的 父亲 他 能 认识 到 自己 女儿 的 价值 也 明白 女儿 的\n",
      "\n",
      "Target1:= What I ve come to realize about Afghanistan , and this is something that is often dismissed in the West , that behind most of us who succeed is a father who recognizes the value in his daughter \n",
      "Predict1:< And , , , I , , , , , , , , , , , , , the , , , the , the , the the the the the the the\n",
      "\n",
      "Input2:> 然而 世上 上有 630 万 的 人 患有 这种 疾病 而且 他们 还 得 忍受 无法 法治 治愈 的 虚弱 震颤 僵化 等 其他 伴随 而来 的 症状 的 折磨 所以 我们 需要 客观 的 工具 在 病重\n",
      "\n",
      "Target2:= However , there are 6 .3 million people worldwide who have the disease , and they have to live with incurable weakness , tremor , rigidity and the other symptoms that go along with the disease , so what \n",
      "Predict2:< But the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 2s (- -75m 59s), Epoch: [5/10], Step: [2820/3335], Train Loss: 3.5058122634887696, BLEU: 0.6620545436919513\n",
      "\n",
      "Input1:> 众议 众议院 议院 不想 让 华盛 华盛顿 沉醉 沉醉于 权力\n",
      "\n",
      "Target1:= The House of Representatives didn t want Washington to get drunk on power  . \n",
      "Predict1:< And is t to to to to to  .  .\n",
      "\n",
      "Input2:> 这些 女孩 是 幸运 的\n",
      "\n",
      "Target2:= These girls were so lucky  . \n",
      "Predict2:< These are are the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 7s (- -75m 53s), Epoch: [5/10], Step: [2840/3335], Train Loss: 3.4911274194717405, BLEU: 0.7285216956163355\n",
      "\n",
      "Input1:> 即使 他们 学会 了 中文 找到 了 工作 他们 的 整个 世界 也 会 在 顷刻 顷刻间 颠覆\n",
      "\n",
      "Target1:= Even after learning a new language and getting a job , their whole world can be turned upside down in an instant  . \n",
      "Predict1:< And they they they they they they they they they they they they they they  .\n",
      "\n",
      "Input2:> 音乐 音乐 掌声 现在 我们 在 把 他们 应用 用到 生活 中 还有 一些 路 要 走 但 在 未来 我们 可以 把 这个 技术 放到 车里 这样 样车 就 能 防止 碰撞 因为 他们 能 看到 拐角\n",
      "\n",
      "Target2:= Now we have some ways to go before we take this outside the lab on the road , but in the future , we could create cars that avoid collisions with what is around the bend , or \n",
      "Predict2:< Now , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 13s (- -75m 48s), Epoch: [5/10], Step: [2860/3335], Train Loss: 3.338436281681061, BLEU: 0.6286963985564057\n",
      "\n",
      "Input1:> 我们 怎么 怎么样 能够 从 邻里 里间 互借 借到 更多 的 东西 同时 避免 在 不合 不合时宜 时宜 的 时候 敲开 对方 的 门\n",
      "\n",
      "Target1:= How can we lend and borrow more things without knocking on each other is doors at a bad time  ? \n",
      "Predict1:< We we we to to the the the the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 翻译 告诉 我 他们 的 故事\n",
      "\n",
      "Target2:= My interpreter told me their stories  . \n",
      "Predict2:< And : me me  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 18s (- -75m 42s), Epoch: [5/10], Step: [2880/3335], Train Loss: 3.4248652696609496, BLEU: 0.6634628845984013\n",
      "\n",
      "Input1:> 所以 很多 很多年 多年 年前 我 有 个 想法 我们 为何 何不 不能 至少 少有 一次 做到 聆听 他们 的 想法 而 不是 把 我们 的 想法 强加 强加于 加于 他们 但 这 不能 以 社区 会议 的\n",
      "\n",
      "Target1:= So years and years ago , I had this idea : Why don t we , for once , instead of arriving in the community to tell people what to do , why don t , for once \n",
      "Predict1:< So , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 大家 都 想 问 的 另 一个 问题 是 为什么 什么 她 不离 离开\n",
      "\n",
      "Target2:= The other question everybody asks is , why doesn t she just leave  ? \n",
      "Predict2:< How , , , , , ,  ? ,  ?  ?  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 24s (- -75m 37s), Epoch: [5/10], Step: [2900/3335], Train Loss: 3.434705150127411, BLEU: 0.6406678027156719\n",
      "\n",
      "Input1:> 还有 在 用 幻灯 幻灯片 演示 你们 的 研究 研究成果 成果 时 不要 要使 使用 点 句式\n",
      "\n",
      "Target1:= And when presenting your work , drop the bullet points  . \n",
      "Predict1:< And the , , , , , , , , , , you , , you  .\n",
      "\n",
      "Input2:> 150 英尺 乘以 10 英尺 大小\n",
      "\n",
      "Target2:= It is 150 feet by 10 feet  . \n",
      "Predict2:< And , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 29s (- -75m 31s), Epoch: [5/10], Step: [2920/3335], Train Loss: 3.46538051366806, BLEU: 0.667085299657463\n",
      "\n",
      "Input1:> 宾夕法尼 宾夕法尼亚 宾夕法尼亚州 尼亚 州立 州立大学 大学 让 我 这个 教 沟通 通学 的 老师 给 工程 工程系 的 学生 上 沟通 通学 课\n",
      "\n",
      "Target1:= Penn State asked me , a communications teacher , to teach a communications class for engineering students  . \n",
      "Predict1:< And : : , , , the , , , , , the the\n",
      "\n",
      "Input2:> 这种 人 不存 存在\n",
      "\n",
      "Target2:= It doesn t exist  . \n",
      "Predict2:< It is people people  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 35s (- -75m 26s), Epoch: [5/10], Step: [2940/3335], Train Loss: 3.3006788969039915, BLEU: 0.6708024407981472\n",
      "\n",
      "Input1:> 看到 了 吧 我 是 个 艺术 艺术家 术家\n",
      "\n",
      "Target1:= See , I am an artist  . \n",
      "Predict1:< I : I I I  .  .  .\n",
      "\n",
      "Input2:> 我 想 介绍 给 你们 认识 我 的 两个 兄弟\n",
      "\n",
      "Target2:= Now I d like to introduce you to my brothers  . \n",
      "Predict2:< I want to to to to you to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 40s (- -75m 20s), Epoch: [5/10], Step: [2960/3335], Train Loss: 3.4295350670814515, BLEU: 0.7806404614444471\n",
      "\n",
      "Input1:> 我们 没有 任何 基础 基础设施 设施 的 建设\n",
      "\n",
      "Target1:= We have zero infrastructure  . \n",
      "Predict1:< We have t have  .  .  .  .\n",
      "\n",
      "Input2:> 我 从 哪里 里来 我 是 谁\n",
      "\n",
      "Target2:= Where am I from  ? Who am I  ? \n",
      "Predict2:< I I to to I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 46s (- -75m 15s), Epoch: [5/10], Step: [2980/3335], Train Loss: 3.2877623081207275, BLEU: 0.6747747803256988\n",
      "\n",
      "Input1:> 大概 是 和 言语 语词 词汇 相关 的 词汇 我 假设\n",
      "\n",
      "Target1:= Something to do with words , I assume  . \n",
      "Predict1:< It is the , and and and and and and  .  .\n",
      "\n",
      "Input2:> 当 她们 走出 矿井 时 全身 UNK\n",
      "\n",
      "Target2:= When they came out of the shaft , they were soaking wet from their own sweat  . \n",
      "Predict2:< When they they they , , they  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 51s (- -75m 9s), Epoch: [5/10], Step: [3000/3335], Train Loss: 3.4715532183647158, BLEU: 0.7536020377127661\n",
      "\n",
      "Input1:> 一连 几周 所有 的 志愿 志愿者 愿者 和 当地 居民 一样 都 找到 了 一些 类似 似的 东西\n",
      "\n",
      "Target1:= For weeks , all the volunteers and locals alike had been finding similar things  . \n",
      "Predict1:< The is the of and and and and and and and and and\n",
      "\n",
      "Input2:> 我们 倾向 于 将 受害 受害者 描述 成 UNK UNK 自己 轻浮 的 女人 就 像 新闻 中 可怕 的 标题 所言\n",
      "\n",
      "Target2:= We tend to stereotype victims as grisly headlines , self-destructive women , damaged goods  . \n",
      "Predict2:< We we to to to the the the , , , , , , , , the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74m 57s (- -75m 4s), Epoch: [5/10], Step: [3020/3335], Train Loss: 3.364979457855225, BLEU: 0.6827362467384447\n",
      "\n",
      "Input1:> 在 死 之前 我 想 在 上百 上百万 百万 的 观众 面前 唱歌\n",
      "\n",
      "Target1:= &quot; Before I die , I want to sing for millions  . &quot; \n",
      "Predict1:< In , , I I I I I , to  .  .\n",
      "\n",
      "Input2:> 什么 叫 UNK 看看 这条 路 路面 面上 的 石头 脱落\n",
      "\n",
      "Target2:= What is UNK  ? You see that in this road that the stones at the surface come off  . \n",
      "Predict2:< So is the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 2s (- -76m 58s), Epoch: [5/10], Step: [3040/3335], Train Loss: 3.3389676094055174, BLEU: 0.7089379820894576\n",
      "\n",
      "Input1:> 政客 们 小心 小心翼翼 翼翼 地 选择 用词 并 利用 用词 词汇 塑造 和 控制 现实 但是 事实 事实上 现实 改变 词汇 远远 远超过 超过 词汇 能 改变 的 现实\n",
      "\n",
      "Target1:= Politicians try to pick words and use words to shape reality and control reality , but in fact , reality changes words far more than words can ever change reality  . \n",
      "Predict1:< And can , , , , , , , , , , , , , , , , , , , , , , , and\n",
      "\n",
      "Input2:> 我们 鼓舞 各个 社区 进行 创业\n",
      "\n",
      "Target2:= We activate communities  . \n",
      "Predict2:< We are we the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 8s (- -76m 53s), Epoch: [5/10], Step: [3060/3335], Train Loss: 3.3794110894203184, BLEU: 0.6759312159390327\n",
      "\n",
      "Input1:> 灯泡 来自 于 一个 坏掉 的 手电 手电筒 电筒\n",
      "\n",
      "Target1:= And that is a small torch from a broken flashlight  . \n",
      "Predict1:< The : : a a a a  .\n",
      "\n",
      "Input2:> 你 要 告诉 他们 创业 创业精神 精神 的 真谛\n",
      "\n",
      "Target2:= And then you have to tell them the truth about entrepreneurship  . \n",
      "Predict2:< You you you to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 13s (- -76m 47s), Epoch: [5/10], Step: [3080/3335], Train Loss: 3.3165111541748047, BLEU: 0.6774592512401388\n",
      "\n",
      "Input1:> 她们 带 着 我 走下 一段 一段段 狭窄 的 楼梯 到 了 一个 肮脏 昏暗 的 地下 地下室\n",
      "\n",
      "Target1:= They ushered me down a narrow set of stairs that led to this dirty , dimly fluorescent lit basement  . \n",
      "Predict1:< They ve a a , , , , , , , , a a  .\n",
      "\n",
      "Input2:> 我们 西方 西方人 方人 是 帝国 帝国主义 帝国主义者 国主 主义 主义者 殖民 殖民主义 殖民主义者 民主 主义 主义者 以及 传教 传教士 教士 我们 跟 人们 打交道 交道 的 方式 只有 两种 一 是 居高临下 UNK 二 是 家长 做派\n",
      "\n",
      "Target2:= We Western people are imperialist , UNK missionaries , and there are only two ways we deal with people : We either patronize them , or we are paternalistic  . \n",
      "Predict2:< We we the the the the the , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 19s (- -76m 42s), Epoch: [5/10], Step: [3100/3335], Train Loss: 3.4404229879379273, BLEU: 0.7858974978735578\n",
      "\n",
      "Input1:> 你 的 钱 可能 被 偷 你 可能 因 战争 而 背井离乡 离乡\n",
      "\n",
      "Target1:= Your money can be stolen  . You can be forced to leave your home during a war  . \n",
      "Predict1:< You you you you , , , , , , ,  .\n",
      "\n",
      "Input2:> 翻译 告诉 我 他们 的 故事\n",
      "\n",
      "Target2:= My interpreter told me their stories  . \n",
      "Predict2:< And : me me  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 24s (- -76m 36s), Epoch: [5/10], Step: [3120/3335], Train Loss: 3.4612868428230286, BLEU: 0.7680144535627828\n",
      "\n",
      "Input1:> 我们 可以 多次 多次重 多次重复 重复\n",
      "\n",
      "Target1:= Several times we can repeat that  . \n",
      "Predict1:< We can can to the  .  .  .\n",
      "\n",
      "Input2:> 而且 这 也 是 为什么 什么 参议 参议院 议院 反对 这个 个头 头衔\n",
      "\n",
      "Target2:= And that is why the Senate objected to it  . \n",
      "Predict2:< And this is is is , , , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 30s (- -76m 30s), Epoch: [5/10], Step: [3140/3335], Train Loss: 3.5378831148147585, BLEU: 0.7911056741747289\n",
      "\n",
      "Input1:> 太 不 简单 了\n",
      "\n",
      "Target1:= Extraordinary  . \n",
      "Predict1:< It is t  .  .  .\n",
      "\n",
      "Input2:> 在 死 之前 我 想 种 一棵 一棵树\n",
      "\n",
      "Target2:= &quot; Before I die , I want to plant a tree  . &quot; \n",
      "Predict2:< In , I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 36s (- -76m 25s), Epoch: [5/10], Step: [3160/3335], Train Loss: 3.4135327100753785, BLEU: 0.7420906565283277\n",
      "\n",
      "Input1:> 帮助 幸存 幸存者 重新 找回 美好 可爱 的 自己 重新 拥有 未来\n",
      "\n",
      "Target1:= UNK survivors as wonderful , lovable people with full futures  . \n",
      "Predict1:< And : : , , the the  .  .  .\n",
      "\n",
      "Input2:> 我们 没有 自由 他们 说道\n",
      "\n",
      "Target2:= &quot; We have no freedom , &quot; they said  . \n",
      "Predict2:< We don t t them  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 41s (- -76m 19s), Epoch: [5/10], Step: [3180/3335], Train Loss: 3.3794633746147156, BLEU: 0.77239991819746\n",
      "\n",
      "Input1:> 他们 得到 了 他们 要 的 体面 头衔\n",
      "\n",
      "Target1:= They got their title of respectability  . \n",
      "Predict1:< They they to they they  .  .  .\n",
      "\n",
      "Input2:> 秃鹫 还 曾经 被 迪斯尼 看上 像 高飞 狗 一样 被 人格 人格化 变成 了 麻木 的 愚蠢 的 动画 UNK 角色\n",
      "\n",
      "Target2:= They ve also be associated with Disney — — personified as goofy , dumb , stupid characters  . \n",
      "Predict2:< The the a , , , , , , , , , a a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 47s (- -76m 14s), Epoch: [5/10], Step: [3200/3335], Train Loss: 3.3504302740097045, BLEU: 0.7734650266205328\n",
      "\n",
      "Input1:> 塔利 塔利班 走 了 父亲 大声 叫 着\n",
      "\n",
      "Target1:= &quot; The Taliban are gone  ! &quot; my father shouted  . \n",
      "Predict1:< The : : , ,  .  .  .\n",
      "\n",
      "Input2:> 思考 死亡 能够 让 你 对 自己 的 人生 有 更 清醒 的 认识\n",
      "\n",
      "Target2:= Thinking about death clarifies your life  . \n",
      "Predict2:< And is you to to to to to to to to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 52s (- -76m 8s), Epoch: [5/10], Step: [3220/3335], Train Loss: 3.4180476903915404, BLEU: 0.7439781594979652\n",
      "\n",
      "Input1:> 这些 照片 很 容易 再次 受损\n",
      "\n",
      "Target1:= It is very easy to do more damage to those damaged photos  . \n",
      "Predict1:< And are are these  .  .  .\n",
      "\n",
      "Input2:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target2:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict2:< He is is because , , , , , , , , , , , he\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 75m 58s (- -76m 3s), Epoch: [5/10], Step: [3240/3335], Train Loss: 3.3674734473228454, BLEU: 0.70811335515976\n",
      "\n",
      "Input1:> 所有 这些 受 海啸 影响 的 照片 在 盐水 咸咸的 海水 里 浸泡 受 细菌 侵蚀 又 埋没 在 垃圾 里 有时 甚至 被 油污 浸染 所有 这些 经过 一段 一段时间 段时间 时间 都 会 持续 破坏 这些\n",
      "\n",
      "Target1:= Now , all these photos had been damaged by water , submerged in salt water , covered in bacteria , in sewage , sometimes even in oil , all of which over time is going to continue to \n",
      "Predict1:< And these these these these , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 而 在 纽约 年轻 黑人 和 拉美 男性 总数 数只 只有 17 万 7 千 所以 对 他们 来说 问题 不是 我会 被 停车 吗\n",
      "\n",
      "Target2:= And there are only UNK young black and Latino men in New York , so for them , it is not a question of , &quot; Will I get stopped  ? &quot; \n",
      "Predict2:< And the the , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 3s (- -77m 57s), Epoch: [5/10], Step: [3260/3335], Train Loss: 3.6166137576103212, BLEU: 0.7757614527275483\n",
      "\n",
      "Input1:> 如果 你 想 现在 就 可以 杀 了 我 他 说 但 我 不会 因 你 落后 的 思想 而 毁 了 我 女儿 的 未来\n",
      "\n",
      "Target1:= &quot; Kill me now , if you wish , &quot; he said , &quot; but I will not ruin my daughter is future because of your old and backward ideas  . &quot; \n",
      "Predict1:< If you you to to , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也 珍爱 UNK 女儿 的 家庭\n",
      "\n",
      "Target2:= I was very lucky to grow up in a family where education was prized and daughters were treasured  . \n",
      "Predict2:< I I a a a a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 9s (- -77m 52s), Epoch: [5/10], Step: [3280/3335], Train Loss: 3.3879917740821837, BLEU: 0.7187591836885513\n",
      "\n",
      "Input1:> 我 想 通过 一个 叫做 回扣 的 作品 来看 看看 在 阿富汗 的 腐败 并且 模拟 一个 警察 警察局 官员 在 喀布尔 布尔 的 大街 街上 建立 一个 假 的 检查 检查站 然后 把 那些 车 停下 停下来\n",
      "\n",
      "Target1:= I want to look into corruption in Afghanistan through a work called &quot; UNK &quot; and impersonate a police officer , set up a fake UNK on the street of Kabul and stop cars , but instead of \n",
      "Predict1:< I I to to a a a a a a a a a a a a a a a , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 我们 可以 建立 一个 社区 在 这个 社区 区里 我们 有 来自 自小 小型 小型企业 企业 背景 的 辅助 商 在 咖啡 咖啡馆 中 酒吧 里 为 你 出谋划策 谋划 划策 我们 能 为 你 提供 这位\n",
      "\n",
      "Target2:= So we can create the community where we have facilitators who come from a small business background sitting in cafes , in bars , and your dedicated buddies who will do to you , what somebody did for \n",
      "Predict2:< So we we we a a , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 14s (- -77m 46s), Epoch: [5/10], Step: [3300/3335], Train Loss: 3.404056024551392, BLEU: 0.7616990483150633\n",
      "\n",
      "Input1:> 我 能够 结束 自己 疯狂 的 爱 的 故事 靠 的 是 打破 沉默\n",
      "\n",
      "Target1:= I was able to end my own crazy love story by breaking the silence  . \n",
      "Predict1:< I I to the the the the the the , the  .  .\n",
      "\n",
      "Input2:> UNK 知道 什么 是 爱\n",
      "\n",
      "Target2:= Remi knows what love is  . \n",
      "Predict2:< What know , know is  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 20s (- -77m 41s), Epoch: [5/10], Step: [3320/3335], Train Loss: 3.3275410175323485, BLEU: 0.7222079420185098\n",
      "\n",
      "Input1:> 谢谢\n",
      "\n",
      "Target1:= Thank you  . \n",
      "Predict1:< Thank you  .\n",
      "\n",
      "Input2:> 这些 人 被 奴役 在 加纳 的 另 一处 矿井 里\n",
      "\n",
      "Target2:= These miners are enslaved in a mine shaft in another part of Ghana  . \n",
      "Predict2:< These are are these the the the  .  .  .\n",
      "[3.566279423236847, 3.4070756435394287, 3.394347310066223, 3.294837462902069, 3.447178316116333, 3.6949307918548584, 3.5530288934707643, 3.5061386704444883, 3.580853283405304, 3.400308334827423, 3.653374123573303, 3.4086325764656067, 3.4760347247123717, 3.4496815323829653, 3.4562154412269592, 3.4839292407035827, 3.467835986614227, 3.4456197142601015, 3.427717089653015, 3.3074670314788817, 3.283340561389923, 3.454622805118561, 3.3726407289505005, 3.2930323600769045, 3.450244355201721, 3.5237938523292542, 3.4599188208580016, 3.329887795448303, 3.4016898036003114, 3.4488552093505858, 3.5067548036575316, 3.424999940395355, 3.3604604482650755, 3.421999680995941, 3.537403094768524, 3.3405981779098513, 3.4492794394493105, 3.339508032798767, 3.499672758579254, 3.450148856639862, 3.4489348769187926, 3.3742238521575927, 3.4247326135635374, 3.3251610159873963, 3.3889285922050476, 3.4824897170066835, 3.4587595582008364, 3.443846845626831, 3.4136322379112243, 3.4292887568473818, 3.449284327030182, 3.299820303916931, 3.424681544303894, 3.5131307244300842, 3.487703490257263, 3.4621851444244385, 3.499940884113312, 3.4203924179077148, 3.401089978218079, 3.3221269726753233, 3.3250633001327516, 3.483081543445587, 3.4559252738952635, 3.4436222314834595, 3.4073214292526246, 3.4071653485298157, 3.578267824649811, 3.56934015750885, 3.425290524959564, 3.3850125074386597, 3.219499480724335, 3.524367356300354, 3.442322278022766, 3.4001275300979614, 3.34035187959671, 3.4071396827697753, 3.371313285827637, 3.5011670708656313, 3.452900528907776, 3.297061097621918, 3.4645060896873474, 3.3940698742866515, 3.5006507515907286, 3.4272173047065735, 3.4793603897094725, 3.4092456102371216, 3.416345810890198, 3.3518608808517456, 3.446413815021515, 3.427246558666229, 3.4387010455131533, 3.3751016736030577, 3.293307197093964, 3.3833961367607115, 3.462352466583252, 3.413485527038574, 3.4228247880935667, 3.4957590818405153, 3.380850911140442, 3.4685551285743714, 3.4198631763458254, 3.4045624852180483, 3.37219694852829, 3.457511568069458, 3.3808371782302857, 3.460650658607483, 3.5170281171798705, 3.5263235330581666, 3.4196643233299255, 3.4837419271469114, 3.444309186935425, 3.422720181941986, 3.3428240537643434, 3.307387042045593, 3.422113132476807, 3.4632876396179197, 3.3917952299118044, 3.391256844997406, 3.2749462485313416, 3.42664794921875, 3.324041783809662, 3.403178858757019, 3.430593752861023, 3.403286361694336, 3.4299074053764342, 3.2163798213005066, 3.495960462093353, 3.48674658536911, 3.3887919783592224, 3.464010846614838, 3.4152575969696044, 3.3129146337509154, 3.4865137577056884, 3.330206263065338, 3.276122677326202, 3.4345097064971926, 3.430942106246948, 3.374164354801178, 3.3557626605033875, 3.3650839805603026, 3.5058122634887696, 3.4911274194717405, 3.338436281681061, 3.4248652696609496, 3.434705150127411, 3.46538051366806, 3.3006788969039915, 3.4295350670814515, 3.2877623081207275, 3.4715532183647158, 3.364979457855225, 3.3389676094055174, 3.3794110894203184, 3.3165111541748047, 3.4404229879379273, 3.4612868428230286, 3.5378831148147585, 3.4135327100753785, 3.3794633746147156, 3.3504302740097045, 3.4180476903915404, 3.3674734473228454, 3.6166137576103212, 3.3879917740821837, 3.404056024551392, 3.3275410175323485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 27s (- -73m 21s), Epoch: [6/10], Step: [20/3335], Train Loss: 3.5050572633743284, BLEU: 0.7209082595608569\n",
      "\n",
      "Input1:> 我们 会 因 芭蕾 芭蕾舞 大师 和 踢踏 踢踏舞 舞蹈 者 而 感到 欣悦 稍后 你们 就 会 知道 了\n",
      "\n",
      "Target1:= We delight at ballet virtuosos and tap dancers you will see later on  . \n",
      "Predict1:< We we we the and and and , , , , , , and , and\n",
      "\n",
      "Input2:> 价格 很 低 我们 也 知道 是 怎么 一回 儿 事\n",
      "\n",
      "Target2:= They are ultra-low cost , and we all know what happens  . \n",
      "Predict2:< And , , , we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 33s (- -75m 21s), Epoch: [6/10], Step: [40/3335], Train Loss: 3.439118194580078, BLEU: 0.7322430955050148\n",
      "\n",
      "Input1:> 而且 它 的 意思 是 一个 诡诈 的 政客\n",
      "\n",
      "Target1:= And what UNK means is &quot; a dishonest politician  . &quot; \n",
      "Predict1:< And it is is a , is  .  .\n",
      "\n",
      "Input2:> 但是 没有 有人 帮助 他们 因为 大家 连 自己 和 自己 的 家人 都 无法 顾全\n",
      "\n",
      "Target2:= But nobody helped them , because they were so focused on taking care of themselves and their families  . \n",
      "Predict2:< But they don t t to , and and and and and and  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 38s (- -76m 37s), Epoch: [6/10], Step: [60/3335], Train Loss: 3.4654672145843506, BLEU: 0.7986870856325144\n",
      "\n",
      "Input1:> 或许 它 正 发生 生在 你 的 身上\n",
      "\n",
      "Target1:= It may even be your story  . \n",
      "Predict1:< And is is you  .  .\n",
      "\n",
      "Input2:> 我 意识 到 如果 我 不 反抗 这个 我 曾 深爱 的 男人 会 杀掉 我\n",
      "\n",
      "Target2:= I realized that the man who I loved so much was going to kill me if I let him  . \n",
      "Predict2:< I I I I I I I I I I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 44s (- -76m 13s), Epoch: [6/10], Step: [80/3335], Train Loss: 3.4366726756095884, BLEU: 0.7219419387912626\n",
      "\n",
      "Input1:> 他 表现 现出 的 深深 的 UNK\n",
      "\n",
      "Target1:= And he was very , very sorry  . \n",
      "Predict1:< He is a a  .  .  .  .\n",
      "\n",
      "Input2:> 我 回答 说 当然 不 啦 我 当然 不怕 怕人 人们 会 偷 它\n",
      "\n",
      "Target2:= And I am like , &quot; Hell no , I ain t afraid they are gonna steal it  . \n",
      "Predict2:< I I said , &quot; , , , , , , , , , , , , &quot; , &quot;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 49s (- -77m 56s), Epoch: [6/10], Step: [100/3335], Train Loss: 3.34930135011673, BLEU: 0.7363789879883519\n",
      "\n",
      "Input1:> 在 这里 工作 的 人 通常 都 要 忍受 客人 人们 残忍 的 性虐 性虐待 虐待\n",
      "\n",
      "Target1:= The workers here often endure tragic sexual abuse at the hands of their customers  . \n",
      "Predict1:< In the people people people people people people people people people people people\n",
      "\n",
      "Input2:> 禁止 在 一小 一小块 小块 土地 地上 种 食物 而 之前 这块 土地 却 没人 关心 笑\n",
      "\n",
      "Target2:= A warrant for planting food on a piece of land that you could care less about  ? &quot; \n",
      "Predict2:< The the the , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 76m 55s (- -77m 42s), Epoch: [6/10], Step: [120/3335], Train Loss: 3.4049430966377257, BLEU: 0.6839676763450683\n",
      "\n",
      "Input1:> 就 这样 我们 一路 来到 了 老挝 但是 我 却 要 几乎 花光 全部 积蓄 来 贿赂 老挝 的 边境 警察\n",
      "\n",
      "Target1:= We made it all the way to the border of Laos , but I had to spend almost all my money to bribe the border guards in Laos  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 150 英尺 乘以 10 英尺 大小\n",
      "\n",
      "Target2:= It is 150 feet by 10 feet  . \n",
      "Predict2:< And : : , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 1s (- -77m 31s), Epoch: [6/10], Step: [140/3335], Train Loss: 3.42221599817276, BLEU: 0.7731654849622953\n",
      "\n",
      "Input1:> 事实 事实上 这些 肌肉 强健 的 男人 若干 若干年 年后 很 有 可能 会 跟 这个 个人 一样 被 汞 中毒 和 肺结核 结核 苦苦 折磨\n",
      "\n",
      "Target1:= In fact , it is very likely that this muscular person will end up like this one here , racked with tuberculosis and mercury poisoning in just a few years  . \n",
      "Predict1:< In fact , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 正在 尽力 让 他们 了解 到 秃鹫 的 益处 并且 发自 发自内心 自内心 内心 心得 去 感激 这些 奇妙 的 生物 和 他们 为 自然 所 做 的 贡献\n",
      "\n",
      "Target2:= We are talking to them about appreciating vultures , about the need from within to appreciate these wonderful creatures and the services that they provide  . \n",
      "Predict2:< We are to to to to to and and and and and and and and and and and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 6s (- -77m 22s), Epoch: [6/10], Step: [160/3335], Train Loss: 3.454615521430969, BLEU: 0.755536795230631\n",
      "\n",
      "Input1:> 所以 他们 给 他 一个 他们 能够 想到 的 最低 低微 的 最 微不足道 不足 不足道 的 最为 可悲 的 头衔\n",
      "\n",
      "Target1:= So they wanted to give him the UNK , UNK , most pathetic title that they could think of  . \n",
      "Predict1:< So they they they to to to their their their their their their their their their their their their their their\n",
      "\n",
      "Input2:> 秃鹫 是 我们 的 大自然 自然 垃圾 清道 清道夫 道夫\n",
      "\n",
      "Target2:= They are our natural garbage collectors  . \n",
      "Predict2:< The is is we we we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 12s (- -77m 13s), Epoch: [6/10], Step: [180/3335], Train Loss: 3.393016278743744, BLEU: 0.746468240293917\n",
      "\n",
      "Input1:> UNK 我 的 下 一项 项发明 发明 是 我 想 做 一个 电力 栅栏 CA 电力 栅栏\n",
      "\n",
      "Target1:= My next invention is , I want to make an electric fence  . Electric fence  ? \n",
      "Predict1:< And I I I I I I a a a to to a a a  .\n",
      "\n",
      "Input2:> 但是 言语 语词 词汇 对于 政治 中心 之 作用 是非 非常 重要 的 并且 所有 的 政客 都 明白 他们 必须 尝试 着 控制 他们 的话 话语 语言 UNK 言论\n",
      "\n",
      "Target2:= But it is very important that words are at the center of politics , and all politicians know they have to try and control language  . \n",
      "Predict2:< But the , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 17s (- -77m 5s), Epoch: [6/10], Step: [200/3335], Train Loss: 3.3423635840415953, BLEU: 0.7469523184988145\n",
      "\n",
      "Input1:> 很多 人 都 淹死 了\n",
      "\n",
      "Target1:= Many of them drown  . \n",
      "Predict1:< Many many people people  .\n",
      "\n",
      "Input2:> 一年 之内 我 做 了 27 个 项目 政府 的 人 跑 来 问 我 你 是 怎么 怎么办 办到 的\n",
      "\n",
      "Target2:= In a year , I had 27 projects going on , and the government came to see me to say , &quot; How can you do that  ? \n",
      "Predict2:< And I I , , , , , , , , , , , , , , , I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 23s (- -78m 57s), Epoch: [6/10], Step: [220/3335], Train Loss: 3.3174962639808654, BLEU: 0.7071591743485732\n",
      "\n",
      "Input1:> 但是 当 这些 都 没有 呈现 现在 孩子 们 眼前 如果 他们 并 没有 体会 到 食物 是 如何 影响 人 的 想法 和 身体 的 他们 只是 盲目 目的 吃 着 任何 摆在 他们 面前 的 食物\n",
      "\n",
      "Target1:= But when none of this is presented to them , if they are not shown how food affects the mind and the body , they blindly eat whatever the hell you put in front of them  . \n",
      "Predict1:< But if they , t , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 掌声 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 28s (- -78m 50s), Epoch: [6/10], Step: [240/3335], Train Loss: 3.362126898765564, BLEU: 0.6960666072043886\n",
      "\n",
      "Input1:> 这样 的 能人 尚未 未出 出生\n",
      "\n",
      "Target1:= This person has never been born  . \n",
      "Predict1:< And is is the  .  .\n",
      "\n",
      "Input2:> 在 印度 印度教 神话 中 秃鹫 之 UNK 名为 雅 达 尤 他 冒 着 生命 生命危险 危险 从 10 头 恶魔 罗 波 那 手中 救出 了 西 妲 女神\n",
      "\n",
      "Target2:= In Hindu mythology , UNK was the vulture god , and he risked his life in order to save the goddess UNK from the UNK UNK UNK  . \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 34s (- -78m 43s), Epoch: [6/10], Step: [260/3335], Train Loss: 3.309951114654541, BLEU: 0.6814475336981979\n",
      "\n",
      "Input1:> 我 希望 这些 影像 能够 在 你们 这样 的 观者 心中 唤起 一股 力量 我 希望 这股 力量 的 星星 星星之火 能够 成为 被 奴役 者 的 希望 之 光 因为 若 没有 这 光亮 奴役 之 兽\n",
      "\n",
      "Target1:= I hope that these images awaken a force in those who view them , people like you , and I hope that force will ignite a fire , and that fire will shine a light on slavery , \n",
      "Predict1:< I I to to to you to you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 当地 有 一位 女士 指导 我们 清洗\n",
      "\n",
      "Target2:= We had an amazing local woman who guided us  . \n",
      "Predict2:< And is a , , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 39s (- -78m 36s), Epoch: [6/10], Step: [280/3335], Train Loss: 3.4142354249954225, BLEU: 0.6928207882339575\n",
      "\n",
      "Input1:> 所以 我 必须 找到 一个 解决 解决问题 问题 的 方法\n",
      "\n",
      "Target1:= So I had to find a way of solving this problem  . \n",
      "Predict1:< So I I to to to to to problem problem\n",
      "\n",
      "Input2:> 我 厌倦 了 这些 现实\n",
      "\n",
      "Target2:= I got tired of seeing this happening  . \n",
      "Predict2:< I ve these  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 45s (- -78m 30s), Epoch: [6/10], Step: [300/3335], Train Loss: 3.381081211566925, BLEU: 0.7276343431964145\n",
      "\n",
      "Input1:> 没有 能 过人 的 窗户\n",
      "\n",
      "Target1:= There were no windows large enough to climb through  . \n",
      "Predict1:< There is no  .  .  .\n",
      "\n",
      "Input2:> 它们 是 那么 的 优雅 查尔斯 尔斯 达尔 达尔文 曾 说 到 他 改变 了 自己 的 看法 在 他 看到 秃鹫 们 在 空中 优雅 而 又 浑然 浑然天成 天成 的 身姿 之后\n",
      "\n",
      "Target2:= They are very graceful  . Charles Darwin said he changed his mind because he watched them fly effortlessly without energy in the skies  . \n",
      "Predict2:< They are the the , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 50s (- -78m 23s), Epoch: [6/10], Step: [320/3335], Train Loss: 3.3412585616111756, BLEU: 0.6801218427983814\n",
      "\n",
      "Input1:> 英语 在 韩国 太重 重要 了 所以 我 必须 须要 开始 学习 第三 UNK 语言 英语\n",
      "\n",
      "Target1:= English was so important in South Korea , so I had to start learning my third language  . \n",
      "Predict1:< And is , , I I I I I to to to to to\n",
      "\n",
      "Input2:> 但是 这 一次 我 要 去 挖掘 那些 见 不得 光 的 秘密\n",
      "\n",
      "Target2:= But this time , I would see the skeletons hidden in the closet  . \n",
      "Predict2:< But I I to to to to to to to to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 77m 56s (- -78m 17s), Epoch: [6/10], Step: [340/3335], Train Loss: 3.4394127368927, BLEU: 0.7465826013853875\n",
      "\n",
      "Input1:> 所以 当 我 为 这次 演讲 做 准备 时时 发现 很 难为 诚实 找到 一个 平衡 平衡点 因为 一方 一方面 方面 如果 我 站 在 这儿 说 看 我 凭着 天生 的 优势 得到 了 这么 多 东西\n",
      "\n",
      "Target1:= So when I was writing this talk , I found it very difficult to strike an honest balance , because on the one hand , I felt very uncomfortable to come out here and say , &quot; Look \n",
      "Predict1:< So I I I to to , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 以为 我 这辈子 辈子 完 了 尽管 尽管如此 如此 我 还是 努力 隐藏 了 自己 的 恐惧 和 担忧 并 回答 了 他们 的 问题\n",
      "\n",
      "Target2:= I thought my life was over , but I managed to control all the emotions inside me and answer the questions  . \n",
      "Predict2:< I I I to I I and and I and and , I I and and and I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 1s (- -78m 11s), Epoch: [6/10], Step: [360/3335], Train Loss: 3.3626280665397643, BLEU: 0.7369639031213131\n",
      "\n",
      "Input1:> 因此 我 觉得 太好了 这 是 我 的 责任 我 必须 维护 它 所以 我 想 做 什么 就 可以 做 什么 了\n",
      "\n",
      "Target1:= So I am like , &quot; Cool  . I can do whatever the hell I want , since it is my responsibility and I gotta maintain it  . &quot; \n",
      "Predict1:< So I I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 如果 有些 女士 对 的 我 出现 感到 惊恐 不必 现在 告诉 我 但 我 马上 就 会 在 twitter 上知 知道 的\n",
      "\n",
      "Target2:= If some of the women were really horrified when I came out , you don t have to tell me now , but I ll find out later on Twitter  . \n",
      "Predict2:< If , is t , , , , , , I I I I I I I I , I I I , I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 7s (- -78m 4s), Epoch: [6/10], Step: [380/3335], Train Loss: 3.3824695110321046, BLEU: 0.7448872674981856\n",
      "\n",
      "Input1:> 像 这样 的 幻灯 幻灯片 不仅 不仅仅 仅仅 乏味 无聊 而且 太 过 依赖 我们 大脑 的 语言 区 使 我们 不知 不知所措\n",
      "\n",
      "Target1:= A slide like this is not only boring , but it relies too much on the language area of our brain , and causes us to become overwhelmed  . \n",
      "Predict1:< And is t , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 掌声 我 的 家人 很 信任 我\n",
      "\n",
      "Target2:= My family believes in me  . \n",
      "Predict2:< I my my my  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 12s (- -79m 58s), Epoch: [6/10], Step: [400/3335], Train Loss: 3.46565865278244, BLEU: 0.6991675069967562\n",
      "\n",
      "Input1:> 我 想 通过 一个 叫做 回扣 的 作品 来看 看看 在 阿富汗 的 腐败 并且 模拟 一个 警察 警察局 官员 在 喀布尔 布尔 的 大街 街上 建立 一个 假 的 检查 检查站 然后 把 那些 车 停下 停下来\n",
      "\n",
      "Target1:= I want to look into corruption in Afghanistan through a work called &quot; UNK &quot; and impersonate a police officer , set up a fake UNK on the street of Kabul and stop cars , but instead of \n",
      "Predict1:< I I to to a a a a a a a a a a a a a a a a , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 那 是 20 个 中央 中央公园 公园\n",
      "\n",
      "Target2:= That is 20 Central Parks  . \n",
      "Predict2:< That is a a  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 18s (- -79m 52s), Epoch: [6/10], Step: [420/3335], Train Loss: 3.343266046047211, BLEU: 0.7185497584204155\n",
      "\n",
      "Input1:> 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "\n",
      "Target1:= It was the only way we both could be educated  . \n",
      "Predict1:< It is is we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 现在 如果 你 把 这个 个子 子弹 这些 光子 打入 这个 瓶子 里 这些 光子 子会 怎么 怎么样 撞击 瓶子\n",
      "\n",
      "Target2:= Now , if you take that bullet and take this packet of photons and fire into this bottle , how will those photons shatter into this bottle  ? \n",
      "Predict2:< Now if you you this this , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 24s (- -79m 46s), Epoch: [6/10], Step: [440/3335], Train Loss: 3.328353476524353, BLEU: 0.7781172878139309\n",
      "\n",
      "Input1:> 当个 拿 铲子 的 流氓 怎么 怎么样\n",
      "\n",
      "Target1:= Get gangsta with your shovel , okay  ? \n",
      "Predict1:< The : : is  ?  ?\n",
      "\n",
      "Input2:> 你 可以 又 一次 看到 波纹 在 桌子 上 展开 背景 是 西红柿 和 墙\n",
      "\n",
      "Target2:= You can watch the ripples again washing over the table , the tomato and the wall in the back  . \n",
      "Predict2:< You can see see see , , , , , , , and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 29s (- -79m 40s), Epoch: [6/10], Step: [460/3335], Train Loss: 3.3547001242637635, BLEU: 0.6998667947873027\n",
      "\n",
      "Input1:> 你 知道 的 这样 大概 会 可行\n",
      "\n",
      "Target1:= And , you know , it could have worked  . \n",
      "Predict1:< You know you you it ,  .  .\n",
      "\n",
      "Input2:> 但是 言语 语词 词汇 对于 政治 中心 之 作用 是非 非常 重要 的 并且 所有 的 政客 都 明白 他们 必须 尝试 着 控制 他们 的话 话语 语言 UNK 言论\n",
      "\n",
      "Target2:= But it is very important that words are at the center of politics , and all politicians know they have to try and control language  . \n",
      "Predict2:< But the , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 35s (- -79m 34s), Epoch: [6/10], Step: [480/3335], Train Loss: 3.404417073726654, BLEU: 0.7103217734133207\n",
      "\n",
      "Input1:> 我们 在 巴士 上 的 旅途 有 一周 之久 好几 几次 都 差点 被 抓住\n",
      "\n",
      "Target1:= The journey by bus took one week , and we were almost caught several times  . \n",
      "Predict1:< We we the in in in in in , , , the  .  .\n",
      "\n",
      "Input2:> 在 中国 我 作为 一个 没 家 的 小女 小女孩 女孩 过 得 很 辛苦\n",
      "\n",
      "Target2:= In China , it was hard living as a young girl without my family  . \n",
      "Predict2:< And I was , a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 40s (- -79m 28s), Epoch: [6/10], Step: [500/3335], Train Loss: 3.3802308320999144, BLEU: 0.7218067072743446\n",
      "\n",
      "Input1:> 可以 了\n",
      "\n",
      "Target1:= All right  . \n",
      "Predict1:< It can\n",
      "\n",
      "Input2:> 当 我 在 拍摄 的 途中 我 带 了 很多 蜡烛 在 翻译 的 帮助 下 我 把 蜡烛 分给 了 我 的 拍摄 对象 我 希望 能 点亮 他们 的 故事 和 他们 的 困境 当 我们\n",
      "\n",
      "Target2:= When I was working in the field , I brought lots of candles with me , and with the help of my interpreter , I imparted to the people I was photographing that I wanted to illuminate their \n",
      "Predict2:< When I I my my , , I I , , I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 46s (- -79m 22s), Epoch: [6/10], Step: [520/3335], Train Loss: 3.4340970039367678, BLEU: 0.6150276646615614\n",
      "\n",
      "Input1:> 这 原理 是 我们 可以 把 一些 光 打到 门上\n",
      "\n",
      "Target1:= The idea is that we could shine some light on the door  . \n",
      "Predict1:< This is is the we we can can  .  .  .\n",
      "\n",
      "Input2:> 呃 在 过去 的 几个 世纪 里 我们 定义 里 的 美 不仅 不仅仅 仅仅 仅只 包括 健康 年轻 和 身材 匀称 这些 我们 出于 生物 本能 去 赞美 美的 元素 还 包括 UNK 苗条 的 体型 富有\n",
      "\n",
      "Target2:= Well , for the past few centuries we have defined beauty not just as health and youth and symmetry that we are biologically programmed to admire , but also as tall , slender figures , and UNK and \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 51s (- -79m 17s), Epoch: [6/10], Step: [540/3335], Train Loss: 3.381242573261261, BLEU: 0.6893529752509797\n",
      "\n",
      "Input1:> 纺织 纺织业 也 是 我们 听到 奴役 劳动 时经 经常 联想 想到 的\n",
      "\n",
      "Target1:= The textile industry is another one we often think of when we hear about slave labor  . \n",
      "Predict1:< And , , we we we , we we we  .  .  .\n",
      "\n",
      "Input2:> 中途 不能 休息 不能 吃饭 不能 喝水 由于 极度 的 缺水 他们 基本 基本上 也 不 需要 上 厕所\n",
      "\n",
      "Target2:= There were no breaks for food , no water breaks , and the severe dehydration made urinating pretty much inconsequential  . \n",
      "Predict2:< And can t t t t t t t t t t t t t t t t  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 78m 57s (- -79m 11s), Epoch: [6/10], Step: [560/3335], Train Loss: 3.3610164523124695, BLEU: 0.7369840584136359\n",
      "\n",
      "Input1:> 因此 城市 的 规划 规划者 他们 坐在 一起 讨论 想 把 洛杉矶 UNK 这个 名字 改 的 代表 点 别的 什么 含义 所以 他们 把 它 改成 了 南 洛杉矶 好像 这 将要 解决 这个 城市 真正 的\n",
      "\n",
      "Target1:= So the city planners , they get together and they figure they are going to change the name South Central to make it represent something else , so they change it to South Los Angeles , like this \n",
      "Predict1:< So , they , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 在 未来 当 飞 米 成像 相机 在 你 的 手机 里 的 时候 你 就 可以 去 超市 不用 摸 就 能 检查 检查一下 一下 下水 水果 是否 熟 了\n",
      "\n",
      "Target2:= So , in the future , when this UNK is in your camera phone , you might be able to go to a supermarket and check if the fruit is ripe without actually touching it  . \n",
      "Predict2:< So , the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 2s (- -79m 5s), Epoch: [6/10], Step: [580/3335], Train Loss: 3.377891206741333, BLEU: 0.6412539300779727\n",
      "\n",
      "Input1:> 我们 看到 了 一样 的 症状\n",
      "\n",
      "Target1:= We see all the same symptoms  . \n",
      "Predict1:< We we see the  .  .\n",
      "\n",
      "Input2:> 我 是 一个 非常 典型 的 受害 受害者 还 因为 我家 家庭 家庭暴力 暴力 的 预兆 和 发展 模式 一无所知 无所 所知\n",
      "\n",
      "Target2:= I was also a very typical victim because I knew nothing about domestic violence , its warning signs or its patterns  . \n",
      "Predict2:< I I a a a , , , , , , , , , , and and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 8s (- -80m 59s), Epoch: [6/10], Step: [600/3335], Train Loss: 3.3709843516349793, BLEU: 0.6942824428332627\n",
      "\n",
      "Input1:> 内罗毕 国家 公园 的 南部 并 没有 全部 围起 栅栏 这 就 意味 意味着 像 斑马 这样 的 野生 野生动物 生动 动物 可以 自由 地 在 公园 园外 UNK 移动\n",
      "\n",
      "Target1:= Nairobi National Park is not UNK in the south widely , which means wild animals like zebras migrate out of the park freely  . \n",
      "Predict1:< The the the the , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 也 很 幸运 幸运地 因为 这项 项发明 发明 而 获得 了 奖学 奖学金 这 是 肯尼 肯尼亚 尼亚 最好 的 学校 布鲁克 豪斯 国际 学校 所 颁发 的 我 对此 感到 十分 的 兴奋\n",
      "\n",
      "Target2:= Because of this invention , I was lucky to get a scholarship in one of the best schools in Kenya , UNK International School , and I am really excited about this  . \n",
      "Predict2:< I I was was was , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 13s (- -80m 53s), Epoch: [6/10], Step: [620/3335], Train Loss: 3.4258567333221435, BLEU: 0.7314377166445534\n",
      "\n",
      "Input1:> 当 我 在 修复 那些 照片 的 时候 我 不禁 想到 其中 的 每个 个人 以及 照片 片中 传递 的 那些 故事\n",
      "\n",
      "Target1:= &quot; As I worked , I couldn t help but think about the individuals and the stories represented in the images  . \n",
      "Predict1:< When I I I the the the the the , the , the the , the the the the the\n",
      "\n",
      "Input2:> 他们 究竟 竟为 为什么 什么 觉得 这样 不好 呢\n",
      "\n",
      "Target2:= Why in the hell would they not okay this  ? \n",
      "Predict2:< Why they they they they  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 19s (- -80m 48s), Epoch: [6/10], Step: [640/3335], Train Loss: 3.3879064559936523, BLEU: 0.6781521811189817\n",
      "\n",
      "Input1:> 下 一个 个人 人们 常 问 我 的 问题 是 我 长大 了 能 当 模特 吗\n",
      "\n",
      "Target1:= The next question people always ask me is , &quot; Can I be a model when I grow up  ? &quot; \n",
      "Predict1:< And question question , question , , , , , , , I I  ?  ?  ?\n",
      "\n",
      "Input2:> 赞比亚 比亚 人 说 没错 这 就是 我们 不在 这里 耕种 的 原因\n",
      "\n",
      "Target2:= And the UNK said , &quot; Yes , that is why we have no agriculture here  . &quot; \n",
      "Predict2:< And : , , , , , , , we we we  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 24s (- -80m 42s), Epoch: [6/10], Step: [660/3335], Train Loss: 3.3480257272720335, BLEU: 0.681873848473933\n",
      "\n",
      "Input1:> 这个 个数 数字 是 在 跨 大西 大西洋 西洋 洋奴 奴隶 贸易 中 从 非洲 被 贩卖 的 UNK 总数 的 两倍\n",
      "\n",
      "Target1:= That is double the amount of people taken from Africa during the entire trans-Atlantic slave trade  . \n",
      "Predict1:< This is is the the the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 而 可持续性 持续 持续性 的 搞笑 之处 处在 在于 你 必须 维持 它\n",
      "\n",
      "Target2:= The funny thing about sustainability , you have to sustain it  . \n",
      "Predict2:< And is is , you you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 30s (- -80m 36s), Epoch: [6/10], Step: [680/3335], Train Loss: 3.4788205146789553, BLEU: 0.6641130458570971\n",
      "\n",
      "Input1:> 这些 女孩 是 幸运 的\n",
      "\n",
      "Target1:= These girls were so lucky  . \n",
      "Predict1:< These are are are  .  .\n",
      "\n",
      "Input2:> 当 他 唱起 我们 儿时 的 歌谣 唱出 就 连 我 都 忘记 了 的 歌词 他 提醒 了 我 我们 对 精神 知之甚少 甚少 而 未知 的 事物 是 如此 迷人\n",
      "\n",
      "Target2:= When he sings songs from our childhood , attempting words that not even I could remember , he reminds me of one thing : how little we know about the mind , and how wonderful the unknown must \n",
      "Predict2:< When when when , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 35s (- -80m 30s), Epoch: [6/10], Step: [700/3335], Train Loss: 3.319532346725464, BLEU: 0.7675171781777779\n",
      "\n",
      "Input1:> 现在 从 这个 故事 里 你 能够 了解 到 三件 有趣 的 事情\n",
      "\n",
      "Target1:= Now you can learn three interesting things from this  . \n",
      "Predict1:< Now , is is you you you you you you  .  .\n",
      "\n",
      "Input2:> 超过 70 的 家庭 家庭暴力 暴力 谋杀 发生 生在 受害 受害者 结束 这段 关系 后 在 她 离开 之后 因为 施虐 者 已经 毫无 毫无顾忌 顾忌\n",
      "\n",
      "Target2:= Over 70 percent of domestic violence murders happen after the victim has ended the relationship , after she is gotten out , because then the UNK has nothing left to lose  . \n",
      "Predict2:< The the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 41s (- -80m 25s), Epoch: [6/10], Step: [720/3335], Train Loss: 3.358142340183258, BLEU: 0.6419666333997662\n",
      "\n",
      "Input1:> 他 查看 每个 个人 的 身份 身份证 并 开始 问问 问题\n",
      "\n",
      "Target1:= He took everyone is I .D . cards , and he started asking them questions  . \n",
      "Predict1:< He he the to , , , , , to  .\n",
      "\n",
      "Input2:> 我 这里 有 一个 关于 声 UNK 颤动 的 视频\n",
      "\n",
      "Target2:= So here we have a video of the vibrating vocal folds  . \n",
      "Predict2:< I am a a a a a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 47s (- -80m 19s), Epoch: [6/10], Step: [740/3335], Train Loss: 3.4067959070205687, BLEU: 0.6480941837047701\n",
      "\n",
      "Input1:> 我 不能 披露 太多 我 离开 朝鲜 时 的 细节 但是 我 只能 说 那 是 在 饥荒 中 最 艰难 的 几年 我 被 送到 中国 和 远亲 一起 住\n",
      "\n",
      "Target1:= I can t reveal many details &#91; about &#93; how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives \n",
      "Predict1:< I I I I I I I I I , I , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 以上 这些 就是 我 觉得 能够 帮助 我们 大家 打开 那 扇 大门 的 钥匙 感受 科学 与 工程 工程学 仙境 的 好 方法\n",
      "\n",
      "Target2:= So I think these are just a few keys that can help the rest of us to open that door and see the wonderland that is science and engineering  . \n",
      "Predict2:< And that is , , , , , , , , , , , , , , , , , , , of  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 52s (- -80m 13s), Epoch: [6/10], Step: [760/3335], Train Loss: 3.3951137542724608, BLEU: 0.6351610540325328\n",
      "\n",
      "Input1:> 我 不 打算 跟 你们 细说 这笔 钱 所 造成 的 损害\n",
      "\n",
      "Target1:= I am not going to tell you the damage that that money has done  . \n",
      "Predict1:< I I t you you you you you you the the  .  .\n",
      "\n",
      "Input2:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target2:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict2:< He was was , , , he , , , , , he , , he ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 79m 58s (- -80m 8s), Epoch: [6/10], Step: [780/3335], Train Loss: 3.3349820733070374, BLEU: 0.6462007907083711\n",
      "\n",
      "Input1:> 当然 它 也 有 缺点 这种 路 的 缺点 是 沥青 可能 会 UNK\n",
      "\n",
      "Target1:= It also has disadvantages , of course , and the disadvantage of this road is that UNK can occur  . \n",
      "Predict1:< And course , , , , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 因此 我 想要 做 的 是 想 把 这个 计划 弄 的 更加 吸引 引人\n",
      "\n",
      "Target2:= So what I want to do here , we gotta make this sexy  . \n",
      "Predict2:< So I I to to to to to to to to to  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 3s (- -80m 2s), Epoch: [6/10], Step: [800/3335], Train Loss: 3.3419443130493165, BLEU: 0.6418406186330479\n",
      "\n",
      "Input1:> 所以 做 计划 有 一个 UNK\n",
      "\n",
      "Target1:= So planning has this blind spot  . \n",
      "Predict1:< So is is a a a  .  .\n",
      "\n",
      "Input2:> 我 确实 实有 太多 从没 机会 穿 的 8 英寸 高跟鞋 但是 免费 的 东西 却是 我 在 现实 现实生活 生活 中 得到 的 而 这 恰恰 是 我们 不 愿意 谈 的\n",
      "\n",
      "Target2:= I do have too many UNK heels which I never get to wear , except for earlier , but the free stuff that I get is the free stuff that I get in real life , and that \n",
      "Predict2:< I I t to the of , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 9s (- -81m 56s), Epoch: [6/10], Step: [820/3335], Train Loss: 3.448560285568237, BLEU: 0.7147738427172655\n",
      "\n",
      "Input1:> 感应 UNK 电机 可以 加热 东西 尤其 擅长 加热 钢铁\n",
      "\n",
      "Target1:= UNK can heat , especially steel ; it is very good at that  . \n",
      "Predict1:< And can can , , , , ,  .  .\n",
      "\n",
      "Input2:> 我 的 行为 很 有 可能 会 让 他们 的 境遇 比 现在 更糟\n",
      "\n",
      "Target2:= I could get them in a worse situation than they were already in  . \n",
      "Predict2:< I I them that , , they they  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 14s (- -81m 51s), Epoch: [6/10], Step: [840/3335], Train Loss: 3.349369192123413, BLEU: 0.712812206764069\n",
      "\n",
      "Input1:> 他 面带 面带笑容 带笑 笑容 这 很少 少见 因为 大部 大部分 部分 的 新闻 都 只 会 使 他 沮丧\n",
      "\n",
      "Target1:= There was a big smile on his face which was unusual then , because the news mostly depressed him  . \n",
      "Predict1:< He is he he , , , , , he , , , he ,  .\n",
      "\n",
      "Input2:> 我 希望 我们 给 这些 被 奴隶 的 人 一道 道光\n",
      "\n",
      "Target2:= I want to shine a light on slavery  . \n",
      "Predict2:< I I to to to to of of of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 20s (- -81m 45s), Epoch: [6/10], Step: [860/3335], Train Loss: 3.512873983383179, BLEU: 0.7595823853042407\n",
      "\n",
      "Input1:> 当 他们 遭受 海啸 袭击 之时 可想 可想而知 必然 四处 四处奔波 奔波 寻求 一线 一线生机 生机 自然 把 其他 东西 他们 的 所有 东西 西都 抛 诸 脑 后\n",
      "\n",
      "Target1:= As they had run from the wave , and for their lives , absolutely everything they had , everything had to be left behind  . \n",
      "Predict1:< When when they they to the , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 如果 想见 我 就 拿 着 UNK 来 花园 吧 这样 我们 就 可以 种 点 东西 啦\n",
      "\n",
      "Target2:= If you want to meet with me , come to the garden with your shovel so we can plant some shit  . \n",
      "Predict2:< If if I I if I , , , , , we , we ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 25s (- -81m 39s), Epoch: [6/10], Step: [880/3335], Train Loss: 3.3630005717277527, BLEU: 0.6815466908983223\n",
      "\n",
      "Input1:> 回到 我 的 问题 为什么 什么 我 要 留下 留下来 下来 了\n",
      "\n",
      "Target1:= Back to my question : Why did I stay  ? \n",
      "Predict1:< And , I I I I I I I I\n",
      "\n",
      "Input2:> 这 原理 是 我们 可以 把 一些 光 打到 门上\n",
      "\n",
      "Target2:= The idea is that we could shine some light on the door  . \n",
      "Predict2:< And is is we we we we can  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 31s (- -81m 34s), Epoch: [6/10], Step: [900/3335], Train Loss: 3.419933593273163, BLEU: 0.7286198707946171\n",
      "\n",
      "Input1:> 所以 现在 的 沥青 有 一些 些小 钢丝 丝绒 在 里面\n",
      "\n",
      "Target1:= So then you have asphalt with very small pieces of steel wool in it  . \n",
      "Predict1:< So now are are are the  .  .  .  .\n",
      "\n",
      "Input2:> 我 嫁给 我 的 第二 UNK 丈夫 UNK 差不多 不多 20 年 了 并 育 有 3 个 孩子\n",
      "\n",
      "Target2:= I ve been married for almost 20 years to my second husband and we have three kids together  . \n",
      "Predict2:< I I my , I , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 36s (- -81m 28s), Epoch: [6/10], Step: [920/3335], Train Loss: 3.5758996844291686, BLEU: 0.7170500075776862\n",
      "\n",
      "Input1:> 现在 如果 我 用 一个 激光 光笔 并且 在 万亿 亿分 之一 一秒 内 开关 一次 也 就是 几 飞秒 我 就 会 制造 一些 光子 这些 光子 仅仅 是 几毫米 毫米 宽 并且 这些 光子 像 子弹\n",
      "\n",
      "Target1:= Now if I take a laser pointer and turn it on and off in one UNK of a second -- which is several femtoseconds -- I ll create a packet of photons barely a millimeter wide , and \n",
      "Predict1:< Now , I I a a , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 真的 被 吓坏 了 这些 学生 生发 发达 的 大脑 还有 他们 所 读 的 巨著 及 他们 使用 的 又 长 又 UNK 的 词语 通通 通都 吓到 我 了\n",
      "\n",
      "Target2:= Really scared  . Scared of these students with their big brains and their big books and their big , unfamiliar words  . \n",
      "Predict2:< And I was the the , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 42s (- -81m 22s), Epoch: [6/10], Step: [940/3335], Train Loss: 3.4319066286087034, BLEU: 0.6855774547767597\n",
      "\n",
      "Input1:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target1:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict1:< And , me , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 无条件 条件 UNK UNK 保留 保留地 分享 他 的 那份 爱\n",
      "\n",
      "Target2:= He shares it unconditionally and he shares it regardless  . \n",
      "Predict2:< He he he to his his , , his his  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 47s (- -81m 17s), Epoch: [6/10], Step: [960/3335], Train Loss: 3.4308770537376403, BLEU: 0.6567054335701205\n",
      "\n",
      "Input1:> 因此 我 觉得 太好了 这 是 我 的 责任 我 必须 维护 它 所以 我 想 做 什么 就 可以 做 什么 了\n",
      "\n",
      "Target1:= So I am like , &quot; Cool  . I can do whatever the hell I want , since it is my responsibility and I gotta maintain it  . &quot; \n",
      "Predict1:< So I I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "Input2:> 再加 加上 我 着实 厌倦 了 开 45 分钟 的 来回 去 买 个 没 打过 杀虫 杀虫剂 的 苹果\n",
      "\n",
      "Target2:= Plus I got tired of driving 45 minutes round trip to get an apple that wasn t impregnated with pesticides  . \n",
      "Predict2:< I I I I I a a , , , , , , , to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 53s (- -81m 11s), Epoch: [6/10], Step: [980/3335], Train Loss: 3.326988196372986, BLEU: 0.7236863312115759\n",
      "\n",
      "Input1:> 五年 年前 我 曾 有 过 一个 有如 漫游 爱丽丝 仙境 的 经历\n",
      "\n",
      "Target1:= Five years ago , I experienced a bit of what it must have been like to be Alice in Wonderland  . \n",
      "Predict1:< I was a a , I a a a  .  .\n",
      "\n",
      "Input2:> 秃鹫 是 他们 中 最 濒临 濒临灭绝 灭绝 的 一种\n",
      "\n",
      "Target2:= UNK are amongst the most threatened group of birds  . \n",
      "Predict2:< The is the of of of of  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 80m 58s (- -81m 5s), Epoch: [6/10], Step: [1000/3335], Train Loss: 3.3763888001441957, BLEU: 0.7177417488681991\n",
      "\n",
      "Input1:> 所以 这些 照片 远远 远不止 不止 是 记录 了 一个 瞬间 或 一次 特别 别的 旅程\n",
      "\n",
      "Target1:= So these photos are far more than proxies for a single moment , or even a specific trip  . \n",
      "Predict1:< So so is a a a , , , , , , , ,  .\n",
      "\n",
      "Input2:> 这 是 初次 尝试 的 一张 照片\n",
      "\n",
      "Target2:= This was the very first  . \n",
      "Predict2:< This is a a of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 4s (- -81m 0s), Epoch: [6/10], Step: [1020/3335], Train Loss: 3.279190647602081, BLEU: 0.6907827809295363\n",
      "\n",
      "Input1:> 他 只是 压力 太大 了 婚礼 的 筹备 和 我家 家庭 家庭成员 成员 的 到来 让 他 喘 不过 过气\n",
      "\n",
      "Target1:= He had just been really stressed out by the wedding and by becoming a family with me  . \n",
      "Predict1:< He he a his his and and and and and and and and and and and and  .\n",
      "\n",
      "Input2:> 有 一天 晚上 我 拿 着手 手电 手电筒 电筒 在 牛棚 边 走动 那天 晚上 狮子 并 没有 来\n",
      "\n",
      "Target2:= So one night , I was walking around the cowshed with a torch , and that day , the lions didn t come  . \n",
      "Predict2:< And was , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 9s (- -82m 54s), Epoch: [6/10], Step: [1040/3335], Train Loss: 3.3351287245750427, BLEU: 0.6826221600428916\n",
      "\n",
      "Input1:> 当 他 唱起 我们 儿时 的 歌谣 唱出 就 连 我 都 忘记 了 的 歌词 他 提醒 了 我 我们 对 精神 知之甚少 甚少 而 未知 的 事物 是 如此 迷人\n",
      "\n",
      "Target1:= When he sings songs from our childhood , attempting words that not even I could remember , he reminds me of one thing : how little we know about the mind , and how wonderful the unknown must \n",
      "Predict1:< When when when the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 初次 尝试 的 一张 照片\n",
      "\n",
      "Target2:= This was the very first  . \n",
      "Predict2:< This is a a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 15s (- -82m 49s), Epoch: [6/10], Step: [1060/3335], Train Loss: 3.4470112442970278, BLEU: 0.6998119274852834\n",
      "\n",
      "Input1:> 援助 的 首要 原则 就是 尊重\n",
      "\n",
      "Target1:= The first principle of aid is respect  . \n",
      "Predict1:< And is is is is  .  .\n",
      "\n",
      "Input2:> 奈 库 贝特 女神 在 古埃及 埃及 埃及文 文化 中 象征 着 监护 监护人 护人 和 母性 通常 和 眼镜 眼镜蛇 女神 维 阿 杰 特 一起 出现 象征 着上 埃及 和 下 埃及 的 团结 和 统一\n",
      "\n",
      "Target2:= UNK was the symbol of the protector and the motherhood , and together with the cobra , symbolized the unity between Upper and Lower Egypt  . \n",
      "Predict2:< It , , , , the , the , , , , , , , , , , , , , , and , and and and and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 21s (- -82m 43s), Epoch: [6/10], Step: [1080/3335], Train Loss: 3.4590395092964172, BLEU: 0.7016810391387593\n",
      "\n",
      "Input1:> 因为 我 的 家人 不 懂 中文 所以 我 得 帮助 他们 在 中国 穿越 2000 多公里 公里 进入 东南 东南亚 南亚\n",
      "\n",
      "Target1:= Since my family couldn t speak Chinese , I had to guide them , somehow , through more than 2,000 miles in China and then into Southeast Asia  . \n",
      "Predict1:< Because I I I I , , , , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 但是 现在 你 知道 现在 有 多少 个 国家 有 总统 吗\n",
      "\n",
      "Target2:= But now , do you know how many nations have a president  ? \n",
      "Predict2:< But now you you you you you you you  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 26s (- -82m 37s), Epoch: [6/10], Step: [1100/3335], Train Loss: 3.362979781627655, BLEU: 0.7094546483072095\n",
      "\n",
      "Input1:> 纺织 纺织业 也 是 我们 听到 奴役 劳动 时经 经常 联想 想到 的\n",
      "\n",
      "Target1:= The textile industry is another one we often think of when we hear about slave labor  . \n",
      "Predict1:< And , , we we we we we we  .  .  .\n",
      "\n",
      "Input2:> 一些 光 流 了 出来 到 了 桌子 上 所以 你 开始 看到 这些 波纹\n",
      "\n",
      "Target2:= Some of the light leaks , goes on the table , and you start seeing these ripples of waves  . \n",
      "Predict2:< And you see , , , , you , you you you you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 32s (- -82m 32s), Epoch: [6/10], Step: [1120/3335], Train Loss: 3.3790847301483153, BLEU: 0.6948101525101935\n",
      "\n",
      "Input1:> 我 在 早上 五点 时 看到 这些 男孩 男孩子 孩子 们 在 收 最后 一个 渔网 可 他们 从 凌晨 1 点 就 开始 工作 了\n",
      "\n",
      "Target1:= I met these boys at five in the morning , when they were hauling in the last of their nets , but they had been working since 1 a .m . \n",
      "Predict1:< I I I in in , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但是 我 从没 没想 没想到 想到 经历 了 14 年 我们 才 得以 重聚\n",
      "\n",
      "Target2:= I could have never imagined that it would take 14 years to live together  . \n",
      "Predict2:< But I I , , , , we we we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 37s (- -82m 26s), Epoch: [6/10], Step: [1140/3335], Train Loss: 3.3409808039665223, BLEU: 0.7310758116490348\n",
      "\n",
      "Input1:> 当 西红柿 成熟 了 UNK 的 好看 极了 一夜 一夜之间 之间 从 UNK 比西 西河 跑 来 两百 两百多 百多 多头 河马 把 所有 西红柿 都 啃 光 了\n",
      "\n",
      "Target1:= When the tomatoes were nice and ripe and red , overnight , some 200 hippos came out from the river and they ate everything  . \n",
      "Predict1:< When the the the the , , , , , , , , , , , , , , , , , , , the the the the\n",
      "\n",
      "Input2:> 看看 以 一位 位非 非洲 女性 的 眼光 我们 所 带来 的 损害 有 多 大\n",
      "\n",
      "Target2:= Read it from an African woman , the damage that we have done  . \n",
      "Predict2:< And the the the the , the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 43s (- -82m 21s), Epoch: [6/10], Step: [1160/3335], Train Loss: 3.3738571286201475, BLEU: 0.7375251145536142\n",
      "\n",
      "Input1:> 但 这 就是 自我 的 形成\n",
      "\n",
      "Target1:= But this is the geography of self  . \n",
      "Predict1:< But that is is the  .  .  .\n",
      "\n",
      "Input2:> 因为 我 的 家人 不 懂 中文 所以 我 得 帮助 他们 在 中国 穿越 2000 多公里 公里 进入 东南 东南亚 南亚\n",
      "\n",
      "Target2:= Since my family couldn t speak Chinese , I had to guide them , somehow , through more than 2,000 miles in China and then into Southeast Asia  . \n",
      "Predict2:< Because I I I , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 48s (- -82m 15s), Epoch: [6/10], Step: [1180/3335], Train Loss: 3.365334463119507, BLEU: 0.7134762946465876\n",
      "\n",
      "Input1:> 我 原本 以为 世界 上 只有 我 一个 个人 会 继续 留在 一个 一个打 我 的 男人 人身 身边 但是 事实 事实上 在 我 这个 年龄 年龄段 这种 事情 非常 普遍\n",
      "\n",
      "Target1:= I would have told you myself that I was the last person on Earth who would stay with a man who beats me , but in fact I was a very typical victim because of my age  . \n",
      "Predict1:< I I a a a , , , , , , I , I , I , I , I , I I , I I I I I\n",
      "\n",
      "Input2:> 一美元 美元 的 绿豆 可以 以为 你 带来 75 美元 的 收成\n",
      "\n",
      "Target2:= When one dollar is worth of green beans will give you 75 dollars  worth of produce  . \n",
      "Predict2:< And , , , , you , , you ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 54s (- -82m 9s), Epoch: [6/10], Step: [1200/3335], Train Loss: 3.337033474445343, BLEU: 0.7671440312640401\n",
      "\n",
      "Input1:> 在 南亚 一些 国家 类似 于 印度 和 巴基 巴基斯 巴基斯坦 基斯 基斯坦 四种 秃鹫 被 列为 极度 濒临 濒临灭绝 灭绝 中 这 意味 意味着 在 他们 将 会 在 10 到 15 年 之内 彻底 灭绝 灭绝\n",
      "\n",
      "Target1:= In South Asia , in countries like India and Pakistan , four species of vultures are listed as critically endangered , which means they have less than 10 or 15 years to go extinct , and the reason \n",
      "Predict1:< In the , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但 我 又 意识 到 这 并 没有 什么 成效 反倒 还 帮助 了 狮子 看到 了 我们 的 牛棚\n",
      "\n",
      "Target2:= But I came to realize that that didn t really help , because it was even helping the lions to see through the cowshed  . \n",
      "Predict2:< But I don t t that , , , , , , , , we  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 81m 59s (- -82m 4s), Epoch: [6/10], Step: [1220/3335], Train Loss: 3.3627277612686157, BLEU: 0.7299727691418864\n",
      "\n",
      "Input1:> 第一 第一个 一个 要 回答 我们 的 问题 高科技 科技 又 怎样\n",
      "\n",
      "Target1:= First question to answer for us : so what  ? \n",
      "Predict1:< The first question , question , we we we we we\n",
      "\n",
      "Input2:> 但是 很快 就 有人 抱怨\n",
      "\n",
      "Target2:= And then somebody complained  . \n",
      "Predict2:< But it is ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 5s (- -83m 58s), Epoch: [6/10], Step: [1240/3335], Train Loss: 3.266325032711029, BLEU: 0.7190454962866476\n",
      "\n",
      "Input1:> 社区 会议 存在 一个 问题\n",
      "\n",
      "Target1:= There is a problem with community meetings  . \n",
      "Predict1:< There is a a problem\n",
      "\n",
      "Input2:> 这 正是 我 想要 的\n",
      "\n",
      "Target2:= That is the whole idea  . \n",
      "Predict2:< And that is I I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 10s (- -83m 53s), Epoch: [6/10], Step: [1260/3335], Train Loss: 3.5840203285217287, BLEU: 0.6882441119486247\n",
      "\n",
      "Input1:> 但是 我 从没 没想 没想到 想到 经历 了 14 年 我们 才 得以 重聚\n",
      "\n",
      "Target1:= I could have never imagined that it would take 14 years to live together  . \n",
      "Predict1:< But I I , , , , , we we we  .  .  .  .\n",
      "\n",
      "Input2:> 她 的 这些 照片 其实 实是 由 她 丈夫 的 同事 事发 发现 的 在 当地 的 消防 防部 部队 的 瓦砾 下发 发现 的 那个 地方 其实 离 她家 很远 他们 居然 认出 了 他\n",
      "\n",
      "Target2:= Her photos were found by her husband is colleagues at a local fire department in the debris a long way from where the home had once stood , and they d recognized him  . \n",
      "Predict2:< And she , , , , , , , , , , , , the , , the , , the , the , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 16s (- -83m 47s), Epoch: [6/10], Step: [1280/3335], Train Loss: 3.35230153799057, BLEU: 0.7504622857641043\n",
      "\n",
      "Input1:> 但是 总 需要 有人 维护 它\n",
      "\n",
      "Target1:= But you have to maintain it  . \n",
      "Predict1:< But it is it to  .  .\n",
      "\n",
      "Input2:> 尽管 我们 都 是 朝鲜 UNK 族人 但是 我们 的 内在 已经 经变 变得 非常 不同 了 由于 67 年 的 分裂\n",
      "\n",
      "Target2:= We are all Korean , but inside , we have become very different due to 67 years of division  . \n",
      "Predict2:< But we we we we , , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 21s (- -83m 41s), Epoch: [6/10], Step: [1300/3335], Train Loss: 3.4756779789924623, BLEU: 0.7175578213949272\n",
      "\n",
      "Input1:> 在 死 之前 我 想为 我 的 海盗 UNK 行为 接受 受审 审判\n",
      "\n",
      "Target1:= &quot; Before I die , I want to be tried for piracy  . &quot; \n",
      "Predict1:< In , , I I I I I I I  .\n",
      "\n",
      "Input2:> 在 死 之前 我 想成 成为 某个 个人 的 骑士\n",
      "\n",
      "Target2:= &quot; Before I die , I want to be someone is cavalry  . &quot; \n",
      "Predict2:< And I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 27s (- -83m 36s), Epoch: [6/10], Step: [1320/3335], Train Loss: 3.320980632305145, BLEU: 0.7571768488204842\n",
      "\n",
      "Input1:> 声音 颤动 虚弱 和 僵化\n",
      "\n",
      "Target1:= We see vocal tremor , weakness and rigidity  . \n",
      "Predict1:< The : : The  .  .  .\n",
      "\n",
      "Input2:> 但 这 就是 自我 的 形成\n",
      "\n",
      "Target2:= But this is the geography of self  . \n",
      "Predict2:< But this is is the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 33s (- -83m 30s), Epoch: [6/10], Step: [1340/3335], Train Loss: 3.470785665512085, BLEU: 0.7713146230872678\n",
      "\n",
      "Input1:> 这 看上 看上去 上去 就 像 这样\n",
      "\n",
      "Target1:= It will look something like this  . \n",
      "Predict1:< It is like like like  .  .\n",
      "\n",
      "Input2:> 事实 事实上 我 要 先 让 它 冷却 下来 才能 向 你们 展示 加热 修复 是否 有效\n",
      "\n",
      "Target2:= Actually , it still has to cool down first before I can show you that the healing works  . \n",
      "Predict2:< And fact , I I to to to to to to to to to to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 38s (- -83m 25s), Epoch: [6/10], Step: [1360/3335], Train Loss: 3.4675057768821715, BLEU: 0.693090518143611\n",
      "\n",
      "Input1:> 一支 放在 床 的 枕头 下面 第三 第三支 三支 则 一直 放在 口袋 里\n",
      "\n",
      "Target1:= He kept one under the pillows on our bed , and the third one he kept in his pocket at all times  . \n",
      "Predict1:< The the the , , , , , , , the the  .\n",
      "\n",
      "Input2:> 因此 我 做 的 就是 在 我 的 房子 前 种 上 食物 森林\n",
      "\n",
      "Target2:= So what I did , I planted a food forest in front of my house  . \n",
      "Predict2:< So I I I I I I the the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 44s (- -83m 19s), Epoch: [6/10], Step: [1380/3335], Train Loss: 3.343624937534332, BLEU: 0.6801690588638576\n",
      "\n",
      "Input1:> 正好 相反 我 是 一个 很 强硬 的 女性 深爱 深爱着 这个 饱受 受困 困扰 的 男人 而且 我 是 这 世上 唯一 唯一一 一一 一个 可以 帮助 康 纳 面对 自己 心 魔 的 人\n",
      "\n",
      "Target1:= Instead , I was a very strong woman in love with a deeply troubled man , and I was the only person on Earth who could help UNK face his demons  . \n",
      "Predict1:< And , , , I I , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 会 发生 什么 呢\n",
      "\n",
      "Target2:= So what happened  ? \n",
      "Predict2:< What does what  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 49s (- -83m 13s), Epoch: [6/10], Step: [1400/3335], Train Loss: 3.330204939842224, BLEU: 0.6868755788223608\n",
      "\n",
      "Input1:> 在 路 的 尽头 他 催促 我们 下车 让 司机 赶紧 走\n",
      "\n",
      "Target1:= At the end of the road , he urged us out of the car , and told the driver to quickly leave  . \n",
      "Predict1:< And the the , , we we we we to  .  .  .\n",
      "\n",
      "Input2:> 在 加德 加德满都 我 由 一些 从前 是 性 奴 的 女人 随 护\n",
      "\n",
      "Target2:= In Kathmandu , I was escorted by women who had previously been sex slaves themselves  . \n",
      "Predict2:< In the , , , , I , the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 82m 55s (- -83m 8s), Epoch: [6/10], Step: [1420/3335], Train Loss: 3.301334047317505, BLEU: 0.6460125833908913\n",
      "\n",
      "Input1:> 在 50 UNK 摄氏 摄氏度 的 高温 中 男人 女人 小孩 事实 事实上 是 一整 整个 家庭 在 厚厚 厚厚的 灰尘 笼罩 下 机械 地 把 砖块 往 自己 的 头顶 顶上 上垒 每次 叠 上 18 块\n",
      "\n",
      "Target1:= UNK in temperatures of 130 degrees , men , women , children , entire families in fact , were cloaked in a heavy blanket of dust , while mechanically stacking bricks on their head , up to 18 \n",
      "Predict1:< In the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这些 井 有 差不多 不多 有 90 多米 深 他们 抬出 抬出来 出来 装满 UNK 的 袋子 这些 石头 被 运送 送到 另外 一个 区域 被 捣碎 以便 提取 金子\n",
      "\n",
      "Target2:= The shafts are up to 300 feet deep , and they carry out heavy bags of stone that later will be transported to another area , where the stone will be pounded so that they can extract the \n",
      "Predict2:< These are are of of , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 0s (- -83m 2s), Epoch: [6/10], Step: [1440/3335], Train Loss: 3.360919439792633, BLEU: 0.6962401774169221\n",
      "\n",
      "Input1:> 我们 能 减少 和 病人 有关 的 后勤 问题\n",
      "\n",
      "Target1:= We can reduce logistical difficulties with patients  . \n",
      "Predict1:< We can can the the the  .  .\n",
      "\n",
      "Input2:> 但 最难 的 是 去 展现 性别 和 种族 种族歧视 歧视 的 职业 传统 因为 我 自己 本身 是 最大 的 受益 受益者\n",
      "\n",
      "Target2:= But mostly it was difficult to unpack a legacy of gender and racial oppression when I am one of the biggest beneficiaries  . \n",
      "Predict2:< But the is the the , , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 6s (- -84m 57s), Epoch: [6/10], Step: [1460/3335], Train Loss: 3.3981236934661867, BLEU: 0.7666595325611987\n",
      "\n",
      "Input1:> 希望 不 像 中间 那 张 那么 怪\n",
      "\n",
      "Target1:= Hopefully less awkward than that one in the middle  . \n",
      "Predict1:< It , t like , , ,  .  .  .\n",
      "\n",
      "Input2:> 我 想到 的 第一 第一个 一个 办法 是 使用 火 因为 我 知道 狮子 们 都 惧怕 火\n",
      "\n",
      "Target2:= And the first idea I got was to use fire , because I thought lions were scared of fire  . \n",
      "Predict2:< The first was I I , I because , because , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 11s (- -84m 51s), Epoch: [6/10], Step: [1480/3335], Train Loss: 3.3309827208518983, BLEU: 0.7942964799176019\n",
      "\n",
      "Input1:> 我们 开始 交往 他 喜欢 关于 我 的 一切 例如 我 的 聪明 我 在 哈佛 的 求学 经历 我 给予 青少 青少年 少年 女性 的 热情 UNK 帮助 以及 我 的 工作\n",
      "\n",
      "Target1:= We started dating , and he loved everything about me , that I was smart , that I d gone to Harvard , that I was passionate about helping teenage girls , and my job  . \n",
      "Predict1:< We had to to my my , , my , , my , , , , , , my , , , , my , , my , , my , my\n",
      "\n",
      "Input2:> 他们 中 的 其中 一个 就是 这位 老奶奶 奶奶\n",
      "\n",
      "Target2:= One of them was this grandmother  . \n",
      "Predict2:< And they of they they they  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 17s (- -84m 45s), Epoch: [6/10], Step: [1500/3335], Train Loss: 3.401806342601776, BLEU: 0.6476857997324905\n",
      "\n",
      "Input1:> 我 记 不 起 第一 第一次 一次 知道 孤独 孤独症 这个 词 的 情形 我 也 记 不 起 有 哪一天 一天 我 是 对 孤独 孤独症 一无所知 无所 所知 的\n",
      "\n",
      "Target1:= And I cannot remember the first moment I encountered autism , but I cannot recall a day without it  . \n",
      "Predict1:< I I I know I I , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 在 为 这次 演讲 做 调查 时 发现 现在 美国 13 岁 女孩 女孩儿 孩儿 群体 中 百分 百分之 53 不 喜欢 她们 的 身体 到 了 17 岁 这 一比 比例 上升 升至 百分 百分之 78\n",
      "\n",
      "Target2:= When I was researching this talk , I found out that of the 13-year-old girls in the United States , 53 percent don t like their bodies , and that number goes to 78 percent by the time \n",
      "Predict2:< I I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 22s (- -84m 40s), Epoch: [6/10], Step: [1520/3335], Train Loss: 3.3854074716567992, BLEU: 0.6734551463243152\n",
      "\n",
      "Input1:> 秃鹫 还 曾经 被 迪斯尼 看上 像 高飞 狗 一样 被 人格 人格化 变成 了 麻木 的 愚蠢 的 动画 UNK 角色\n",
      "\n",
      "Target1:= They ve also be associated with Disney — — personified as goofy , dumb , stupid characters  . \n",
      "Predict1:< And the , , , the , , , , , , , the the  .\n",
      "\n",
      "Input2:> 所以 你 已经 拿到 了 这个 奖学 奖学金 Richard UNK 是 的\n",
      "\n",
      "Target2:= So you got this scholarship  . Yep  . \n",
      "Predict2:< So you you you you the the , the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 28s (- -84m 34s), Epoch: [6/10], Step: [1540/3335], Train Loss: 3.3883564352989195, BLEU: 0.711890931738857\n",
      "\n",
      "Input1:> 那么 最后 一个 问题 是 当 模特 是 什么 感觉\n",
      "\n",
      "Target1:= So the last question people ask me is , &quot; What is it like to be a model  ? &quot; \n",
      "Predict1:< So , question question question question , is is is  ?  ?\n",
      "\n",
      "Input2:> 而且 每 一个 个人 都 感到 神经 神经错乱 错乱 似地 厌烦 了 事实 事实上 因为 这场 辩论 持续 了 3 个 礼拜\n",
      "\n",
      "Target2:= And everybody got insanely bored , actually , because this debate went on for three weeks  . \n",
      "Predict2:< And it is a , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 34s (- -84m 29s), Epoch: [6/10], Step: [1560/3335], Train Loss: 3.2258586049079896, BLEU: 0.7565955723031372\n",
      "\n",
      "Input1:> 我 不知 知道 那 意味 意味着 什么 但是 我 能 看出 父亲 非常 非常 非常高兴 高兴\n",
      "\n",
      "Target1:= I didn t know what it meant , but I could see that my father was very , very happy  . \n",
      "Predict1:< I don t know I I I , , , , , , , it , it  .\n",
      "\n",
      "Input2:> 我们 能 为 临床 临床试验 试验 组织 大量 征召 且 耗费 很 低 我们 也 能 使 人数 的 规模 的 展示 在 第一 一时 一时间 时间 变得 可行\n",
      "\n",
      "Target2:= We can perform low-cost mass recruitment for clinical trials , and we can make UNK screening feasible for the first time  . \n",
      "Predict2:< We can we the the the , , , , , , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 39s (- -84m 23s), Epoch: [6/10], Step: [1580/3335], Train Loss: 3.389696145057678, BLEU: 0.6776440044722949\n",
      "\n",
      "Input1:> 我 获得 了 乘飞机 飞机 来 TED 的 机会 这 是 我 的 第一 第一次 一次\n",
      "\n",
      "Target1:= I got a chance to come by plane for my first time for TED  . \n",
      "Predict1:< I I the the the the the I I I I  .  .  .\n",
      "\n",
      "Input2:> 和 你 的 孩子 你 的 同事 你 的 朋友 和 家人 讨论 这个 话题 将 虐待 UNK 之于 光\n",
      "\n",
      "Target2:= Show abuse the light of day by talking about it with your children , your coworkers , your friends and family  . \n",
      "Predict2:< And you your your your , and your and and your and and your and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 45s (- -84m 18s), Epoch: [6/10], Step: [1600/3335], Train Loss: 3.354289948940277, BLEU: 0.7030364165678611\n",
      "\n",
      "Input1:> 在 韩国 国定 定居 比 我 想象 的 更加 困难 并且 充满 了 挑战\n",
      "\n",
      "Target1:= Settling down in South Korea was a lot more challenging than I had expected  . \n",
      "Predict1:< In the , , , , , , , , I ,\n",
      "\n",
      "Input2:> 然后 他 跟 我 解释 到 在 这样 的 地方 抒发 发自 自己 的 情绪 是 很 危险 的 不仅 对 我 而言 危险 对 这些 工人 人们 也 是\n",
      "\n",
      "Target2:= And he very clearly explained to me that emotional displays are very dangerous in a place like this , not just for me , but for them  . \n",
      "Predict2:< And then I , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 50s (- -84m 12s), Epoch: [6/10], Step: [1620/3335], Train Loss: 3.4580812573432924, BLEU: 0.6982794136127327\n",
      "\n",
      "Input1:> 这 是 一个 意外 而 他 以后 不会 再 伤害 我\n",
      "\n",
      "Target1:= It was an isolated incident , and he was never going to hurt me again  . \n",
      "Predict1:< It is a a , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 这 是 我 的 信条 我 告诉 人们 种植 你们 自己 的 食物 吧\n",
      "\n",
      "Target2:= It is my gospel , when I am telling people , grow your own food  . \n",
      "Predict2:< And is is my I I I I I I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 83m 56s (- -84m 6s), Epoch: [6/10], Step: [1640/3335], Train Loss: 3.2957476139068604, BLEU: 0.7676987371193345\n",
      "\n",
      "Input1:> 当 摄影 摄影师 要 我 弓起 背 摸 那个 男士 的 头发 时 我 非常 不自 不自在 自在\n",
      "\n",
      "Target1:= I was totally uncomfortable , and the photographer was telling me to arch my back and put my hand in that guy is hair  . \n",
      "Predict1:< When I I I , , , , , , , , I , I I\n",
      "\n",
      "Input2:> 我 一点 一点儿 点儿 也 不明 明白 UNK 是 什么 意思\n",
      "\n",
      "Target2:= Now I have no idea what &quot; UNK &quot; is  . \n",
      "Predict2:< I I t to , is , is is  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 1s (- -84m 1s), Epoch: [6/10], Step: [1660/3335], Train Loss: 3.261260139942169, BLEU: 0.7856499594850308\n",
      "\n",
      "Input1:> 你 再 需要 一个 像 这样 的 机器 一个 感应 UNK 电机 它 可以 用来 做饭\n",
      "\n",
      "Target1:= Then you need a machine , like you see here , that you can use for cooking -- an induction machine  . \n",
      "Predict1:< You you to to a a a a a a a a a a a a  .\n",
      "\n",
      "Input2:> 所以 我们 要 做 的 就是 把 钢铁 加热 把 沥青 溶化 沥青 就 会 流进 那些 些小 裂缝 缝里 脱落 的 小石 UNK 石子 就 又 被 粘上 了\n",
      "\n",
      "Target2:= Then what you do is you heat up the steel , you melt the bitumen , and the bitumen will flow into these UNK , and the stones are again fixed to the surface  . \n",
      "Predict2:< So we we we to to to to to to , , , , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 7s (- -85m 55s), Epoch: [6/10], Step: [1680/3335], Train Loss: 3.2261544704437255, BLEU: 0.779702782934823\n",
      "\n",
      "Input1:> 怎么 会 有人 人愿 愿意 和 一个 一个打 她 的 男人 继续 住在一起 一起\n",
      "\n",
      "Target1:= Why would anyone stay with a man who beats her  ? \n",
      "Predict1:< How she her to her her and and and  .  .\n",
      "\n",
      "Input2:> 这些 年 他 接受 受过 很多 治疗 也 有 很大 的 进步\n",
      "\n",
      "Target2:= So great progress and treatment has been made over the years  . \n",
      "Predict2:< And are , , of , , , of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 12s (- -85m 50s), Epoch: [6/10], Step: [1700/3335], Train Loss: 3.2738405227661134, BLEU: 0.7664715321038339\n",
      "\n",
      "Input1:> 很多 人 都 淹死 了\n",
      "\n",
      "Target1:= Many of them drown  . \n",
      "Predict1:< Many many people people  .\n",
      "\n",
      "Input2:> 所以 不管 是 看到 拐角 还是 为 下一代 一代 制造 健康 影像 还是 增加 新 的 可视 可视化 自从 我们 的 发明 我们 就 已经 把 所有 的 资料 和 细节 放到 了 我们 的 网上 并且 我们\n",
      "\n",
      "Target2:= So whether it is for photography around corners , or creating the next generation of health imaging , or creating new visualizations , since our invention , we have open-sourced all the data and details on our website \n",
      "Predict2:< So , is the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 18s (- -85m 44s), Epoch: [6/10], Step: [1720/3335], Train Loss: 3.3522937059402467, BLEU: 0.7936996464423758\n",
      "\n",
      "Input1:> 又 一次 我们 坐 的 巴士 被 拦下 了 一个 中国 警官 上 了 车\n",
      "\n",
      "Target1:= One time , our bus was stopped and boarded by a Chinese police officer  . \n",
      "Predict1:< And we we we , , , , , , the the  .  .  .\n",
      "\n",
      "Input2:> 他们 中 的 其中 一个 就是 这位 老奶奶 奶奶\n",
      "\n",
      "Target2:= One of them was this grandmother  . \n",
      "Predict2:< And they they they they is  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 23s (- -85m 39s), Epoch: [6/10], Step: [1740/3335], Train Loss: 3.342925012111664, BLEU: 0.772659994980371\n",
      "\n",
      "Input1:> 一支 放在 车子 的 置 物 箱 里\n",
      "\n",
      "Target1:= He kept one in the glove compartment of our car  . \n",
      "Predict1:< The is : the the the  .\n",
      "\n",
      "Input2:> 我 想要 让 他们 知道 我们 将 为 他们 作证 尽 我们 所能 让 他们 的 人生 有所 改变\n",
      "\n",
      "Target2:= I wanted them to know that we will be bearing witness to them , and that we will do whatever we can to help make a difference in their lives  . \n",
      "Predict2:< I want to to to to to to to to to to to to them to them  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 29s (- -85m 33s), Epoch: [6/10], Step: [1760/3335], Train Loss: 3.282460355758667, BLEU: 0.7354765102828925\n",
      "\n",
      "Input1:> 我 以为 我 这辈子 辈子 完 了 尽管 尽管如此 如此 我 还是 努力 隐藏 了 自己 的 恐惧 和 担忧 并 回答 了 他们 的 问题\n",
      "\n",
      "Target1:= I thought my life was over , but I managed to control all the emotions inside me and answer the questions  . \n",
      "Predict1:< I I I I I I , I I , , I , , , , , I , , I I I I\n",
      "\n",
      "Input2:> 我 想 这 就是 内罗毕 公园 的 狮子 UNK 子会 那么 少 的 原因\n",
      "\n",
      "Target2:= And I think this is why the Nairobi National Park lions are few  . \n",
      "Predict2:< I I think that is is , is , the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 34s (- -85m 27s), Epoch: [6/10], Step: [1780/3335], Train Loss: 3.369970142841339, BLEU: 0.7153943582879775\n",
      "\n",
      "Input1:> 他们 知道 这些 影像 会 被 外面 的 人 看见\n",
      "\n",
      "Target1:= They knew their image would be seen by you out in the world  . \n",
      "Predict1:< They know know the the the the the  .  .  .\n",
      "\n",
      "Input2:> 我 在 一个 UNK 遇见 了 康 纳 那时 是 一月 一月份 月份 天 很 冷\n",
      "\n",
      "Target2:= I met UNK on a cold , rainy January night  . \n",
      "Predict2:< I was a a a a , , , , , , , a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 40s (- -85m 22s), Epoch: [6/10], Step: [1800/3335], Train Loss: 3.4213993668556215, BLEU: 0.7383711396317366\n",
      "\n",
      "Input1:> 种 一棵 一棵树 能够 收到 一千 甚至 一万 万颗 种子\n",
      "\n",
      "Target1:= Growing one plant will give you 1,000 , 10,000 seeds  . \n",
      "Predict1:< And is the the the the the  .  .\n",
      "\n",
      "Input2:> 记得 得当 当时 站 在 几乎 是 一片 黑暗 当中 我 刹那 刹那间 那间 感到 一阵 强烈 的 恐惧 那一刻 一刻 我 在 想 被困 在 这个 地狱 里 是 多么 的 恐怖\n",
      "\n",
      "Target2:= Standing in the near darkness , I remember feeling this quick , hot fear , and in that instant , I could only imagine what it must be like to be trapped in that hell  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 45s (- -85m 16s), Epoch: [6/10], Step: [1820/3335], Train Loss: 3.457650887966156, BLEU: 0.74878013071915\n",
      "\n",
      "Input1:> 花园 非常 的 美丽\n",
      "\n",
      "Target1:= And the garden , it was beautiful  . \n",
      "Predict1:< It is very very  .  .\n",
      "\n",
      "Input2:> 然后 我 想 酷 就 这样 吧\n",
      "\n",
      "Target2:= And I was like , &quot; Cool  . Bring it  . &quot; \n",
      "Predict2:< And then I to to , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 51s (- -85m 11s), Epoch: [6/10], Step: [1840/3335], Train Loss: 3.494132125377655, BLEU: 0.7625688199959227\n",
      "\n",
      "Input1:> 所以 我们 带 着意 意大利 大利 的 种子 来到 了 赞比亚 比亚 南部 一个 壮丽 迷人 的 山谷 通向 UNK 比西 西河 我们 教 当地 当地人 人种 种植 意大利 大利 利西 西红柿 和 UNK 葫芦 等等\n",
      "\n",
      "Target1:= So we arrived there with Italian seeds in southern Zambia in this absolutely magnificent valley going down to the UNK River , and we taught the local people how to grow Italian tomatoes and UNK and  . . . \n",
      "Predict1:< So we we we a , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 她 的 两个 个儿 儿子 虽然 已经 不小 了 还 在 在 两个 不同 的 学校 上学\n",
      "\n",
      "Target2:= Her two little boys , not so little anymore , but her two boys were both at school , separate schools  . \n",
      "Predict2:< She was in in in in in in in in in in in in  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84m 57s (- -85m 5s), Epoch: [6/10], Step: [1860/3335], Train Loss: 3.3774458289146425, BLEU: 0.7158811166461249\n",
      "\n",
      "Input1:> 应该 UNK 一个 共和 共和国 的 领袖 为什么 什么\n",
      "\n",
      "Target1:= What do you call the leader of a UNK country  ? \n",
      "Predict1:< What is a a  ?  ?\n",
      "\n",
      "Input2:> 说 你 长大 后 想 当 模特 就 像 你 说 你 长大 后 想 赢 彩票 一样\n",
      "\n",
      "Target2:= Saying that you want to be a model when you grow up is akin to saying that you want to win the Powerball when you grow up  . \n",
      "Predict2:< And you said , , you , , you , , , , , , , , you , , , you ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 2s (- -85m 0s), Epoch: [6/10], Step: [1880/3335], Train Loss: 3.397180104255676, BLEU: 0.7800993306873372\n",
      "\n",
      "Input1:> 每个 个人 对此 的 反应 和 行动 也 如出一辙 一辙\n",
      "\n",
      "Target1:= And everyone was doing the same  . \n",
      "Predict1:< And is is , and and  .  .\n",
      "\n",
      "Input2:> 我 的 意思 是 一些 人 想 叫 他 为 首席 行政 行政官 华盛 华盛顿 而 另 一些 人称 他 为 乔治 华盛 华盛顿 殿下 还有 有人 叫 他 美利 美利坚 联合 联合政府 政府 自由 解放 的 保卫\n",
      "\n",
      "Target2:= I mean , some people wanted him to be called Chief UNK Washington , and other people , His Highness George Washington , and other people , UNK of the UNK of the People of the United States \n",
      "Predict2:< I I , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 8s (- -86m 54s), Epoch: [6/10], Step: [1900/3335], Train Loss: 3.451694738864899, BLEU: 0.7293020444800461\n",
      "\n",
      "Input1:> 2011 年 3 月 11 日 正如 全球 其他 其他人 他人 一样 我 在家 家里 看到 日本 上演 的 悲剧\n",
      "\n",
      "Target1:= On March 11 , 2011 , I watched from home , as the rest of the world did , as the tragic events unfolded in Japan  . \n",
      "Predict1:< And the , , , , , , , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 这 两个 词 来自 于 拉丁 词根 UNK 意 即 父亲\n",
      "\n",
      "Target2:= The two words come from the Latin root &quot; UNK , &quot; which means &quot; father  . &quot; \n",
      "Predict2:< And is is a , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 13s (- -86m 49s), Epoch: [6/10], Step: [1920/3335], Train Loss: 3.3309595584869385, BLEU: 0.7510496041917998\n",
      "\n",
      "Input1:> 你 能 做 什么 你 能 生产 产出 出来 吗\n",
      "\n",
      "Target1:= What can you do  ? Can you make it  ? \n",
      "Predict1:< Can you you do  ?  ?  ?  ?  ?\n",
      "\n",
      "Input2:> 谢谢\n",
      "\n",
      "Target2:= Thank you  . \n",
      "Predict2:< Thank you  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 19s (- -86m 43s), Epoch: [6/10], Step: [1940/3335], Train Loss: 3.3633249163627625, BLEU: 0.7343328477952715\n",
      "\n",
      "Input1:> UNK 沉浸 在 他 自己 的 世界 里 应用 他 自己 制定 的 法则 他 从 细微 的 事物 中 寻找 找乐 乐趣 比如 绕 着 屋子 排列 他 的 玩具 玩具车 盯 着 洗衣 洗衣机 把 他\n",
      "\n",
      "Target1:= Remi lived and reigned in his own world , with his own rules , and he found pleasure in the smallest things , like lining up cars around the room and staring at the washing machine and eating \n",
      "Predict1:< And his his his his his his his his his his his his his his his his his his his his his his his his his his , , his his his , , , his his his\n",
      "\n",
      "Input2:> 彼得 德鲁 德鲁克 在 涉足 商界 之前 是 一位 哲学 教授 彼得 德鲁 德鲁克 曾 说 过 计划 其实 实是 与 创业 型 社会 及 创业 型 经济 不相容 相容 的\n",
      "\n",
      "Target2:= Peter UNK was a professor of philosophy before becoming involved in business , and this is what Peter UNK says : &quot; Planning is actually incompatible with an entrepreneurial society and economy  . &quot; \n",
      "Predict2:< The : : , , , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 24s (- -86m 37s), Epoch: [6/10], Step: [1960/3335], Train Loss: 3.4871838808059694, BLEU: 0.7078390090151263\n",
      "\n",
      "Input1:> 今天 的 奴隶 奴隶制 奴隶制度 制度 关乎 商业 商业利益 利益 奴隶 奴隶制 制造 的 商品 具有 价值 但是 创造 造价 价值 的 人 却是 被 随意 处置 的\n",
      "\n",
      "Target1:= Today is slavery is about commerce , so the goods that enslaved people produce have value , but the people producing them are disposable  . \n",
      "Predict1:< The , , , , of , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 但是 当 我 回到 阿富汗 看见 我 的 学生 以及 支持 她们 并 鼓励 她们 的 父母 时 我 看到 了 未来 的 希望 也 看到 这种 变化 会 延续 延续下去 下去\n",
      "\n",
      "Target2:= But when I am back in Afghanistan , when I see the students in my school and their parents who advocate for them , who encourage them , I see a promising future and lasting change  . \n",
      "Predict2:< But when I I I , , , , , , and and and and and , and , and and , and , and and , , and and and and and and and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 30s (- -86m 32s), Epoch: [6/10], Step: [1980/3335], Train Loss: 3.5003145217895506, BLEU: 0.6759653273687243\n",
      "\n",
      "Input1:> 我 并不知道 不知 知道 他 是 在 虐待 我\n",
      "\n",
      "Target1:= I didn t know he was abusing me  . \n",
      "Predict1:< I didn t know I I  .  .  .  .\n",
      "\n",
      "Input2:> 你 知道 的 这样 大概 会 可行\n",
      "\n",
      "Target2:= And , you know , it could have worked  . \n",
      "Predict2:< You know you it it  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 35s (- -86m 26s), Epoch: [6/10], Step: [2000/3335], Train Loss: 3.4164061069488527, BLEU: 0.7637874739862615\n",
      "\n",
      "Input1:> 我 的 邻居 邻居们 都 听说 了 这个 办法\n",
      "\n",
      "Target1:= And my neighboring homes heard about this idea  . \n",
      "Predict1:< My I my , ,  .  .  .  .\n",
      "\n",
      "Input2:> 下 一个 问题 是 你 能 得到 免费 的 东西 吗\n",
      "\n",
      "Target2:= Okay , so the next question people always ask me is , &quot; Do you get free stuff  ? &quot; \n",
      "Predict2:< So , question is is you you you you  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 41s (- -86m 21s), Epoch: [6/10], Step: [2020/3335], Train Loss: 3.308202874660492, BLEU: 0.7517082300990234\n",
      "\n",
      "Input1:> 我 并 不是 在 帮助 你 他 回答\n",
      "\n",
      "Target1:= &quot; I am not helping you , &quot; he said  . \n",
      "Predict1:< I don t not not you  .  .  .\n",
      "\n",
      "Input2:> 最 坏 的 消息 是 我 要 把 这件 毛衣 从头 头上 UNK 套进 套进去 进去 因为 这时 这时候 时候 你们 会 笑 我 所以 当 我 穿衣 穿衣服 衣服 的 时候 你们 什么 都 不要 做\n",
      "\n",
      "Target2:= The worst part is putting this sweater over my head , because that is when you ll all laugh at me , so don t do anything while it is over my head  . \n",
      "Predict2:< And , , , , , , , , , , , , , , , , , , , , , , , , , , you , , you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 46s (- -86m 15s), Epoch: [6/10], Step: [2040/3335], Train Loss: 3.3333568215370177, BLEU: 0.7131038319836049\n",
      "\n",
      "Input1:> 但 我们 还是 会 得到 大量 来自 不同 环境 下 的 采样 数据 而且 重点 是 不同 的 环境 因为 这样 我们 才能 消除 除掉 那些 混淆 的 因素 寻求 这种 疾病 真正 的 标记\n",
      "\n",
      "Target1:= But we ll get a very large sample of data that is collected from all different circumstances , and it is getting it in different circumstances that matter because then we are looking at ironing out the confounding \n",
      "Predict1:< But we we we to the the , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这种 材料 有点 吵 如果 我们 要 修 的 路 紧邻 城市 像 荷兰 的 道路 那样 我们 就 会 希望 噪音 小 一些\n",
      "\n",
      "Target2:= It is a noisy material , and if we produce roads like in the Netherlands , very close to cities , then we would like a silent road  . \n",
      "Predict2:< And is a if if if , if , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 52s (- -86m 10s), Epoch: [6/10], Step: [2060/3335], Train Loss: 3.3697004914283752, BLEU: 0.7344858886411214\n",
      "\n",
      "Input1:> 我们 还 可以 作为 世界 与 生活 在 朝鲜 的 同胞 之间 的 沟通 桥梁 因为 我们 很多 人 都 和 生活 在 朝鲜 的 家人 保持 保持联系 联系 我们 寄送 送回 回去 的 信息 和 钱 都\n",
      "\n",
      "Target1:= We can also act as a bridge between the people inside North Korea and the outside world , because many of us stay in contact with family members still inside , and we send information and money that \n",
      "Predict1:< We we we the the the the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 正是 我 想要 的\n",
      "\n",
      "Target2:= That is the whole idea  . \n",
      "Predict2:< That is is I I  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 85m 57s (- -86m 4s), Epoch: [6/10], Step: [2080/3335], Train Loss: 3.317417311668396, BLEU: 0.7783082884873362\n",
      "\n",
      "Input1:> 这些 事 就 发生 生在 我们 身边 我们 只是 不知 知道 罢了\n",
      "\n",
      "Target1:= It is all around us  . We just don t see it  . \n",
      "Predict1:< And is we , we , we we we we we we we  .  .\n",
      "\n",
      "Input2:> 在 过去 的 28 年 里 我 一直 在 拍摄 本土 文化 的 纪录 纪录片 走访 了 6 大洲 70 多个 国家 而 在 2009 年 我 非常 荣幸 地 能 以 独立 参展 UNK 展商 的 身份\n",
      "\n",
      "Target2:= For the last 28 years , I ve been documenting indigenous cultures in more than 70 countries on six continents , and in 2009 I had the great honor of being the sole UNK at the Vancouver Peace \n",
      "Predict2:< In years years , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 3s (- -87m 58s), Epoch: [6/10], Step: [2100/3335], Train Loss: 3.3152585864067077, BLEU: 0.8032456873169863\n",
      "\n",
      "Input1:> 当时 我 在 一家 意大利 大利 非政府 政府 组织 织工 工作 我们 在 非洲 建立 的 每 一个 项目 都 失败 了\n",
      "\n",
      "Target1:= I worked for an Italian NGO , and every single project that we set up in Africa failed  . \n",
      "Predict1:< I I , , in in in in in in in in in in in in in  .\n",
      "\n",
      "Input2:> 是 充满 创业 热情 的 当地 当地人 你们 为 他们 鼓掌\n",
      "\n",
      "Target2:= Local , passionate people  . That is who you have applauded  . \n",
      "Predict2:< It is a , , , they ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 9s (- -87m 53s), Epoch: [6/10], Step: [2120/3335], Train Loss: 3.288197135925293, BLEU: 0.8257311992279248\n",
      "\n",
      "Input1:> 我们 可以 多次 多次重 多次重复 重复\n",
      "\n",
      "Target1:= Several times we can repeat that  . \n",
      "Predict1:< We can to to the  .  .\n",
      "\n",
      "Input2:> 会 发生 什么 呢\n",
      "\n",
      "Target2:= So what happened  ? \n",
      "Predict2:< What what do  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 14s (- -87m 47s), Epoch: [6/10], Step: [2140/3335], Train Loss: 3.369998347759247, BLEU: 0.7891708868068914\n",
      "\n",
      "Input1:> 我 要说 的 是 家庭 家庭暴力 暴力 的 秘密 而 我 将要 回答 的 问题 这个 问题 每个 个人 都 曾经 问起 为什么 什么 她 家庭 家庭暴力 暴力 受害 受害者 会 留下 留下来 下来\n",
      "\n",
      "Target1:= My topic is the secrets of domestic violence , and the question I am going to tackle is the one question everyone always asks : Why does she stay  ? \n",
      "Predict1:< And I I , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 还有 一次 当 我 在 洛杉矶 洛杉矶市 市中心 中心 的 一个 流浪 流浪汉 之家 家安 安置 了 一个 花园\n",
      "\n",
      "Target2:= There is another time when I put a garden in this homeless shelter in downtown Los Angeles  . \n",
      "Predict2:< And , I I I , , , , , , , a a a a a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 20s (- -87m 42s), Epoch: [6/10], Step: [2160/3335], Train Loss: 3.3832085728645325, BLEU: 0.8592147868036972\n",
      "\n",
      "Input1:> 我 觉得 十分 自豪\n",
      "\n",
      "Target1:= And I was very proud  . \n",
      "Predict1:< I I I  .  .\n",
      "\n",
      "Input2:> 但 可口 可口可乐 可乐 可 没有 资助 这个 实验\n",
      "\n",
      "Target2:= But , Coca-Cola did not sponsor this research  . \n",
      "Predict2:< But , t t t  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 25s (- -87m 36s), Epoch: [6/10], Step: [2180/3335], Train Loss: 3.425387406349182, BLEU: 0.7550663608079239\n",
      "\n",
      "Input1:> 两周 内 我 共 得到 150 位 志愿 志愿者 愿者 的 答复 他们 表示 愿意 加入 我们 的 行列\n",
      "\n",
      "Target1:= Within two weeks , I had 150 people wanting to join in  . \n",
      "Predict1:< And the , , , , , , , , , , , the the  .\n",
      "\n",
      "Input2:> 此外 我 意识 到 韩国 与 朝鲜 之间 有 巨大 的 差异\n",
      "\n",
      "Target2:= Also , I realized there was a wide gap between North and South  . \n",
      "Predict2:< And I I I I I , , the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 31s (- -87m 31s), Epoch: [6/10], Step: [2200/3335], Train Loss: 3.3752126455307008, BLEU: 0.8329720049732325\n",
      "\n",
      "Input1:> 我 想要 人们 拿走 它 但是 同时 我 也 希望 人们 能够 够重 重归 健康 的 身体\n",
      "\n",
      "Target1:= I want them to take it , but at the same time , I want them to take back their health  . &quot; \n",
      "Predict1:< I I to to to to to to to to to to to to to to to to to  .\n",
      "\n",
      "Input2:> 家庭 家庭暴力 暴力 模式 的 第三 第三阶段 三阶 阶段 就是 开始 用 暴力 威胁 受害 受害者 并 观察 她 的 反应\n",
      "\n",
      "Target2:= The next step in the domestic violence pattern is to introduce the threat of violence and see how she reacts  . \n",
      "Predict2:< The is the the the the , , , , , , , to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 36s (- -87m 25s), Epoch: [6/10], Step: [2220/3335], Train Loss: 3.420849311351776, BLEU: 0.735264343170269\n",
      "\n",
      "Input1:> 你们 应该 看看 这些 垃圾 掌声 你们 应该 看看 我们 塞 了 多少 垃圾 给 这些 满怀 信任 的 非洲 非洲人 人民\n",
      "\n",
      "Target1:= You should see the rubbish — -- You should see the rubbish that we have bestowed on unsuspecting African people  . \n",
      "Predict1:< And you you you you you , , , , , , , , , , , , , , the the the\n",
      "\n",
      "Input2:> 我们 时常 拍照\n",
      "\n",
      "Target2:= We take photos constantly  . \n",
      "Predict2:< We are we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 42s (- -87m 20s), Epoch: [6/10], Step: [2240/3335], Train Loss: 3.3476681470870973, BLEU: 0.7404045993982116\n",
      "\n",
      "Input1:> 我们 所 拥有 的 最 珍贵 的 两样 东西 一个 是 时间 还有 一个 是 与 他人 的 联系\n",
      "\n",
      "Target1:= Two of the most valuable things we have are time and our relationships with other people  . \n",
      "Predict1:< We of of a of of of , , , , , , , is  .\n",
      "\n",
      "Input2:> 和 其他 两千 六百 六百五十 百五 百五十 五十 五十万 十万 的 美国 国人 一样 我 生活 在 食物 的 荒漠 洛杉矶 UNK 是 得来 速 快餐 商业 商业模式 模式 顾客 不用 下车 即可 可获 获得 服务 和 飞车\n",
      "\n",
      "Target2:= Just like UNK million other Americans , I live in a food desert , South Central Los Angeles , home of the drive-thru and the UNK  . \n",
      "Predict2:< And the of , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 47s (- -87m 14s), Epoch: [6/10], Step: [2260/3335], Train Loss: 3.3343098998069762, BLEU: 0.7824362645297878\n",
      "\n",
      "Input1:> 我 厌倦 了 这些 现实\n",
      "\n",
      "Target1:= I got tired of seeing this happening  . \n",
      "Predict1:< I ve these  .  .  .\n",
      "\n",
      "Input2:> 他们 捕杀 了 我们 的 家畜\n",
      "\n",
      "Target2:= They kill our livestock  . \n",
      "Predict2:< They are us our  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 53s (- -87m 8s), Epoch: [6/10], Step: [2280/3335], Train Loss: 3.2962451934814454, BLEU: 0.776959542521948\n",
      "\n",
      "Input1:> 笑 掌声 我 所讲 的 就是 让 人们 得到 工作 让 孩子 们 远离 街道 让 他们 了解 自己 种植 植食 食物 的 喜悦 荣誉 和 荣耀 开放 农民 市场\n",
      "\n",
      "Target1:= What I am talking about is putting people to work , and getting kids off the street , and letting them know the joy , the pride and the honor in growing your own food , opening farmer \n",
      "Predict1:< And the , that to to to to to to to to to to to to to their to to to their to their to their\n",
      "\n",
      "Input2:> 还有 一次 当 我 在 洛杉矶 洛杉矶市 市中心 中心 的 一个 流浪 流浪汉 之家 家安 安置 了 一个 花园\n",
      "\n",
      "Target2:= There is another time when I put a garden in this homeless shelter in downtown Los Angeles  . \n",
      "Predict2:< And I I I I a a a a a a a a a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 86m 58s (- -87m 3s), Epoch: [6/10], Step: [2300/3335], Train Loss: 3.4219049215316772, BLEU: 0.7649148152423537\n",
      "\n",
      "Input1:> 由此 开始 了 我 深入 现代 奴隶 奴隶制 奴隶制度 制度 的 旅程\n",
      "\n",
      "Target1:= Thus began my journey into modern day slavery  . \n",
      "Predict1:< And then I started , I my the  .  .  .\n",
      "\n",
      "Input2:> 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "\n",
      "Target2:= It was the only way we both could be educated  . \n",
      "Predict2:< It is is we we we we we we  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 4s (- -88m 57s), Epoch: [6/10], Step: [2320/3335], Train Loss: 3.305134189128876, BLEU: 0.75862976752442\n",
      "\n",
      "Input1:> 笑 掌声 我 所讲 的 就是 让 人们 得到 工作 让 孩子 们 远离 街道 让 他们 了解 自己 种植 植食 食物 的 喜悦 荣誉 和 荣耀 开放 农民 市场\n",
      "\n",
      "Target1:= What I am talking about is putting people to work , and getting kids off the street , and letting them know the joy , the pride and the honor in growing your own food , opening farmer \n",
      "Predict1:< And the , , , to to to to to to to to to to to their to their to their to their their their to\n",
      "\n",
      "Input2:> 不能 阻止 疾病 的 传播\n",
      "\n",
      "Target2:= MPs do not help to prevent the spread of diseases  . \n",
      "Predict2:< It can t to  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 9s (- -88m 52s), Epoch: [6/10], Step: [2340/3335], Train Loss: 3.3322100162506105, BLEU: 0.7597742293333652\n",
      "\n",
      "Input1:> 我们 怎么 怎么样 能够 从 邻里 里间 互借 借到 更多 的 东西 同时 避免 在 不合 不合时宜 时宜 的 时候 敲开 对方 的 门\n",
      "\n",
      "Target1:= How can we lend and borrow more things without knocking on each other is doors at a bad time  ? \n",
      "Predict1:< We we we to to the the the the the the the the the the the the the the the the\n",
      "\n",
      "Input2:> 然而 与 创业 创业者 业者 打交道 交道 有 个 秘诀\n",
      "\n",
      "Target2:= However , there is a secret to work with entrepreneurs  . \n",
      "Predict2:< But there , , , a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 15s (- -88m 46s), Epoch: [6/10], Step: [2360/3335], Train Loss: 3.359457325935364, BLEU: 0.7788190085764625\n",
      "\n",
      "Input1:> 每个 个人 对此 的 反应 和 行动 也 如出一辙 一辙\n",
      "\n",
      "Target1:= And everyone was doing the same  . \n",
      "Predict1:< And is is , and and  .  .\n",
      "\n",
      "Input2:> 但 可口 可口可乐 可乐 可 没有 资助 这个 实验\n",
      "\n",
      "Target2:= But , Coca-Cola did not sponsor this research  . \n",
      "Predict2:< But , t t t  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 21s (- -88m 41s), Epoch: [6/10], Step: [2380/3335], Train Loss: 3.228517436981201, BLEU: 0.7978390941968824\n",
      "\n",
      "Input1:> 我 养 了 一只 黑色 色拉 拉布 拉布拉多 布拉 拉多 猎犬 开本 UNK 奥 赛德 面包 面包车 包车\n",
      "\n",
      "Target1:= I have that black lab , and I have that UNK  . \n",
      "Predict1:< I I a a a a , , , , , , , a a  .\n",
      "\n",
      "Input2:> 一旦 价格 价格低 价格低廉 低廉 它 也 就 能 被 大规 大规模 规模 地 扩展\n",
      "\n",
      "Target2:= When something becomes ultra-low cost , it becomes massively scalable  . \n",
      "Predict2:< And it , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 26s (- -88m 35s), Epoch: [6/10], Step: [2400/3335], Train Loss: 3.3854960441589355, BLEU: 0.7299090301332534\n",
      "\n",
      "Input1:> 否则 照片 里 她 和 她 先生 的 脸部 就 无法 完全 修复 了 我们 也 因此 得以 把 所有 部分 整合 合到 一张 好 的 相片 里 然后 重新 新制 制作 这 张 照片\n",
      "\n",
      "Target1:= Areas like her husband and her face , which otherwise would have been completely impossible to fix , we could just put them together in one good photo , and remake the whole photo  . \n",
      "Predict1:< And , , , , her , her her and her and and her and and her and and her and her her and her her her her\n",
      "\n",
      "Input2:> 很 感谢 你 UNK 谢谢 掌声\n",
      "\n",
      "Target2:= Thank you so much  . Thank you  . \n",
      "Predict2:< Thank you very much  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 32s (- -88m 30s), Epoch: [6/10], Step: [2420/3335], Train Loss: 3.324708414077759, BLEU: 0.7991766105705443\n",
      "\n",
      "Input1:> 我 想 通过 一个 叫做 回扣 的 作品 来看 看看 在 阿富汗 的 腐败 并且 模拟 一个 警察 警察局 官员 在 喀布尔 布尔 的 大街 街上 建立 一个 假 的 检查 检查站 然后 把 那些 车 停下 停下来\n",
      "\n",
      "Target1:= I want to look into corruption in Afghanistan through a work called &quot; UNK &quot; and impersonate a police officer , set up a fake UNK on the street of Kabul and stop cars , but instead of \n",
      "Predict1:< I I to to to a a a a a a a a a a a a a a a and , , and , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 非常 非常感谢 感谢 肯尼 肯尼亚 尼亚 UNK 亚语 在 我 的 语言 里 它 的 意思 是 非常 非常感谢 感谢\n",
      "\n",
      "Target2:= UNK UNK  . It means in my language , thank you very much  . \n",
      "Predict2:< Thank you very very very , , , , , , very ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 37s (- -88m 24s), Epoch: [6/10], Step: [2440/3335], Train Loss: 3.298094701766968, BLEU: 0.7749132988818493\n",
      "\n",
      "Input1:> 其他 可能 的 结果 包括 长期 的 跟踪 甚至 施虐 者 再婚 之后 仍 会 跟踪 拒绝 绝经 经济 支持 欺骗 家庭 法庭 来 恐吓 受害 受害者 和 她 的 孩子 孩子 们 通常 常会 被 家庭 问题\n",
      "\n",
      "Target1:= Other outcomes include long-term stalking , even after the UNK UNK ; denial of financial resources ; and manipulation of the family court system to terrify the victim and her children , who are regularly forced by family \n",
      "Predict1:< And the of of , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 然而 与 创业 创业者 业者 打交道 交道 有 个 秘诀\n",
      "\n",
      "Target2:= However , there is a secret to work with entrepreneurs  . \n",
      "Predict2:< But there , , , a  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 43s (- -88m 18s), Epoch: [6/10], Step: [2460/3335], Train Loss: 3.4054956436157227, BLEU: 0.6756390306871414\n",
      "\n",
      "Input1:> 我 让 这 五个 渔民 合伙 运作 不是 把 美味 的 金枪 金枪鱼 以 60 美分 一 公斤 的 低价 卖 给 奥尔巴 奥尔巴尼 尔巴 尼 美国 的 罐头 工厂 而是 以 15 美元 一 公斤 的 高价\n",
      "\n",
      "Target1:= And I helped these five fishermen to work together and get this beautiful tuna not to the cannery in Albany for 60 cents a kilo , but we found a way to take the fish for sushi to \n",
      "Predict1:< I I t to to to the the the , , , , , , , , , , , , , , , , , , , the\n",
      "\n",
      "Input2:> 现在 我 所讲 讲述 的 这个 疯狂 爱情 的 故事 是 一个 伪装 伪装成 装成 爱情 的 心理 陷阱 每年 都 会 有 上百 上百万 百万 的 女性 甚至 也 包括 一些 男性 会 掉 进 这个 陷阱\n",
      "\n",
      "Target2:= I am here to tell you the story of crazy love , a psychological trap disguised as love , one that millions of women and even a few men fall into every year  . \n",
      "Predict2:< Now , I I a a a a , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 48s (- -88m 13s), Epoch: [6/10], Step: [2480/3335], Train Loss: 3.3319650530815124, BLEU: 0.7402317926721024\n",
      "\n",
      "Input1:> 坑坑 坑坑洼洼 坑洼 洼 当然 会 变成 成问题 问题 但 我们 有 解决 的 办法\n",
      "\n",
      "Target1:= UNK , of course , that can become a problem , but we have a solution  . \n",
      "Predict1:< And the , , , , , , , , , , , we we  .\n",
      "\n",
      "Input2:> 在 加德 加德满都 我 由 一些 从前 是 性 奴 的 女人 随 护\n",
      "\n",
      "Target2:= In Kathmandu , I was escorted by women who had previously been sex slaves themselves  . \n",
      "Predict2:< In the , , I I I the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 54s (- -88m 7s), Epoch: [6/10], Step: [2500/3335], Train Loss: 3.294480001926422, BLEU: 0.7636257149405085\n",
      "\n",
      "Input1:> 这 很 怪\n",
      "\n",
      "Target1:= That was awkward  . \n",
      "Predict1:< It is is  .  .\n",
      "\n",
      "Input2:> 如同 你们 所见 这条 河 在 某些 地方 非常 窄 窄 到 朝鲜 朝鲜人 能 借此 偷渡 渡到 中国\n",
      "\n",
      "Target2:= As you can see , the river can be very narrow at certain points , allowing North Koreans to secretly cross  . \n",
      "Predict2:< And you you you , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 87m 59s (- -88m 2s), Epoch: [6/10], Step: [2520/3335], Train Loss: 3.337425935268402, BLEU: 0.6892271932155526\n",
      "\n",
      "Input1:> 我们 会 因 芭蕾 芭蕾舞 大师 和 踢踏 踢踏舞 舞蹈 者 而 感到 欣悦 稍后 你们 就 会 知道 了\n",
      "\n",
      "Target1:= We delight at ballet virtuosos and tap dancers you will see later on  . \n",
      "Predict1:< We we we to and and and , and , , , , , and ,\n",
      "\n",
      "Input2:> 就 在 我 刚 开始 习惯 新生 生活 不久 我 接到 了 一个 令人 令人震惊 震惊 的 电话\n",
      "\n",
      "Target2:= Just as I was starting to get used to my new life , I received a shocking phone call  . \n",
      "Predict2:< And I I I I I I a a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 5s (- -89m 56s), Epoch: [6/10], Step: [2540/3335], Train Loss: 3.358245050907135, BLEU: 0.7152319952265942\n",
      "\n",
      "Input1:> 他 查看 每个 个人 的 身份 身份证 并 开始 问问 问题\n",
      "\n",
      "Target1:= He took everyone is I .D . cards , and he started asking them questions  . \n",
      "Predict1:< He he a to , , , , he  .\n",
      "\n",
      "Input2:> 我 的 手 滑 了 一下 立即 想到 数天 前 碰到 的 一名 矿工 他 一手 UNK 就 掉 下 了 无底 深井\n",
      "\n",
      "Target2:= When my hand slips , I suddenly remember a miner I had met days before who had lost his grip and fell countless feet down that shaft  . \n",
      "Predict2:< My I was to , , , , , , , , , , , , he , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 10s (- -89m 51s), Epoch: [6/10], Step: [2560/3335], Train Loss: 3.4440040230751037, BLEU: 0.7192763628041973\n",
      "\n",
      "Input1:> 当 他 唱起 我们 儿时 的 歌谣 唱出 就 连 我 都 忘记 了 的 歌词 他 提醒 了 我 我们 对 精神 知之甚少 甚少 而 未知 的 事物 是 如此 迷人\n",
      "\n",
      "Target1:= When he sings songs from our childhood , attempting words that not even I could remember , he reminds me of one thing : how little we know about the mind , and how wonderful the unknown must \n",
      "Predict1:< When when was , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 告诉 别人 你 今天 听到 的\n",
      "\n",
      "Target2:= Talk about what you heard here  . \n",
      "Predict2:< You : : you you  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 16s (- -89m 45s), Epoch: [6/10], Step: [2580/3335], Train Loss: 3.29481760263443, BLEU: 0.779913993636876\n",
      "\n",
      "Input1:> 然后 我 想 酷 就 这样 吧\n",
      "\n",
      "Target1:= And I was like , &quot; Cool  . Bring it  . &quot; \n",
      "Predict1:< And I I to to to  .\n",
      "\n",
      "Input2:> 现在 我 在 这个 房间 房间内 感到 一种 令人 不适 的 紧张 紧张感 因为 我 不该 穿 这条 裙子\n",
      "\n",
      "Target2:= And I feel like there is an uncomfortable tension in the room right now because I should not have worn this dress  . \n",
      "Predict2:< Now I I I this I I I I I I I I I  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 21s (- -89m 40s), Epoch: [6/10], Step: [2600/3335], Train Loss: 3.2616710305213927, BLEU: 0.7556396721628722\n",
      "\n",
      "Input1:> 不过 做 这些 事儿 你 可能 会 进 监狱 它们 会 被 误解 UNK\n",
      "\n",
      "Target1:= But doing these things , they can get you in jail , they can be misunderstood , misinterpreted  . \n",
      "Predict1:< But you you you you you you you , , , , to  .  .  .\n",
      "\n",
      "Input2:> 我们 在 巴士 上 的 旅途 有 一周 之久 好几 几次 都 差点 被 抓住\n",
      "\n",
      "Target2:= The journey by bus took one week , and we were almost caught several times  . \n",
      "Predict2:< We we the the in , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 27s (- -89m 34s), Epoch: [6/10], Step: [2620/3335], Train Loss: 3.4244514226913454, BLEU: 0.8266747489291654\n",
      "\n",
      "Input1:> 我们 现在 一起 躺 在 地板 上 我们 的 身体 都 太虚 虚弱 准备 等 死\n",
      "\n",
      "Target1:= We are lying on the floor together , and our bodies are so weak we are ready to die  . &quot; \n",
      "Predict1:< We are our our our , , , we , we in  .  .\n",
      "\n",
      "Input2:> 他 出身 阿富汗 边远 边远地区 地区 有着 与 他人 不同 的 见解 他 坚持 让 他 的 女儿 我 的 母亲 去 上学 并 因此 被迫 与 他 的 父亲 断绝 父子 父子关系 子关 关系\n",
      "\n",
      "Target2:= A total maverick from a remote province of Afghanistan , he insisted that his daughter , my mom , go to school , and for that he was UNK by his father  . \n",
      "Predict2:< He he to to to to , and , and , , and , , and , , and , , , and , , , , , , , and to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 32s (- -89m 29s), Epoch: [6/10], Step: [2640/3335], Train Loss: 3.4288423657417297, BLEU: 0.7237457025496281\n",
      "\n",
      "Input1:> 我们 马赛 族 的 人 相信 我们 是 带 着 我们 的 动物 和 家园 从 天堂 而来 然后 后放 放牧 生活 这 就是 我们 为什么 什么 如此 重视 它们\n",
      "\n",
      "Target1:= My community , the Maasai , we believe that we came from heaven with all our animals and all the land for herding them , and that is why we value them so much  . \n",
      "Predict1:< We we we we , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 这个 年龄 的 女性 中 高中 高中毕业 毕业 的 人数 数不到 不到 6 如果 不是 我家 家人 的 坚持 我 也 会 像 绝大 绝大多数 大多 大多数 多数 数人 一样\n",
      "\n",
      "Target2:= Fewer than six percent of women my age have made it beyond high school , and had my family not been so committed to my education , I would be one of them  . \n",
      "Predict2:< I I the the the , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 38s (- -89m 23s), Epoch: [6/10], Step: [2660/3335], Train Loss: 3.479403567314148, BLEU: 0.7696356996455526\n",
      "\n",
      "Input1:> 我们 没有 自由 他们 说道\n",
      "\n",
      "Target1:= &quot; We have no freedom , &quot; they said  . \n",
      "Predict1:< We are t to  .  .  .  .  .\n",
      "\n",
      "Input2:> 我们 每天 要 走 不同 的 路线 这样 才 没有 有人 会 怀疑 我们 要 去 哪里\n",
      "\n",
      "Target2:= Each day , we took a different route so that no one would suspect where we were going  . \n",
      "Predict2:< We we to to to to , , , , , , , to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 44s (- -89m 17s), Epoch: [6/10], Step: [2680/3335], Train Loss: 3.3478458285331727, BLEU: 0.8112356577331927\n",
      "\n",
      "Input1:> 呃 在 过去 的 几个 世纪 里 我们 定义 里 的 美 不仅 不仅仅 仅仅 仅只 包括 健康 年轻 和 身材 匀称 这些 我们 出于 生物 本能 去 赞美 美的 元素 还 包括 UNK 苗条 的 体型 富有\n",
      "\n",
      "Target1:= Well , for the past few centuries we have defined beauty not just as health and youth and symmetry that we are biologically programmed to admire , but also as tall , slender figures , and UNK and \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 当时 非常 震惊\n",
      "\n",
      "Target2:= I was so shocked  . \n",
      "Predict2:< I was very  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 49s (- -89m 12s), Epoch: [6/10], Step: [2700/3335], Train Loss: 3.3513260006904604, BLEU: 0.6998606767360563\n",
      "\n",
      "Input1:> 我们 种 的 西红柿 漂亮 极了 在意 意大利 大利 一个 西红柿 长 这么 大 在 赞比亚 比亚 能 长 这么 大\n",
      "\n",
      "Target1:= We had these magnificent tomatoes  . In Italy , a tomato would grow to this size  . In Zambia , to this size  . \n",
      "Predict1:< We we a a a a , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 你 知道 吗 你 可以 选择 任何 的 职业\n",
      "\n",
      "Target2:= You know  ? You can be anything  . \n",
      "Predict2:< You you you you know , you you  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88m 55s (- -89m 6s), Epoch: [6/10], Step: [2720/3335], Train Loss: 3.435275340080261, BLEU: 0.6925223043159465\n",
      "\n",
      "Input1:> 就是 这样\n",
      "\n",
      "Target1:= Absolutely  . \n",
      "Predict1:< That is is  .  .\n",
      "\n",
      "Input2:> 下 一个 个人 人们 常 问 的 问题 是 他们 会 修 照片 吗\n",
      "\n",
      "Target2:= The next question people always ask me is , &quot; Do they retouch all the photos  ? &quot; \n",
      "Predict2:< The question question question question , , , , ,  ?  ?  ?  ?  ?  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 0s (- -89m 1s), Epoch: [6/10], Step: [2740/3335], Train Loss: 3.1938355922698975, BLEU: 0.7311925111197859\n",
      "\n",
      "Input1:> 你 可以 看到 这个 电灯 电灯泡 灯泡 是 朝外 的 因为 狮子 是从 那儿 过来 的\n",
      "\n",
      "Target1:= As you can see , the bulbs face outside , because that is where the lions come from  . \n",
      "Predict1:< You you see see see , , because is is is is is  .  .\n",
      "\n",
      "Input2:> 今天 我 站 在 这里 是因为 因为 我 是 个 模特\n",
      "\n",
      "Target2:= And I am on this stage because I am a model  . \n",
      "Predict2:< I I am I I I I I here  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 6s (- -90m 55s), Epoch: [6/10], Step: [2760/3335], Train Loss: 3.310626411437988, BLEU: 0.7656855627379962\n",
      "\n",
      "Input1:> 在 日本 呆 了 6 个 月 以后 携手 志愿 志愿者 愿者 共有 1 100 名 志愿 志愿者 愿者 加入 帮助 日本 救灾 其中 几百 几百名 百名 帮 我们 手工 清洗 超过 13 5 万张 照片 其中 的\n",
      "\n",
      "Target1:= After six months in Japan , 1,100 volunteers had passed through All Hands , hundreds of whom had helped us UNK over 135,000 photographs , the large majority \n",
      "Predict1:< In the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 以为 我 这辈子 辈子 完 了 尽管 尽管如此 如此 我 还是 努力 隐藏 了 自己 的 恐惧 和 担忧 并 回答 了 他们 的 问题\n",
      "\n",
      "Target2:= I thought my life was over , but I managed to control all the emotions inside me and answer the questions  . \n",
      "Predict2:< I I I I I I and , I and , I and , I and , I I and I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 11s (- -90m 50s), Epoch: [6/10], Step: [2780/3335], Train Loss: 3.3507811546325685, BLEU: 0.8286070018231163\n",
      "\n",
      "Input1:> 对 一些 修 图 师 这段 经历 为 他们 建立 了 一些 联系 联系 到 一些 更 宏大 的 事情 例如 回馈 社会 施展 他们 的 才能 不仅 不仅仅 仅仅 用于 苗条 的 模特 和 拥有 完美 美的\n",
      "\n",
      "Target1:= For some of them , it is given them a connection to something bigger , giving something back , using their talents on something other than skinny models and perfect skin  . \n",
      "Predict1:< For , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这个 小男孩 男孩 8 岁 大\n",
      "\n",
      "Target2:= This young child is eight years old  . \n",
      "Predict2:< This is a a ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 17s (- -90m 44s), Epoch: [6/10], Step: [2800/3335], Train Loss: 3.2940311670303344, BLEU: 0.7812207673843793\n",
      "\n",
      "Input1:> 当时 我 并不知道 不知 知道 引诱 和 迷惑 受害 受害者 是 家庭 家庭暴力 暴力 关系 开始 的 第一 第一步 一步\n",
      "\n",
      "Target1:= I didn t know that the first stage in any domestic violence relationship is to seduce and charm the victim  . \n",
      "Predict1:< I I I know , , , , , , , and and and the the\n",
      "\n",
      "Input2:> 他 到 家 时 电话 电话响 了 一个 声音 警告 他 如果 他 再 将 女儿 送去 学校 刚才 的 事情 就 会 再次 再次发生 发生\n",
      "\n",
      "Target2:= As he arrived home , the phone rang , a voice warning him that if he sent his daughter back to school , they would try again  . \n",
      "Predict2:< He he a a , , he , , he , , he , , , he , , , , , he , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 22s (- -90m 39s), Epoch: [6/10], Step: [2820/3335], Train Loss: 3.376509666442871, BLEU: 0.7638343899190806\n",
      "\n",
      "Input1:> 如果 孩子 们 种植 甘蓝 孩子 们 就 会 吃 甘蓝\n",
      "\n",
      "Target1:= If kids grow kale , kids eat kale  . \n",
      "Predict1:< If if children children , , , , ,\n",
      "\n",
      "Input2:> 今天 我 仍然 在 打破 着 沉默\n",
      "\n",
      "Target2:= I am still breaking the silence today  . \n",
      "Predict2:< Today I I I to in  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 28s (- -90m 33s), Epoch: [6/10], Step: [2840/3335], Train Loss: 3.2312732577323913, BLEU: 0.7774016287944792\n",
      "\n",
      "Input1:> 一个 一个个 个个 小 隔间 里 性 奴 们 女人 和 小孩 有些 只有 七岁 大 被迫 取悦 客户 好 让 他们 消费 更多 的 食物 和 酒\n",
      "\n",
      "Target1:= Each has small , private rooms , where the slaves , women , along with young girls and boys , some as young as seven years old , are forced to entertain the clients , encouraging them to \n",
      "Predict1:< A is a a , , , , , , , and , and , , and , , , and , , and and and\n",
      "\n",
      "Input2:> 我 一次 都 没有 我们 用 了 32 次\n",
      "\n",
      "Target2:= Never the word &quot; I , &quot; and the word &quot; we &quot; 32 times  . \n",
      "Predict2:< I I t I , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 33s (- -90m 28s), Epoch: [6/10], Step: [2860/3335], Train Loss: 3.2689907431602476, BLEU: 0.7973057937130434\n",
      "\n",
      "Input1:> 世界 上 所有 有成 成功 的 企业 只有 一个 共同 共同点 同点 只有 一个 没有 一个 企业 是 靠 一 人 之 力 创建 的\n",
      "\n",
      "Target1:= There is only one thing that all the successful companies in the world have in common , only one : None were started by one person  . \n",
      "Predict1:< The is a of the the , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 所以 我 总是 生活 在 恐惧 中 我 害怕 我 的 身份 会 被 人 发现 我会 被 遣送 送回 朝鲜 去 接受 悲惨 的 命运\n",
      "\n",
      "Target2:= So I was living in constant fear that my identity could be revealed , and I would be UNK to a horrible fate back in North Korea  . \n",
      "Predict2:< So I I I I I I I I I I I I I I I I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 39s (- -90m 22s), Epoch: [6/10], Step: [2880/3335], Train Loss: 3.359529268741608, BLEU: 0.7343819910329455\n",
      "\n",
      "Input1:> 当然 当地 当地人 对此 表示 毫无 无兴趣 兴趣 所以 我们 付 工资 请 他们 来 劳作 他们 也 是 一会 一会儿 会儿 来 一会 一会儿 会儿 不来 的\n",
      "\n",
      "Target1:= And of course the local people had absolutely no interest in doing that , so we paid them to come and work , and sometimes they would show up  . \n",
      "Predict1:< And , , , , , , , , , , , , , , , , , , , , , to to to\n",
      "\n",
      "Input2:> 一年 之内 我 做 了 27 个 项目 政府 的 人 跑 来 问 我 你 是 怎么 怎么办 办到 的\n",
      "\n",
      "Target2:= In a year , I had 27 projects going on , and the government came to see me to say , &quot; How can you do that  ? \n",
      "Predict2:< And I a , , , , , , , , , , , , , , ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 44s (- -90m 16s), Epoch: [6/10], Step: [2900/3335], Train Loss: 3.5058035373687746, BLEU: 0.7233604754281934\n",
      "\n",
      "Input1:> 虐待 可能 正 发生 生在 你 的 女儿 女儿身 身上 发生 生在 你 的 姐妹 你 最好 的 朋友 身上\n",
      "\n",
      "Target1:= Abuse could be affecting your daughter , your sister , your best friend right now  . \n",
      "Predict1:< And is is you you , you you your your your your your your\n",
      "\n",
      "Input2:> 我 生活 在 洛杉矶 UNK\n",
      "\n",
      "Target2:= I live in South Central  . \n",
      "Predict2:< I I in in in  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 50s (- -90m 11s), Epoch: [6/10], Step: [2920/3335], Train Loss: 3.4018950343132017, BLEU: 0.762906568867582\n",
      "\n",
      "Input1:> 在 洛杉矶 UNK 人们 死于 可 治愈 的 疾病\n",
      "\n",
      "Target1:= People are dying from curable diseases in South Central Los Angeles  . \n",
      "Predict1:< In the the , , the the the\n",
      "\n",
      "Input2:> 我 拥有 哈佛 佛学 UNK 学院 的 英语 学士 学士学位 学位 还 拿到 了 沃顿 商学 商学院 学院 的 市场 市场营销 营销 方面 的 MBA 学位\n",
      "\n",
      "Target2:= I have a UNK in English from Harvard College , an MBA in marketing from Wharton Business School  . \n",
      "Predict2:< I I I the the the the , , , , , , , , , the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 89m 56s (- -90m 5s), Epoch: [6/10], Step: [2940/3335], Train Loss: 3.366445076465607, BLEU: 0.8136142028048917\n",
      "\n",
      "Input1:> 我 真心 心地 感谢 他 并 问道 你 为什么 什么 要 帮助 我\n",
      "\n",
      "Target1:= I thanked him with all my heart , and I asked him , &quot; Why are you helping me  ? &quot; \n",
      "Predict1:< I I to to , , , you you you you you  .\n",
      "\n",
      "Input2:> 你 可以 成为 美国 总统 或是 下一代 一代 互联 互联网 联网 的 发明 发明者 明者 或是 个 忍者 般的 心胸 胸外科 外科 诗人 这个 不错 因为 你 会 是 第一 第一个 一个\n",
      "\n",
      "Target2:= You could be the President of the United States , or the inventor of the next Internet , or a ninja UNK surgeon poet , which would be awesome , because you d be the first one  . \n",
      "Predict2:< You can can a a the , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 1s (- -90m 0s), Epoch: [6/10], Step: [2960/3335], Train Loss: 3.3286344408988953, BLEU: 0.8065572870997177\n",
      "\n",
      "Input1:> 我 也 很 幸运 幸运地 因为 这项 项发明 发明 而 获得 了 奖学 奖学金 这 是 肯尼 肯尼亚 尼亚 最好 的 学校 布鲁克 豪斯 国际 学校 所 颁发 的 我 对此 感到 十分 的 兴奋\n",
      "\n",
      "Target1:= Because of this invention , I was lucky to get a scholarship in one of the best schools in Kenya , UNK International School , and I am really excited about this  . \n",
      "Predict1:< I I was was because , because , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 错 他们 都 是 奴隶\n",
      "\n",
      "Target2:= Wrong  . They were all enslaved  . \n",
      "Predict2:< And they they  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 7s (- -91m 54s), Epoch: [6/10], Step: [2980/3335], Train Loss: 3.331407165527344, BLEU: 0.7058972138899702\n",
      "\n",
      "Input1:> 幸亏 我 带 了 另外 一套\n",
      "\n",
      "Target1:= So luckily I brought an outfit change  . \n",
      "Predict1:< I I I I  .  .  .\n",
      "\n",
      "Input2:> 这 是 拍照 拍照片 照片 当天\n",
      "\n",
      "Target2:= Here is me on the same day as this shoot  . \n",
      "Predict2:< This is a  .  .  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 12s (- -91m 49s), Epoch: [6/10], Step: [3000/3335], Train Loss: 3.4536547541618345, BLEU: 0.7195919530748293\n",
      "\n",
      "Input1:> 我 把 花园 和 土地 视为 画布 而 植物 和 树木 就是 我 在 画布 上 的 装饰\n",
      "\n",
      "Target1:= I use the garden , the soil , like it is a piece of cloth , and the plants and the trees , that is my UNK for that cloth  . \n",
      "Predict1:< I I the the and and and and and and and and and and and and and  .\n",
      "\n",
      "Input2:> 我 想 读 给 大家 一封 封电子邮件 电子 电子邮件 邮件 作为 结尾 我 收到 其中 一封 来自 辛迪 是 六个 六个月 之后 我 从 日本 回家 的 那天\n",
      "\n",
      "Target2:= I would like to conclude by reading an email I got from one of them , Cindy , the day I finally got back from Japan after six months  . \n",
      "Predict2:< I I to to to to I , I , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 18s (- -91m 43s), Epoch: [6/10], Step: [3020/3335], Train Loss: 3.3996463775634767, BLEU: 0.7846984480962543\n",
      "\n",
      "Input1:> CA 会 是 一场 UNK Richard UNK 你 与众不同 不同\n",
      "\n",
      "Target1:= In the trenches  . Richard UNK , you are something else  . \n",
      "Predict1:< It is a a , you  .  .  .\n",
      "\n",
      "Input2:> 他 只是 压力 太大 了 婚礼 的 筹备 和 我家 家庭 家庭成员 成员 的 到来 让 他 喘 不过 过气\n",
      "\n",
      "Target2:= He had just been really stressed out by the wedding and by becoming a family with me  . \n",
      "Predict2:< He he a his , and and and and and and and and and and and and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 23s (- -91m 38s), Epoch: [6/10], Step: [3040/3335], Train Loss: 3.3445743799209593, BLEU: 0.7796345470045613\n",
      "\n",
      "Input1:> 我 确实 实有 太多 从没 机会 穿 的 8 英寸 高跟鞋 但是 免费 的 东西 却是 我 在 现实 现实生活 生活 中 得到 的 而 这 恰恰 是 我们 不 愿意 谈 的\n",
      "\n",
      "Target1:= I do have too many UNK heels which I never get to wear , except for earlier , but the free stuff that I get is the free stuff that I get in real life , and that \n",
      "Predict1:< I I t to to , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 不知 知道 那 张 是 怎么 怎么回事 回事 回事儿 事儿\n",
      "\n",
      "Target2:= That was , I don t know what happened there  . \n",
      "Predict2:< I don t know know what , know  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 29s (- -91m 32s), Epoch: [6/10], Step: [3060/3335], Train Loss: 3.4713653922080994, BLEU: 0.8579884731770496\n",
      "\n",
      "Input1:> 那些 大石 UNK 石板 比 搬运 石板 的 孩子 还要 重 孩子 们 用 树枝 绳子 破烂 的 衣服 制成 背带 把 石板 绑 在 自己 己身 身上\n",
      "\n",
      "Target1:= The big sheets of slate were heavier than the children carrying them , and the kids hoisted them from their heads using these handmade harnesses of sticks and rope and torn cloth  . \n",
      "Predict1:< And the the the the the the , , , , , , , , , , , , , to to to\n",
      "\n",
      "Input2:> 还有 一次 当 我 在 洛杉矶 洛杉矶市 市中心 中心 的 一个 流浪 流浪汉 之家 家安 安置 了 一个 花园\n",
      "\n",
      "Target2:= There is another time when I put a garden in this homeless shelter in downtown Los Angeles  . \n",
      "Predict2:< And , I I I I , , a a a a a a a  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 34s (- -91m 26s), Epoch: [6/10], Step: [3080/3335], Train Loss: 3.3640848875045775, BLEU: 0.7960240375256522\n",
      "\n",
      "Input1:> 我 在 移民 UNK 和 警察 警察局 来来 来来回回 来回 回回 折腾 了 好多 多次 绝望 地 努力 着 希望 能 把 他们 救出 出来 但是 我 已 没有 足够 的 钱 来 贿赂 或是 交 罚金 了\n",
      "\n",
      "Target1:= I went back and forth between the immigration office and the police station , desperately trying to get my family out , but I didn t have enough money to pay a bribe or fine anymore  . \n",
      "Predict1:< I I to to to , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 卓越 进步 和 改变 的 可能 可能性 在 我们 想 与 他人 变得 一样 的 努力 中 消亡\n",
      "\n",
      "Target2:= The chance for greatness , for progress and for change dies the moment we try to be like someone else  . \n",
      "Predict2:< The the the of , , , , , , , , the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 40s (- -91m 21s), Epoch: [6/10], Step: [3100/3335], Train Loss: 3.373091387748718, BLEU: 0.8082534308380422\n",
      "\n",
      "Input1:> 一年 之内 我 做 了 27 个 项目 政府 的 人 跑 来 问 我 你 是 怎么 怎么办 办到 的\n",
      "\n",
      "Target1:= In a year , I had 27 projects going on , and the government came to see me to say , &quot; How can you do that  ? \n",
      "Predict1:< And I a a , , , , , , , , , , , , , to to\n",
      "\n",
      "Input2:> 但是 我 那 受过 教育 的 母亲 成为 为了 一名 名教 教师\n",
      "\n",
      "Target2:= But my educated mother became a teacher  . \n",
      "Predict2:< But I was a to to of  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 45s (- -91m 15s), Epoch: [6/10], Step: [3120/3335], Train Loss: 3.1844985365867613, BLEU: 0.7816839164748979\n",
      "\n",
      "Input1:> 是 的 他们 会 修 几乎 所有 照片 但 这 只是 工作 的 一小 一小部分 小部 部分\n",
      "\n",
      "Target1:= And yeah , they pretty much retouch all the photos , but that is only a small component of what is happening  . \n",
      "Predict1:< It is , , they , , , , they , , they ,  .  .\n",
      "\n",
      "Input2:> 他 被 丢到 了 伦敦 塔里 遭到 囚禁 但是 他 还是 很 勇敢 拥有 足够 的 勇气 在 议会 会上 提议 并且 到 最后 他 在 伦敦 拥有 众多 的 支持 声音 而 赢得 了 胜利\n",
      "\n",
      "Target2:= And he was thrown into the Tower of London and imprisoned , but he was brave enough , he was brave enough to take them on , and in the end he had such popular support in London \n",
      "Predict2:< He he a a , , , , , , he , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 51s (- -91m 10s), Epoch: [6/10], Step: [3140/3335], Train Loss: 3.3411162853240968, BLEU: 0.7916427416616467\n",
      "\n",
      "Input1:> 这 是 我 为何 要 维护 它 的 原因\n",
      "\n",
      "Target1:= And this is how I decided to maintain it  . \n",
      "Predict1:< This is is I I I I  .  .\n",
      "\n",
      "Input2:> 对 我 来说 这 是 人们 问 过 的 最 让 我 伤心 和 痛苦 的 问题 你 不能 理解 但是 只有 我们 受害 受害者 自己 清楚 离开 施虐 者 是 多么 的 危险\n",
      "\n",
      "Target2:= To me , this is the saddest and most painful question that people ask , because we victims know something you usually don t : It is incredibly dangerous to leave an UNK  . \n",
      "Predict2:< And , me , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 90m 56s (- -91m 4s), Epoch: [6/10], Step: [3160/3335], Train Loss: 3.2788153886795044, BLEU: 0.7777689388342279\n",
      "\n",
      "Input1:> 这 是 那 张 照片 几个 月前 我 跟 祖母 一起 起拍 的 照片\n",
      "\n",
      "Target1:= This is what I looked like with my grandma just a few months earlier  . \n",
      "Predict1:< This is is my my I I I I my  .  .  .\n",
      "\n",
      "Input2:> 而 这 也 是 为什么 什么 它 在 街上 的 原因\n",
      "\n",
      "Target2:= That is why it is on the street  . \n",
      "Predict2:< And it is is it it is  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 2s (- -92m 59s), Epoch: [6/10], Step: [3180/3335], Train Loss: 3.281387221813202, BLEU: 0.8372595941529505\n",
      "\n",
      "Input1:> 那个 善良 的 陌生 陌生人 生人 象征 着 我 以及 朝鲜 朝鲜人 最 需要 的 希望 他 使 我 明白 陌生 陌生人 生人 的 善意 和 国际 社会 的 支持 真是 我们 朝鲜 朝鲜人 需要 的 希望 之\n",
      "\n",
      "Target1:= The kind stranger symbolized new hope for me and the North Korean people when we needed it most , and he showed me the kindness of strangers and the support of the international community are truly the rays \n",
      "Predict1:< And is the , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 他 无所 无所谓 所谓 宗教 宗教信仰 信仰 的 差异 还有 他 从来 从来没 说 过 谎\n",
      "\n",
      "Target2:= He doesn t care about religious differences , and get this : He has never told a lie  . \n",
      "Predict2:< He he , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 8s (- -92m 53s), Epoch: [6/10], Step: [3200/3335], Train Loss: 3.4947312593460085, BLEU: 0.8360679319622117\n",
      "\n",
      "Input1:> 你 可以 向 客户 提出 你 的 点子\n",
      "\n",
      "Target1:= You can give somebody an idea  . \n",
      "Predict1:< You can can to your your  .  .  .\n",
      "\n",
      "Input2:> 他 经常 尖叫\n",
      "\n",
      "Target2:= He screamed a lot  . \n",
      "Predict2:< He he he  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 13s (- -92m 48s), Epoch: [6/10], Step: [3220/3335], Train Loss: 3.4098549246788026, BLEU: 0.770296029925051\n",
      "\n",
      "Input1:> 但 不幸 的 是 我 得 告诉 你们 在 2007 年时 一名 纽约 纽约大学 大学 的 博士 在 启发 之下 统计 了 所有 在职 模特 的 人数 在 UNK 名 正式 受雇 模特 中 只有 27 名 也\n",
      "\n",
      "Target1:= But unfortunately I have to inform you that in 2007 , a very inspired NYU Ph .D . student counted all the models on the runway , every single one that was hired , and of the UNK models that \n",
      "Predict1:< But I , , you you you , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 如果 这些 人 要为 自己 争取 取自 自由 奴隶 奴隶主 就 烧掉 他们 所有 的 房子\n",
      "\n",
      "Target2:= When these villagers claimed their freedom , the slaveholders burned down all of their houses  . \n",
      "Predict2:< If people people people , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 19s (- -92m 42s), Epoch: [6/10], Step: [3240/3335], Train Loss: 3.3514888405799867, BLEU: 0.7695115177107889\n",
      "\n",
      "Input1:> 第一 第一个 一个 问题 是 你 是 怎么 成为 模特 的\n",
      "\n",
      "Target1:= So the first question is , how do you become a model  ? \n",
      "Predict1:< The first first first is is you you  ?\n",
      "\n",
      "Input2:> 内罗毕 国家 公园 的 南部 并 没有 全部 围起 栅栏 这 就 意味 意味着 像 斑马 这样 的 野生 野生动物 生动 动物 可以 自由 地 在 公园 园外 UNK 移动\n",
      "\n",
      "Target2:= Nairobi National Park is not UNK in the south widely , which means wild animals like zebras migrate out of the park freely  . \n",
      "Predict2:< The is is is the , , , , , , , , , , , , , , , , the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 24s (- -92m 37s), Epoch: [6/10], Step: [3260/3335], Train Loss: 3.452976846694946, BLEU: 0.8290008157177385\n",
      "\n",
      "Input1:> 你 有 能力 制止 家庭 家庭暴力 暴力 只 需要 要点 点亮 星星 星星之火\n",
      "\n",
      "Target1:= You have the power to end domestic violence simply by shining a spotlight on it  . \n",
      "Predict1:< You have to to to , , , , , , ,  .  .\n",
      "\n",
      "Input2:> 在 50 UNK 摄氏 摄氏度 的 高温 中 男人 女人 小孩 事实 事实上 是 一整 整个 家庭 在 厚厚 厚厚的 灰尘 笼罩 下 机械 地 把 砖块 往 自己 的 头顶 顶上 上垒 每次 叠 上 18 块\n",
      "\n",
      "Target2:= UNK in temperatures of 130 degrees , men , women , children , entire families in fact , were cloaked in a heavy blanket of dust , while mechanically stacking bricks on their head , up to 18 \n",
      "Predict2:< In the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 30s (- -92m 31s), Epoch: [6/10], Step: [3280/3335], Train Loss: 3.3275214076042174, BLEU: 0.8116604679807468\n",
      "\n",
      "Input1:> 而是 我 要 被 停 几次 我 什么 时候 会 被 停\n",
      "\n",
      "Target1:= but &quot; How many times will I get stopped  ? When will I get stopped  ? &quot; \n",
      "Predict1:< I I t to to I I I I I I I I\n",
      "\n",
      "Input2:> 但 最难 的 是 去 展现 性别 和 种族 种族歧视 歧视 的 职业 传统 因为 我 自己 本身 是 最大 的 受益 受益者\n",
      "\n",
      "Target2:= But mostly it was difficult to unpack a legacy of gender and racial oppression when I am one of the biggest beneficiaries  . \n",
      "Predict2:< But the is the the the , , , , , , , , , , , the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 35s (- -92m 26s), Epoch: [6/10], Step: [3300/3335], Train Loss: 3.399491250514984, BLEU: 0.7770525732777366\n",
      "\n",
      "Input1:> 但是 这 一次 我 要 去 挖掘 那些 见 不得 光 的 秘密\n",
      "\n",
      "Target1:= But this time , I would see the skeletons hidden in the closet  . \n",
      "Predict1:< But it is I to to to to to to to to  .\n",
      "\n",
      "Input2:> 但是 总 需要 有人 维护 它\n",
      "\n",
      "Target2:= But you have to maintain it  . \n",
      "Predict2:< But it is it to  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 41s (- -92m 20s), Epoch: [6/10], Step: [3320/3335], Train Loss: 3.366211438179016, BLEU: 0.7046505769135835\n",
      "\n",
      "Input1:> 而且 这 也 是 为什么 什么 参议 参议院 议院 反对 这个 个头 头衔\n",
      "\n",
      "Target1:= And that is why the Senate objected to it  . \n",
      "Predict1:< And this is is why the , , , , the  .  .\n",
      "\n",
      "Input2:> 我们 开始 交往 他 喜欢 关于 我 的 一切 例如 我 的 聪明 我 在 哈佛 的 求学 经历 我 给予 青少 青少年 少年 女性 的 热情 UNK 帮助 以及 我 的 工作\n",
      "\n",
      "Target2:= We started dating , and he loved everything about me , that I was smart , that I d gone to Harvard , that I was passionate about helping teenage girls , and my job  . \n",
      "Predict2:< We started to to my , my , , my , , , my , , , , , , my , , , my , , my , my\n",
      "[3.5050572633743284, 3.439118194580078, 3.4654672145843506, 3.4366726756095884, 3.34930135011673, 3.4049430966377257, 3.42221599817276, 3.454615521430969, 3.393016278743744, 3.3423635840415953, 3.3174962639808654, 3.362126898765564, 3.309951114654541, 3.4142354249954225, 3.381081211566925, 3.3412585616111756, 3.4394127368927, 3.3626280665397643, 3.3824695110321046, 3.46565865278244, 3.343266046047211, 3.328353476524353, 3.3547001242637635, 3.404417073726654, 3.3802308320999144, 3.4340970039367678, 3.381242573261261, 3.3610164523124695, 3.377891206741333, 3.3709843516349793, 3.4258567333221435, 3.3879064559936523, 3.3480257272720335, 3.4788205146789553, 3.319532346725464, 3.358142340183258, 3.4067959070205687, 3.3951137542724608, 3.3349820733070374, 3.3419443130493165, 3.448560285568237, 3.349369192123413, 3.512873983383179, 3.3630005717277527, 3.419933593273163, 3.5758996844291686, 3.4319066286087034, 3.4308770537376403, 3.326988196372986, 3.3763888001441957, 3.279190647602081, 3.3351287245750427, 3.4470112442970278, 3.4590395092964172, 3.362979781627655, 3.3790847301483153, 3.3409808039665223, 3.3738571286201475, 3.365334463119507, 3.337033474445343, 3.3627277612686157, 3.266325032711029, 3.5840203285217287, 3.35230153799057, 3.4756779789924623, 3.320980632305145, 3.470785665512085, 3.4675057768821715, 3.343624937534332, 3.330204939842224, 3.301334047317505, 3.360919439792633, 3.3981236934661867, 3.3309827208518983, 3.401806342601776, 3.3854074716567992, 3.3883564352989195, 3.2258586049079896, 3.389696145057678, 3.354289948940277, 3.4580812573432924, 3.2957476139068604, 3.261260139942169, 3.2261544704437255, 3.2738405227661134, 3.3522937059402467, 3.342925012111664, 3.282460355758667, 3.369970142841339, 3.4213993668556215, 3.457650887966156, 3.494132125377655, 3.3774458289146425, 3.397180104255676, 3.451694738864899, 3.3309595584869385, 3.3633249163627625, 3.4871838808059694, 3.5003145217895506, 3.4164061069488527, 3.308202874660492, 3.3333568215370177, 3.3697004914283752, 3.317417311668396, 3.3152585864067077, 3.288197135925293, 3.369998347759247, 3.3832085728645325, 3.425387406349182, 3.3752126455307008, 3.420849311351776, 3.3476681470870973, 3.3343098998069762, 3.2962451934814454, 3.4219049215316772, 3.305134189128876, 3.3322100162506105, 3.359457325935364, 3.228517436981201, 3.3854960441589355, 3.324708414077759, 3.298094701766968, 3.4054956436157227, 3.3319650530815124, 3.294480001926422, 3.337425935268402, 3.358245050907135, 3.4440040230751037, 3.29481760263443, 3.2616710305213927, 3.4244514226913454, 3.4288423657417297, 3.479403567314148, 3.3478458285331727, 3.3513260006904604, 3.435275340080261, 3.1938355922698975, 3.310626411437988, 3.3507811546325685, 3.2940311670303344, 3.376509666442871, 3.2312732577323913, 3.2689907431602476, 3.359529268741608, 3.5058035373687746, 3.4018950343132017, 3.366445076465607, 3.3286344408988953, 3.331407165527344, 3.4536547541618345, 3.3996463775634767, 3.3445743799209593, 3.4713653922080994, 3.3640848875045775, 3.373091387748718, 3.1844985365867613, 3.3411162853240968, 3.2788153886795044, 3.281387221813202, 3.4947312593460085, 3.4098549246788026, 3.3514888405799867, 3.452976846694946, 3.3275214076042174, 3.399491250514984, 3.366211438179016]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 48s (- -88m 46s), Epoch: [7/10], Step: [20/3335], Train Loss: 3.265683615207672, BLEU: 0.807179882570685\n",
      "\n",
      "Input1:> 秃鹫 是 我们 的 大自然 自然 垃圾 清道 清道夫 道夫\n",
      "\n",
      "Target1:= They are our natural garbage collectors  . \n",
      "Predict1:< The is we we we our our\n",
      "\n",
      "Input2:> 我 搬到 了 纽约 开始 了 自己 第一 第一份 一份 工作 在 UNK 杂志 杂志社 当 编辑 和 撰稿 撰稿人\n",
      "\n",
      "Target2:= I had moved to New York City for my first job as a writer and editor at Seventeen magazine  . \n",
      "Predict2:< I started the the and and and and , and , and , and , and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 54s (- -90m 23s), Epoch: [7/10], Step: [40/3335], Train Loss: 3.480336940288544, BLEU: 0.8041358504260075\n",
      "\n",
      "Input1:> 我 突然 明白 我们 创造 的 最重 重要 的 事情 之一 是 我们 的 记忆\n",
      "\n",
      "Target1:= And it hit me : One of the most important things we all make are memories  . \n",
      "Predict1:< And I , the the the , , , , , , our , our our  .  .\n",
      "\n",
      "Input2:> 回到 我 的 问题 为什么 什么 我 要 留下 留下来 下来 了\n",
      "\n",
      "Target2:= Back to my question : Why did I stay  ? \n",
      "Predict2:< So , I I I I I I I I  ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 59s (- -91m 32s), Epoch: [7/10], Step: [60/3335], Train Loss: 3.3527695417404173, BLEU: 0.8428968947397647\n",
      "\n",
      "Input1:> 举 个例 例子 我 所 住 的 街区 的 肥胖 肥胖率 要 比 相距 我们 10 英里 不到 的 比 佛利 山庄 高出 5 倍 也许 是 8 倍\n",
      "\n",
      "Target1:= For instance , the obesity rate in my neighborhood is five times higher than , say , Beverly Hills , which is probably eight , 10 miles away  . \n",
      "Predict1:< So , example , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 在 剑桥 剑桥市 市长 长大 有 一次 我 走进 一家 家商 商店 却 忘 了 UNK 他们 送 了 我 一条 裙子\n",
      "\n",
      "Target2:= I grew up in Cambridge , and one time I went into a store and I forgot my money and they gave me the dress for free  . \n",
      "Predict2:< I I a a a , , , I , I , I , I , I I I I I I I I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 5s (- -91m 3s), Epoch: [7/10], Step: [80/3335], Train Loss: 3.3558590412139893, BLEU: 0.7726040551760243\n",
      "\n",
      "Input1:> 这些 光会 反射 射进 进入 房间 一些 会 返回 回到 门上 然后 后进 进入 照相 照相机 相机 这样 我们 就 可以 以利 利用 这些 光 的 反射\n",
      "\n",
      "Target1:= It is going to bounce , go inside the room , some of that is going to reflect back on the door , and then back to the camera , and we could exploit these multiple bounces of \n",
      "Predict1:< And these these these , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 并不知道 不知 知道 他 是 在 虐待 我\n",
      "\n",
      "Target2:= I didn t know he was abusing me  . \n",
      "Predict2:< I didn t him him him  .  .  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 10s (- -92m 44s), Epoch: [7/10], Step: [100/3335], Train Loss: 3.3237775444984434, BLEU: 0.7438735860496039\n",
      "\n",
      "Input1:> 首先 我 要 表扬 下 你们 对于 模特 界 的 了解 令人 印象 深刻\n",
      "\n",
      "Target1:= And first , I commend you on your model knowledge  . Very impressive  . \n",
      "Predict1:< First , I I you you you you you you you  .  .\n",
      "\n",
      "Input2:> 事件 发生 的 顺序 在 相机 里 有时 有时候 时候 被 反转 了 所以 通过 对应 时间 和 空间 的 扭曲 我们 可以 修正 这个 扭曲\n",
      "\n",
      "Target2:= The order at which events take place in the world appear in the camera with sometimes reversed order , so by applying the corresponding space and time warp , we can correct for this distortion  . \n",
      "Predict2:< And the , , , , , , , , , , , , , and and and and and and  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 16s (- -92m 29s), Epoch: [7/10], Step: [120/3335], Train Loss: 3.266356647014618, BLEU: 0.7925980571696352\n",
      "\n",
      "Input1:> 而且 仅仅 几年 年后 我们 有 了 第一 第一次 一次 记录 使用 短语 as UNK as UNK UNK 敢作 敢作敢为\n",
      "\n",
      "Target1:= And it was only a few years later that we have the first recorded use of the phrase &quot; as bold as brass  . &quot; \n",
      "Predict1:< And then , , , , , we , we , , we , we , we ,  .  .\n",
      "\n",
      "Input2:> 可以 考虑 用 一些 例子 故事 还有 类比 这些 都 是 使 你们 的 谈论 内容 吸引 我们 让 我们 兴奋 的 方法\n",
      "\n",
      "Target2:= A few things to consider are having examples , stories and analogies  . Those are ways to engage and excite us about your content  . \n",
      "Predict2:< And can can of of of , , , , , , , , , , , , can ,  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 22s (- -92m 17s), Epoch: [7/10], Step: [140/3335], Train Loss: 3.487698495388031, BLEU: 0.8269019209360684\n",
      "\n",
      "Input1:> 如果 我们 能 让 这种 材料 自我 修复 就 有 可能 有 解决 解决方案 方案\n",
      "\n",
      "Target1:= If we can make this material UNK , then probably we have a solution  . \n",
      "Predict1:< If we can can to , , , we we ,\n",
      "\n",
      "Input2:> 而 就 在 几个 月 之后 我 意识 到 他 与众不同 不同\n",
      "\n",
      "Target2:= And after a few months went by , I realized that he was different  . \n",
      "Predict2:< And , , , , , , I ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 27s (- -92m 7s), Epoch: [7/10], Step: [160/3335], Train Loss: 3.3884538292884825, BLEU: 0.7759888564128502\n",
      "\n",
      "Input1:> 纺织 纺织业 也 是 我们 听到 奴役 劳动 时经 经常 联想 想到 的\n",
      "\n",
      "Target1:= The textile industry is another one we often think of when we hear about slave labor  . \n",
      "Predict1:< And , , we we we we we we we  .  .\n",
      "\n",
      "Input2:> 这个 想法 让 我 坐立 难 安 几周 后 我 就 飞去 了 洛杉矶 去 见 Free The UNK 组织 的 负责 负责人 责人 想要 有所 有所作为 所作 作为\n",
      "\n",
      "Target2:= It started burning a hole in my stomach , so within weeks , I flew down to Los Angeles to meet with the director of Free the Slaves and offer them my help  . \n",
      "Predict2:< And is is me to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 33s (- -93m 57s), Epoch: [7/10], Step: [180/3335], Train Loss: 3.3317485570907595, BLEU: 0.817731321148915\n",
      "\n",
      "Input1:> 我 在 印度 拜访 过 一个 村子 全家 全家人 家人 都 是 丝绸 贸易 的 奴隶\n",
      "\n",
      "Target1:= I visited villages in India where entire families were enslaved in the silk trade  . \n",
      "Predict1:< I I a a in in in in , , , , in in  .\n",
      "\n",
      "Input2:> 但 我们 还是 会 得到 大量 来自 不同 环境 下 的 采样 数据 而且 重点 是 不同 的 环境 因为 这样 我们 才能 消除 除掉 那些 混淆 的 因素 寻求 这种 疾病 真正 的 标记\n",
      "\n",
      "Target2:= But we ll get a very large sample of data that is collected from all different circumstances , and it is getting it in different circumstances that matter because then we are looking at ironing out the confounding \n",
      "Predict2:< But we we we to the the the , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 38s (- -93m 49s), Epoch: [7/10], Step: [200/3335], Train Loss: 3.3101846575737, BLEU: 0.7888049296467483\n",
      "\n",
      "Input1:> 这 很 怪\n",
      "\n",
      "Target1:= That was awkward  . \n",
      "Predict1:< It is is  .  .\n",
      "\n",
      "Input2:> 我们 能 进行 高频 高频率 频率 的 监测 以 获取 客观 数据\n",
      "\n",
      "Target2:= We can do high-frequency monitoring to get objective data  . \n",
      "Predict2:< We can the the the the the the the  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 44s (- -93m 41s), Epoch: [7/10], Step: [220/3335], Train Loss: 3.380391812324524, BLEU: 0.7163138230580766\n",
      "\n",
      "Input1:> 正常 忽视 了 差异 带来 的 美 人 与 人 的 差异 不 代表 其中 一方 是 错 的\n",
      "\n",
      "Target1:= UNK overlooks the beauty that differences give us , and the fact that we are different doesn t mean that one of us is wrong  . \n",
      "Predict1:< The people people people , , , , , , , , , ,\n",
      "\n",
      "Input2:> 这 是 我 UNK 法国 UNK 杂志 前 几天 前 在 睡衣 party 上 拍 的 照片\n",
      "\n",
      "Target2:= Here is me at a slumber party a few days before I shot French UNK  . \n",
      "Predict2:< This is a the I I , , , the the the the the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 49s (- -93m 33s), Epoch: [7/10], Step: [240/3335], Train Loss: 3.336719346046448, BLEU: 0.7240087243912579\n",
      "\n",
      "Input1:> 我 甚至 经历 了 自我 身份 的 质疑\n",
      "\n",
      "Target1:= I even went through an identity crisis  . \n",
      "Predict1:< I I a a  .  .  .\n",
      "\n",
      "Input2:> 他 只是 压力 太大 了 婚礼 的 筹备 和 我家 家庭 家庭成员 成员 的 到来 让 他 喘 不过 过气\n",
      "\n",
      "Target2:= He had just been really stressed out by the wedding and by becoming a family with me  . \n",
      "Predict2:< He he a a and and , and and and and and and and and and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 92m 55s (- -93m 26s), Epoch: [7/10], Step: [260/3335], Train Loss: 3.3886611461639404, BLEU: 0.8002276494515284\n",
      "\n",
      "Input1:> 而且 速度 也 快 最多 多只 只要 30 秒\n",
      "\n",
      "Target1:= They are high-speed , take about 30 seconds at most  . \n",
      "Predict1:< And it is a , , , ,  .  .\n",
      "\n",
      "Input2:> 还有 一次 当 我 在 洛杉矶 洛杉矶市 市中心 中心 的 一个 流浪 流浪汉 之家 家安 安置 了 一个 花园\n",
      "\n",
      "Target2:= There is another time when I put a garden in this homeless shelter in downtown Los Angeles  . \n",
      "Predict2:< And I I I I I a a a a a a a a  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 0s (- -93m 19s), Epoch: [7/10], Step: [280/3335], Train Loss: 3.4493130922317503, BLEU: 0.825745933038805\n",
      "\n",
      "Input1:> 我 的 手 滑 了 一下 立即 想到 数天 前 碰到 的 一名 矿工 他 一手 UNK 就 掉 下 了 无底 深井\n",
      "\n",
      "Target1:= When my hand slips , I suddenly remember a miner I had met days before who had lost his grip and fell countless feet down that shaft  . \n",
      "Predict1:< My I my , , , , , , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 我们 与 客户 在 餐馆 和 酒吧 见面\n",
      "\n",
      "Target2:= We meet at the cafe  . We meet at the pub  . \n",
      "Predict2:< We are the the and and and  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 6s (- -93m 12s), Epoch: [7/10], Step: [300/3335], Train Loss: 3.324395740032196, BLEU: 0.8192270568704606\n",
      "\n",
      "Input1:> 我们 的 做法 是 与 客户 一对 一对一 为了 能 一对 一对一 我们 必须 建设 一套 前所 前所未有 未有 的 社会 基础 基础设施 设施\n",
      "\n",
      "Target1:= What we do , we work one-on-one , and to work one-on-one , you have to create a social infrastructure that doesn t exist  . \n",
      "Predict1:< Our is our we , we , , , to to to to to to to to to to to\n",
      "\n",
      "Input2:> 他 经常 尖叫\n",
      "\n",
      "Target2:= He screamed a lot  . \n",
      "Predict2:< He he  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 11s (- -93m 5s), Epoch: [7/10], Step: [320/3335], Train Loss: 3.214868485927582, BLEU: 0.769665831950696\n",
      "\n",
      "Input1:> 没有 艺术 指导 没有 造型 UNK 没有 重拍 的 机会 更 别提 提灯 灯光 灯光效果 光效 效果\n",
      "\n",
      "Target1:= There were no art directors , no stylists , no chance for UNK , not even any regard for lighting  . \n",
      "Predict1:< There no no no no no no no no no no no no no no no no\n",
      "\n",
      "Input2:> 因为 我 当时 是 个 小孩 小孩子 孩子 我 可以 一整 一整天 整天 不 干 别的 在 自己 的 房间 里 鼓捣 一天 我 甚至 把 我 妈妈 新 买 的 收音 收音机 拆 得 七零八 七零八落 那天\n",
      "\n",
      "Target2:= Since I was a small boy , I used to work in my room for the whole day , and I even took apart my mom is new radio , and that day she almost killed me , \n",
      "Predict2:< Because I I I I I I I , I I , I , , I , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 17s (- -94m 58s), Epoch: [7/10], Step: [340/3335], Train Loss: 3.4582165718078612, BLEU: 0.7979357223733025\n",
      "\n",
      "Input1:> 我 看到 似乎 是 一家 一家人 家人 在 船上 捕鱼 两个 哥哥 几个 小点 的 孩子 挺 像 的 吧\n",
      "\n",
      "Target1:= I saw what seemed to be a family fishing on a boat , two older brothers , some younger kids , makes sense right  ? \n",
      "Predict1:< I I was a a a , , , , , , , , , , a  .  .\n",
      "\n",
      "Input2:> 我 让 这 五个 渔民 合伙 运作 不是 把 美味 的 金枪 金枪鱼 以 60 美分 一 公斤 的 低价 卖 给 奥尔巴 奥尔巴尼 尔巴 尼 美国 的 罐头 工厂 而是 以 15 美元 一 公斤 的 高价\n",
      "\n",
      "Target2:= And I helped these five fishermen to work together and get this beautiful tuna not to the cannery in Albany for 60 cents a kilo , but we found a way to take the fish for sushi to \n",
      "Predict2:< I I t to to to to the the , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 23s (- -94m 52s), Epoch: [7/10], Step: [360/3335], Train Loss: 3.397655260562897, BLEU: 0.7531695963938078\n",
      "\n",
      "Input1:> 错 他们 都 是 奴隶\n",
      "\n",
      "Target1:= Wrong  . They were all enslaved  . \n",
      "Predict1:< And they them  .  .  .\n",
      "\n",
      "Input2:> 当地 有 一位 女士 指导 我们 清洗\n",
      "\n",
      "Target2:= We had an amazing local woman who guided us  . \n",
      "Predict2:< And is a , , ,  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 28s (- -94m 46s), Epoch: [7/10], Step: [380/3335], Train Loss: 3.409615182876587, BLEU: 0.7959906698350764\n",
      "\n",
      "Input1:> 所以 他们 给 他 一个 他们 能够 想到 的 最低 低微 的 最 微不足道 不足 不足道 的 最为 可悲 的 头衔\n",
      "\n",
      "Target1:= So they wanted to give him the UNK , UNK , most pathetic title that they could think of  . \n",
      "Predict1:< So they they to to to to their their their their their their their their their their their their their  .\n",
      "\n",
      "Input2:> 在 西非 一些 人 毫无 怜悯 的 叫卖 死去 的 秃鹫 来 满足 巫术 和 收藏 的 需求\n",
      "\n",
      "Target2:= In West Africa , there is a horrific trade of dead vultures to serve the witchcraft and the fetish market  . \n",
      "Predict2:< In the , , , , , , and the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 34s (- -94m 39s), Epoch: [7/10], Step: [400/3335], Train Loss: 3.338765633106232, BLEU: 0.7650889065475557\n",
      "\n",
      "Input1:> UNK 我 的 下 一项 项发明 发明 是 我 想 做 一个 电力 栅栏 CA 电力 栅栏\n",
      "\n",
      "Target1:= My next invention is , I want to make an electric fence  . Electric fence  ? \n",
      "Predict1:< And I I I I I I I a a to a to to a a a  .\n",
      "\n",
      "Input2:> 所以 在 未来 当 飞 米 成像 相机 在 你 的 手机 里 的 时候 你 就 可以 去 超市 不用 摸 就 能 检查 检查一下 一下 下水 水果 是否 熟 了\n",
      "\n",
      "Target2:= So , in the future , when this UNK is in your camera phone , you might be able to go to a supermarket and check if the fruit is ripe without actually touching it  . \n",
      "Predict2:< So , the , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 39s (- -94m 33s), Epoch: [7/10], Step: [420/3335], Train Loss: 3.322028064727783, BLEU: 0.7994982868354806\n",
      "\n",
      "Input1:> 现在 超人 能 飞\n",
      "\n",
      "Target1:= So , Superman can fly  . \n",
      "Predict1:< Now , is is  .  .\n",
      "\n",
      "Input2:> 这 也 能 为 病人 节省 节省下 那段 去 诊所 的 路程 如果 病人 能 独立 完成 这个 测试 呢\n",
      "\n",
      "Target2:= Now , that would actually save on a difficult trip to the clinic , and what if patients could do that test themselves , right  ? \n",
      "Predict2:< And is is be to , , , , , , , , , to to to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 45s (- -94m 27s), Epoch: [7/10], Step: [440/3335], Train Loss: 3.3454020619392395, BLEU: 0.7600370379414535\n",
      "\n",
      "Input1:> 他 很 愿意 了解 我 的 家庭 我 的 童年 我 的 愿望 和 梦想\n",
      "\n",
      "Target1:= He wanted to know everything about my family and my childhood and my hopes and dreams  . \n",
      "Predict1:< He was me me me and and and my and me me me me  .\n",
      "\n",
      "Input2:> 但 两个 词 的 意思 不同\n",
      "\n",
      "Target2:= But they mean two different things  . \n",
      "Predict2:< But what is a different  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 50s (- -94m 21s), Epoch: [7/10], Step: [460/3335], Train Loss: 3.349942719936371, BLEU: 0.821175514846208\n",
      "\n",
      "Input1:> 直至 现在 我 才 意识 到 丢失 了 这些 照片 肯定 是 这些 人 失去 的 一块 重大 大部 大部分 部分\n",
      "\n",
      "Target1:= Now , it wasn t until this point that I realized that these photos were such a huge part of the personal loss these people had felt  . \n",
      "Predict1:< And , I I these these these these , the the the the the the the  .  .\n",
      "\n",
      "Input2:> 警报 拉 响 的 时候 他 不得 不得不 冲向 向海 海啸 最前 前线\n",
      "\n",
      "Target2:= He had to go towards the water as the sirens sounded  . \n",
      "Predict2:< He was , , , , , , , , , ,  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 93m 56s (- -94m 15s), Epoch: [7/10], Step: [480/3335], Train Loss: 3.4012877106666566, BLEU: 0.8497274927542124\n",
      "\n",
      "Input1:> UNK 知道 什么 是 爱\n",
      "\n",
      "Target1:= Remi knows what love is  . \n",
      "Predict1:< What , know , is know\n",
      "\n",
      "Input2:> 我 不能 披露 太多 我 离开 朝鲜 时 的 细节 但是 我 只能 说 那 是 在 饥荒 中 最 艰难 的 几年 我 被 送到 中国 和 远亲 一起 住\n",
      "\n",
      "Target2:= I can t reveal many details &#91; about &#93; how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives \n",
      "Predict2:< I I I I I the I I , I , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 94m 1s (- -94m 9s), Epoch: [7/10], Step: [500/3335], Train Loss: 3.379266107082367, BLEU: 0.8005072867208092\n",
      "\n",
      "Input1:> 比如 艾哈迈 艾哈迈德 哈迈德 迈德 这 不是 真名 我 也 不能 让 你们 看见 他 的 长相 他 是 我 一个 学生 的 父亲\n",
      "\n",
      "Target1:= Like Ahmed  . That is not his real name , and I cannot show you his face , but Ahmed is the father of one of my students  . \n",
      "Predict1:< For , , , , , , , , , , , , , , , , , , , , , , ,  .\n",
      "\n",
      "Input2:> 在 这个 物欲 UNK 横流 的 时代 里 努力 坚持 自我 铭记 人生 的 短暂 与 生命 的 脆弱 变得 比 以往 任何 何时 时代 都 更 重要\n",
      "\n",
      "Target2:= In our age of increasing distractions , it is more important than ever to find ways to maintain perspective and remember that life is brief and tender  . \n",
      "Predict2:< In the , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 94m 7s (- -94m 3s), Epoch: [7/10], Step: [520/3335], Train Loss: 3.30777622461319, BLEU: 0.8159498173566931\n",
      "\n",
      "Input1:> 这 让 我们 相信 自己 并 不 孤单\n",
      "\n",
      "Target1:= It is about knowing you are not alone  . \n",
      "Predict1:< It is not us us to  .  .  .\n",
      "\n",
      "Input2:> 它 随处 随处可见 可见\n",
      "\n",
      "Target2:= It is everywhere  . \n",
      "Predict2:< It is it  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 94m 12s (- -95m 57s), Epoch: [7/10], Step: [540/3335], Train Loss: 3.275642716884613, BLEU: 0.7906745039700658\n",
      "\n",
      "Input1:> 鼓掌 基本 基本上 如果 你们 想见 我 如果 想要 碰面 如果 想 坐在 舒适 的 椅子 上 讲 一些 有 的 没 的 就 别 给 我 打电话 电话\n",
      "\n",
      "Target1:= So basically , if you want to meet with me , you know , if you want to meet , don t call me if you want to sit around in cushy chairs and have meetings where you \n",
      "Predict1:< And , I I I I if I if I if I if I , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我 曾经 误以为 以为 只有 我 一个 个人 有 这样 的 遭遇\n",
      "\n",
      "Target2:= I was mistaken in thinking that I was unique and alone in this situation  . \n",
      "Predict2:< I I a a a , , , I  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 94m 18s (- -95m 51s), Epoch: [7/10], Step: [560/3335], Train Loss: 3.3543291091918945, BLEU: 0.8072013383711368\n",
      "\n",
      "Input1:> 那些 枪 对 我 来说 是 一个 明显 的 信号 尽管 他 并 没有 拿起 枪 指着 我 我 已经 无时 无时不刻 不 处在 危险 的 边缘\n",
      "\n",
      "Target1:= But those guns were really a message for me , and even though he hadn t raised a hand to me , my life was already in grave danger every minute of every day  . \n",
      "Predict1:< And is is , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "Input2:> 我们 能 减少 和 病人 有关 的 后勤 问题\n",
      "\n",
      "Target2:= We can reduce logistical difficulties with patients  . \n",
      "Predict2:< We can we the the the  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 94m 23s (- -95m 45s), Epoch: [7/10], Step: [580/3335], Train Loss: 3.263369154930115, BLEU: 0.8118237030883791\n",
      "\n",
      "Input1:> 由此 开始 了 我 深入 现代 奴隶 奴隶制 奴隶制度 制度 的 旅程\n",
      "\n",
      "Target1:= Thus began my journey into modern day slavery  . \n",
      "Predict1:< And then I , I I , , the  .  .  .\n",
      "\n",
      "Input2:> 所以 现在 的 沥青 有 一些 些小 钢丝 丝绒 在 里面\n",
      "\n",
      "Target2:= So then you have asphalt with very small pieces of steel wool in it  . \n",
      "Predict2:< So now are are are are the  .  .  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "learning_rate=0.0001\n",
    "num_epoch = 10\n",
    "print_every = 20\n",
    "plot_every = 20\n",
    "\n",
    "encoder1 = EncoderRNN(source_tra.n_words,hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, target_tra.n_words).to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder1.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    plot_bleu_score_val = []\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "#         print(i)\n",
    "        encoder1.train()\n",
    "        decoder1.train()\n",
    "        input_tensor = input_sentences.transpose(0,1)   \n",
    "        target_tensor = target_sentences.transpose(0,1)\n",
    "        loss = train(input_tensor, target_tensor, encoder1,\n",
    "                     decoder1, encoder_optimizer, decoder_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i > 0 and i % print_every == 0:\n",
    "            inputs, corpus, truths = evaluate(encoder1, decoder1, val_loader, max_length=MAX_SENT_LEN)\n",
    "            bleu_score_val_avg = bleu_new(corpus, truths)#np.mean(bleu_score_val)\n",
    "\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, BLEU: {}'.format(\n",
    "                timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                len(train_loader),print_loss_avg,bleu_score_val_avg))\n",
    "            \n",
    "            print('\\nInput1:> %s'%(' '.join([source_tra.index2word[i.item()] for i in inputs[0][3] if i.item() not in set([PAD_IDX,EOS_token,SOS_token])])))\n",
    "            print('\\nTarget1:= %s'%(convert_idx_2_sent_new(truths[0][3], target_tra)),\n",
    "                    '\\nPredict1:< %s' %(convert_idx_2_sent_new(corpus[0][3], target_tra)))\n",
    "            \n",
    "            print('\\nInput2:> %s'%(' '.join([source_tra.index2word[i.item()] for i in inputs[1][3] if i.item() not in set([PAD_IDX,EOS_token,SOS_token])])))\n",
    "            print('\\nTarget2:= %s'%(convert_idx_2_sent_new(truths[1][3], target_tra)),\n",
    "                    '\\nPredict2:< %s' %(convert_idx_2_sent_new(corpus[1][3], target_tra)))\n",
    "\n",
    "        if i > 0 and i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    print(plot_losses)\n",
    "\n",
    "        #if i % print_every == 0:\n",
    "            #corpus, truths = evaluate_rnn(encoder1, attn_decoder1, val_loader, max_length=MAX_SENT_LEN)\n",
    "            #score_ls = bleu(corpus, truths)\n",
    "            #avg_score = np.array(score_ls).mean()\n",
    "            #print_loss_avg = print_loss_total / print_every\n",
    "            #print_loss_total = 0\n",
    "            #print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Average BLEU: {}'.format(\n",
    "                    #timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                    #len(train_loader),print_loss_avg, avg_score))\n",
    "\n",
    "        #if i > 0 and i % plot_every == 0:\n",
    "            #plot_loss_avg = plot_loss_total / plot_every\n",
    "            #plot_losses.append(plot_loss_avg)\n",
    "            #plot_loss_total = 0\n",
    "\n",
    "                \n",
    "    #print(plot_losses)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
