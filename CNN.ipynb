{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Beam search  \n",
    "- self-attention  \n",
    "- EOS mask loss\n",
    "//////\n",
    "- CNN - done\n",
    "- plot - done\n",
    "- BLEU - done\n",
    "-  Evaluation - done\n",
    "- early stopping - done\n",
    "-  GPU - done\n",
    "- Batch encoder - done\n",
    "- pretrained wordvec/dataloader- done\n",
    "- sort in collate_func - done\n",
    "- save best model - done\n",
    "\n",
    "\n",
    "\n",
    "To run this notebook for actual analysis:  \n",
    "remove breaker in train_model  \n",
    "increase hidden size  \n",
    "increase eva_every  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import pickle as pkl\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import sacrebleu\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_IDX = 2\n",
    "UNK_IDX = 3\n",
    "VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_loader(file_name):\n",
    "    with open(file_name+'.p', 'rb') as f:\n",
    "        objct = pkl.load(f)\n",
    "    return(objct)\n",
    "\n",
    "def pkl_dumper(obj, file_name):\n",
    "    with open(file_name+'.p', 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_wordvec(lan):\n",
    "    if lan == 'zh':\n",
    "        filename = 'wiki.zh.vec'\n",
    "    elif lan == 'en':\n",
    "        filename = 'wiki-news-300d-1M.vec'\n",
    "    else:\n",
    "        filename = 'wiki-news-300d-1M.vec' #######\n",
    "    with open(os.path.join(data_dir, filename)) as f:\n",
    "        word_vecs = np.zeros((VOCAB_SIZE+4, 300))\n",
    "        word_vecs[UNK_IDX] = np.random.normal(scale=0.6, size=(300, ))\n",
    "        word_vecs[SOS_token] = np.random.normal(scale=0.6, size=(300, ))\n",
    "        word_vecs[EOS_token] = np.random.normal(scale=0.6, size=(300, ))\n",
    "\n",
    "        words_ft = {'<pad>': PAD_IDX,\n",
    "                   '<unk>': UNK_IDX, \n",
    "                   '<sos>': SOS_token,\n",
    "                   '<eos>': EOS_token}\n",
    "        idx2words_ft = {PAD_IDX:'<pad>', UNK_IDX: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "        ordered_words_ft = ['<sos>', '<eos>', '<pad>', '<unk>']\n",
    "        count = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if len(idx2words_ft) >= VOCAB_SIZE: \n",
    "                break\n",
    "            s = line.split()\n",
    "            if (np.asarray(s[1:]).size != 300):\n",
    "                print(lan, i, np.asarray(s[1:]).size)\n",
    "                continue\n",
    "            word_vecs[count+4, :] = np.asarray(s[1:])\n",
    "            words_ft[s[0]] = count+4\n",
    "            idx2words_ft[count+4] = s[0]\n",
    "            ordered_words_ft.append(s[0])\n",
    "            count += 1\n",
    "    word_vecs = torch.FloatTensor(word_vecs)\n",
    "    pkl_dumper(word_vecs, os.path.join(data_dir, lan + '_word_vecs'))\n",
    "    pkl_dumper(words_ft, os.path.join(data_dir, lan + '_words_ft'))\n",
    "    pkl_dumper(idx2words_ft, os.path.join(data_dir, lan + '_idx2words_ft'))\n",
    "    pkl_dumper(ordered_words_ft, os.path.join(data_dir, lan + '_ordered_words_ft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_pretrained_wordvec('en')\n",
    "# load_pretrained_wordvec('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = {}\n",
    "word2index = {}\n",
    "index2word = {}\n",
    "word_vecs['en'] = pkl_loader('data/en_word_vecs')\n",
    "word_vecs['zh'] = pkl_loader('data/zh_word_vecs')\n",
    "word2index['en'] = pkl_loader('data/en_words_ft')\n",
    "word2index['zh'] = pkl_loader('data/zh_words_ft')\n",
    "index2word['en'] = pkl_loader('data/en_idx2words_ft')\n",
    "index2word['zh'] = pkl_loader('data/zh_idx2words_ft')\n",
    "# en_ordered_words_ft = pkl_loader('data/en_ordered_words_ft')\n",
    "# zh_ordered_words_ft = pkl_loader('data/zh_ordered_words_ft')\n",
    "VOCAB_SIZE = len(word2index['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index['en']), len(word2index['zh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_words_ft['apple'], en_idx2words_ft[10398]\n",
    "# zh_words_ft['知识'], zh_idx2words_ft[zh_words_ft['知识']]\n",
    "# en_idx2words_ft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name, index2word, word2index):\n",
    "        self.name = name\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        self.n_words = len(index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = re.sub(r\"&apos;m\", r\"am\", s)\n",
    "    s = re.sub(r\"&apos;t\", r\"not\", s)\n",
    "    s = re.sub(r\"&apos;s\", r\"is\", s)\n",
    "    s = re.sub(r\"&apos;re\", r\"are\", s)\n",
    "    s = re.sub(r\"&quot;\", r\"\", s)\n",
    "    s = re.sub(r\"&apos;ve\", r\"have\", s)\n",
    "    s = re.sub(r\"&apos;d\", r\"had\", s)\n",
    "    s = re.sub(r\"&apos;\", r\"\", s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.5)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingLangs(sourcelang, targetlang, setname):\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    print('Reading lines...')\n",
    "    # Read the file \n",
    "    with open('data/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,sourcelang)) as f:\n",
    "        for line in f.readlines():\n",
    "            input_ls.append([normalizeString(word) for word in line.split()])\n",
    "    with open('data/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,targetlang)) as f:\n",
    "        for line in f.readlines():\n",
    "            output_ls.append([normalizeString(word) for word in line.split()])\n",
    "    pairs = list(zip(input_ls, output_ls))\n",
    "    pairs = [pair for pair in pairs if (len(pair[0])+len(pair[1]))>0]\n",
    "    print('Read %s sentence pairs'%(len(input_ls)))\n",
    "    if sourcelang == 'zh':\n",
    "        input_lang = Lang(sourcelang, index2word['zh'], word2index['zh'])\n",
    "    else:\n",
    "        input_lang = Lang(sourcelang, index2word['zh'], word2index['zh']) ####\n",
    "    output_lang = Lang(targetlang, index2word['en'], word2index['en'])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213377 sentence pairs\n",
      "Counted words:\n",
      "zh 50000\n",
      "en 50000\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Counted words:\n",
      "zh 50000\n",
      "en 50000\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Counted words:\n",
      "zh 50000\n",
      "en 50000\n"
     ]
    }
   ],
   "source": [
    "source_tra, target_tra, pairs_tra = loadingLangs('zh', 'en', 'train')\n",
    "source_val, target_val, pairs_val = loadingLangs('zh', 'en', 'dev')\n",
    "source_tes, target_tes, pairs_tes = loadingLangs('zh', 'en', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% of chinese sentences length = 35.0\n",
      "90% of english sentences length = 39.0\n",
      "(['这', '是', '真的', '吗'], ['Can', 'that', 'be', 'right', ' ?'])\n"
     ]
    }
   ],
   "source": [
    "print(\"90% of chinese sentences length = {0}\".format(np.percentile([len(x[0]) for x in pairs_tra], 90)))\n",
    "print(\"90% of english sentences length = {0}\".format(np.percentile([len(x[1]) for x in pairs_tra], 90)))\n",
    "print(random.choice(pairs_tra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 40\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair,source,target):\n",
    "    input_lang = source\n",
    "    output_lang = target\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0]).reshape((-1))\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1]).reshape((-1))\n",
    "    return (input_tensor, input_tensor.shape[0], target_tensor, target_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, source, target, pairs):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        inp_ten, inp_len, tar_ten, tar_len = tensorsFromPair(self.pairs[key], self.source, self.target)\n",
    "        item = {}\n",
    "        item['inputtensor'] = inp_ten[:MAX_SENT_LEN]\n",
    "        item['inputlen'] = min(inp_len, MAX_SENT_LEN)\n",
    "        item['targettensor'] = tar_ten[:MAX_SENT_LEN]\n",
    "        item['targetlen'] = min(tar_len, MAX_SENT_LEN)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NMTDataset(source_tra, target_tra, pairs_tra)\n",
    "val_data = NMTDataset(source_tra, target_tra, pairs_val)\n",
    "test_data = NMTDataset(source_tra, target_tra, pairs_tes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate function\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_data, tar_data, src_len, tar_len = [], [], [], []\n",
    "    for datum in batch:        \n",
    "        src_datum = np.pad(np.array(datum['inputtensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['inputlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        tar_datum = np.pad(np.array(datum['targettensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['targetlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        src_data.append(src_datum)\n",
    "        tar_data.append(tar_datum)\n",
    "        src_len.append(datum['inputlen'])\n",
    "        tar_len.append(datum['targetlen'])\n",
    "        \n",
    "    ind_dec_order = np.argsort(src_len)[::-1]\n",
    "    src_data = np.array(src_data)[ind_dec_order]\n",
    "    src_len = np.array(src_len)[ind_dec_order]\n",
    "    tar_data = np.array(tar_data)[ind_dec_order]\n",
    "    tar_len = np.array(tar_len)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(src_data)).to(device),torch.from_numpy(np.array(tar_data)).to(device),\n",
    "                torch.LongTensor(np.array(src_len)).to(device),torch.LongTensor(np.array(tar_len)).to(device)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_func)\n",
    "val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence batch: \n",
      "tensor([[ 3222,  1346,     6,  ..., 37090,  1412,  2562],\n",
      "        [ 6349,  1346,  1788,  ...,     2,     2,     2],\n",
      "        [ 2293,    50,  7292,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [18456, 27489, 27489,  ...,     2,     2,     2],\n",
      "        [  827,     3, 10986,  ...,     2,     2,     2],\n",
      "        [29401,    33,     1,  ...,     2,     2,     2]])\n",
      "input batch dimension: torch.Size([32, 40])\n",
      "target sentence batch: \n",
      "tensor([[ 9954,    95,    63,  ...,   110,  1553,  7466],\n",
      "        [   87,   410,     4,  ...,     1,     2,     2],\n",
      "        [   32, 11107,   236,  ...,  8242,     3,     1],\n",
      "        ...,\n",
      "        [23218,    13,     3,  ...,     2,     2,     2],\n",
      "        [  518,    34,   220,  ...,     2,     2,     2],\n",
      "        [26459,     3,     1,  ...,     2,     2,     2]])\n",
      "target batch dimension: torch.Size([32, 40])\n",
      "input sentence len: \n",
      "tensor([40, 37, 35, 32, 31, 30, 26, 26, 25, 25, 25, 24, 21, 21, 21, 21, 19, 19,\n",
      "        17, 16, 13, 13, 10, 10, 10,  9,  9,  8,  7,  6,  5,  3])\n",
      "target sentence len: \n",
      "tensor([40, 38, 40, 40, 33, 32, 22, 40, 23, 23, 31, 30, 20, 29, 28, 14, 21, 15,\n",
      "        17, 15, 16, 17, 13, 11, 14, 17, 11,  7,  8, 11,  6,  3])\n"
     ]
    }
   ],
   "source": [
    "# sample data loader\n",
    "count = 0\n",
    "for data in train_loader:\n",
    "    count+=1\n",
    "    print('input sentence batch: ')\n",
    "    print(data[0])\n",
    "    print('input batch dimension: {}'.format(data[0].size()))\n",
    "    print('target sentence batch: ')\n",
    "    print(data[1])\n",
    "    print('target batch dimension: {}'.format(data[1].size()))\n",
    "    print('input sentence len: ')\n",
    "    print(data[2])\n",
    "    print('target sentence len: ')\n",
    "    print(data[3])\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    'HIDDEN_SIZE': 16,\n",
    "    'LR': 0.01,\n",
    "    'EVA_EVERY': 2,\n",
    "    'DROP_OUT': 0.3,\n",
    "    'TEACHER_RATIO': 0.5,\n",
    "    'N_LAYERS': 2,\n",
    "    'KER_SIZE': 3,\n",
    "    'NUM_EPOCHS': 2   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers = 1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True) \n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device) \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"input size is: \", input.size())\n",
    "        batch_size = input.size()[1]\n",
    "        seq_len = input.size()[0]\n",
    "        embedded = self.embedding(input).view(seq_len, batch_size, -1)\n",
    "#         print(seq_len, batch_size)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "#         print(\"output size is: \", output.size())\n",
    "        output = self.fc1(output)\n",
    "#         print(\"output size is: \", output.size())\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, word_vecs=None, ker_size=3, dropout_p=0.5):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if len(word_vecs) > 0:\n",
    "            self.embedding = nn.Embedding(input_size, 300, padding_idx=PAD_IDX)\n",
    "            self.embedding.weight = nn.Parameter(word_vecs)\n",
    "            self.embedding.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD_IDX)\n",
    "          \n",
    "          \n",
    "        self.seq = nn.Sequential(nn.Conv1d(300, hidden_size, kernel_size=ker_size, padding=(ker_size-1)//2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool1d(kernel_size=ker_size, stride=1, padding=(ker_size-1)//2))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.transpose(0,1)\n",
    "        batch_size, seq_len = input.size()\n",
    "        # input size for conv1d is , N is a batch size, C denotes a number of channels, L is a length of signal sequence.\n",
    "        output = self.embedding(input).view(batch_size, -1, seq_len)\n",
    "        output = self.seq(output)\n",
    "        output = output.view(seq_len, batch_size, -1) \n",
    "        output = self.fc(output)\n",
    "        output = F.relu(self.dropout(output))\n",
    "        output = self.fc(output)\n",
    "        hidden = torch.nn.functional.max_pool1d(output.view(batch_size, -1, seq_len), seq_len).permute(2, 0, 1)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_SENT_LEN, n_layers=2):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.view(1,-1)\n",
    "        seq_len, batch_size = input.size()\n",
    "        output= self.embedding(input).view(1, batch_size, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_SENT_LEN):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.view(1,-1)\n",
    "        batch_size = input.size()[1]\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, batch_size, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  \n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied.transpose(0,1)[0]), 1)\n",
    "        \n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, mode_dec=None, mode_enc=None):\n",
    "    \n",
    "    hidden_size = hyper['HIDDEN_SIZE']\n",
    "    learning_rate = hyper['LR']\n",
    "    dropout_p = hyper['DROP_OUT']\n",
    "    teacher_forcing_ratio = hyper['TEACHER_RATIO']\n",
    "    n_layers = hyper['N_LAYERS']\n",
    "    ker_size = hyper['KER_SIZE']\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size()[0] \n",
    "    target_length = target_tensor.size()[0]\n",
    "    batch_size = input_tensor.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_outputs = torch.zeros(target_length, batch_size, encoder.hidden_size, device=device) \n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # feed-forward layer resulting encoder outputs, ei refers to each word token in input sentence\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    if mode_enc == 'cnn':\n",
    "        encoder_hidden = nn.Linear(hidden_size,hidden_size)(encoder_hidden[0].cpu()).to(device).unsqueeze(0)\n",
    "    else:\n",
    "        encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "            torch.cat((encoder_hidden[0].cpu(),encoder_hidden[1].cpu()),dim = 1)).to(device).unsqueeze(0)\n",
    "   # print('encoder_hidden size:', encoder_hidden.size())\n",
    "#     print(\"t:\", encoder_hidden.dtype)\n",
    "    decoder_hidden = encoder_hidden.to(device)\n",
    "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)  \n",
    "    #print('input to decoder:', decoder_input.size(), decoder_hidden.size(), encoder_outputs.size())\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            if mode_dec == 'attn':\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                \n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = target_tensor[di]  \n",
    "            loss += criterion(decoder_output, target_tensor[di])    \n",
    "    else:\n",
    "  \n",
    "        for di in range(target_length):\n",
    "            if mode_dec == 'attn':\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.transpose(0,1).detach()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=5)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    " \n",
    "            #decoder_input = topi.squeeze().detach()\n",
    "    \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step() \n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_2_sent(pred_tensor, truth_tensor,lang_obj):\n",
    "    pred_word_list = []\n",
    "    truth_word_list = []\n",
    "    for i in pred_tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            pred_word_list.append(lang_obj.index2word[i.item()])\n",
    "    for j in truth_tensor:\n",
    "        if j.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            truth_word_list.append(lang_obj.index2word[j.item()])\n",
    "    pred_sent = (' ').join(pred_word_list)\n",
    "    truth_sent = (' ').join(truth_word_list)\n",
    "    return pred_sent, truth_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(corpus, truths):\n",
    "    '''\n",
    "    corpus: list, NBs * BATCHSIZE * MAX_LEN\n",
    "    truths: list, NBs * BATCHSIZE * MAX_LEN\n",
    "    \n",
    "    return: array of length NBs, avg blue score for each batch\n",
    "    '''\n",
    "    n = len(corpus)\n",
    "    bleus = [0]*n\n",
    "    for i in range(n):\n",
    "        pred, true = corpus[i], truths[i]\n",
    "        sumbleu = 0.0\n",
    "        for j in range(len(corpus[i])):\n",
    "            pred_tensor, true_tensor = pred[j], true[j]\n",
    "            pred_sent, true_sent = convert_idx_2_sent(pred_tensor, true_tensor, target_tra)\n",
    "            sumbleu += corpus_bleu(true_sent, pred_sent).score\n",
    "        avgbleu = sumbleu / len(corpus[i])\n",
    "        bleus[i] = avgbleu\n",
    "    return bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, data_loader, mode_enc, mode_dec, max_length=MAX_SENT_LEN):\n",
    "    start = time.time()\n",
    "    hidden_size = hyper['HIDDEN_SIZE']\n",
    "    learning_rate = hyper['LR']\n",
    "    dropout_p = hyper['DROP_OUT']\n",
    "    teacher_forcing_ratio = hyper['TEACHER_RATIO']\n",
    "    n_layers = hyper['N_LAYERS']\n",
    "    ker_size = hyper['KER_SIZE']\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    corpus = []\n",
    "    truths = []\n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(data_loader):\n",
    "#         if i % 5 == 0:\n",
    "#             print('Time: {}, Step: [{}/{}]'.format(\n",
    "#                 timeSince(start, i + 1/len(train_loader)), i, len(data_loader)))\n",
    "        input_tensor = input_sentences.transpose(0,1).to(device)\n",
    "        target_tensor = target_sentences.transpose(0,1).to(device) \n",
    "        truths.append(target_tensor)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        batch_size = input_tensor.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size, device=device)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        if mode_enc == 'cnn':\n",
    "            encoder_hidden = nn.Linear(hidden_size,hidden_size)(encoder_hidden[0].cpu()).to(device).unsqueeze(0)\n",
    "        else:\n",
    "            encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "            torch.cat((encoder_hidden[0].cpu(),encoder_hidden[1].cpu()),dim = 1)).to(device).unsqueeze(0)\n",
    "\n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "        decoder_input = torch.tensor([[SOS_token]*batch_size], device=device) \n",
    "        decoded_words = torch.zeros(batch_size, max_length)\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "        for di in range(max_length):\n",
    "            if mode_dec == 'attn':\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoded_words[:,di] = topi.squeeze()\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        corpus.append(decoded_words)\n",
    "    return corpus, truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_bleu(bleu_score,\n",
    "                losses):\n",
    "    \n",
    "    batches = np.arange(0, len(bleu_score))\n",
    "    f, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1 = axs[0]\n",
    "    ax1.plot(batches, losses, label='Validation loss')\n",
    "    ax1.set_xlabel(\"number of batches\")\n",
    "#     ax1.plot(batches, validation_loss_history, alpha=0.7, label='Validation Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    ax2 = axs[1]\n",
    "    ax2.plot(batches, bleu_score, label='Validation BLEU Score')\n",
    "    ax2.set_xlabel(\"number of batches\")\n",
    "#     ax2.plot(batches, validation_acc_history, alpha=0.7, label='Validation Accuracy')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mode_enc, mode_dec, hyper, start_epoch=0):\n",
    "    start = time.time()\n",
    "    hidden_size = hyper['HIDDEN_SIZE']\n",
    "    learning_rate = hyper['LR']\n",
    "    eva_every = hyper['EVA_EVERY']\n",
    "    dropout_p = hyper['DROP_OUT']\n",
    "    teacher_forcing_ratio = hyper['TEACHER_RATIO']\n",
    "    n_layers = hyper['N_LAYERS']\n",
    "    ker_size = hyper['KER_SIZE']\n",
    "    num_epoch = hyper['NUM_EPOCHS']\n",
    "    early_stopping = False\n",
    "    patience = 3\n",
    "    required_progress = 0.01\n",
    "    \n",
    "    if mode_enc == 'rnn':\n",
    "        encoder = EncoderRNN(source_tra.n_words, hidden_size).to(device)\n",
    "        \n",
    "    else:\n",
    "        encoder = EncoderCNN(source_tra.n_words, hidden_size, word_vecs=word_vecs[source_tra.name], dropout_p=dropout_p, ker_size=ker_size).to(device)\n",
    "       \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if mode_dec == 'attn':\n",
    "            decoder = AttnDecoder(hidden_size, target_tra.n_words, dropout_p=dropout_p).to(device)\n",
    "    else:\n",
    "            decoder = Decoder(hidden_size, target_tra.n_words, dropout_p=dropout_p).to(device)\n",
    "            \n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() \n",
    "    plot_bleu_score_val = []\n",
    "    plot_losses = []\n",
    "    loss_total = 0 \n",
    "    best_score = None\n",
    "    count = 0\n",
    "    filename = 'best' #########\n",
    "    for epoch in range(1, num_epoch + 1): \n",
    "        for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "            ### delete break\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            input_tensor = input_sentences.transpose(0,1).to(device)    \n",
    "            target_tensor = target_sentences.transpose(0,1).to(device)\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion, mode_dec=mode_dec, mode_enc=mode_enc)\n",
    "            loss_total += loss\n",
    "            if i > 0 and i % eva_every == 0:\n",
    "                    corpus, truths = evaluate(encoder, decoder, val_loader, max_length=MAX_SENT_LEN, mode_enc=mode_enc, mode_dec=mode_dec)\n",
    "                    bleu_score_val = bleu(corpus, truths)\n",
    "                    bleu_score_val_avg = np.mean(bleu_score_val)\n",
    "                    loss_avg = loss_total / eva_every\n",
    "                    loss_total = 0\n",
    "                    plot_losses.append(loss_avg)\n",
    "                    plot_bleu_score_val.append(bleu_score_val_avg)\n",
    "                    if best_score is None:\n",
    "                        best_score = bleu_score_val_avg\n",
    "                    if bleu_score_val_avg < best_score + required_progress:\n",
    "                        count += 1\n",
    "                    elif bleu_score_val_avg > best_score:\n",
    "                        state = {'epoch': start_epoch + epoch + 1, \n",
    "                                 'state_dict_enc': encoder.state_dict(),\n",
    "                                 'state_dict_dec': decoder.state_dict(), \n",
    "                                 'best_accuracy': best_score, \n",
    "                                 'optimizer_enc': encoder_optimizer.state_dict(),\n",
    "                                'optimizer_dec': decoder_optimizer.state_dict()}\n",
    "                        print ('new best achieved')\n",
    "                        torch.save(state, filename+'.pth.tar')\n",
    "                        best_score = bleu_score_val_avg\n",
    "                        count = 0\n",
    "                    if early_stopping:\n",
    "                        if count >= patience:\n",
    "                            print(\"earily stop triggered\")\n",
    "                            break\n",
    "                    print('Time: {0}, Epoch: [{1}/{2}], Step: [{3}/{4}], Train Loss: {5}, BLEU score: {6}'.format(\n",
    "                        timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                        len(train_loader), loss_avg, bleu_score_val_avg))\n",
    "        \n",
    "        if early_stopping:\n",
    "            if count >= patience:\n",
    "                break\n",
    "    plot_loss_bleu(plot_bleu_score_val, plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN+Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model('rnn', 'attn', hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rnn+noattn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model('rnn', 'noattn', hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn+attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 11s (- -2m 54s), Epoch: [1/2], Step: [2/6664], Train Loss: 16.008903121948244, BLEU score: 0.4291337220502725\n",
      "new best achieved\n",
      "Time: 3m 52s (- -3m 5s), Epoch: [1/2], Step: [4/6664], Train Loss: 10.05141716003418, BLEU score: 1.3063846802801398\n",
      "new best achieved\n",
      "Time: 5m 12s (- -5m 39s), Epoch: [1/2], Step: [6/6664], Train Loss: 9.34718589782715, BLEU score: 1.3421112879420785\n",
      "new best achieved\n",
      "Time: 6m 33s (- -6m 15s), Epoch: [1/2], Step: [8/6664], Train Loss: 8.96654853820801, BLEU score: 1.361862116688696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-de156ab5022f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-31e15b08f40d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(mode_enc, mode_dec, hyper, start_epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meva_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SENT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_dec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mbleu_score_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mbleu_score_val_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_score_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-996c04ba4823>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, data_loader, mode_enc, mode_dec, max_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 decoder_output, decoder_hidden = decoder(\n\u001b[1;32m     42\u001b[0m                     decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mdecoded_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model('cnn', 'attn', hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn+noattn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('cnn', 'noattn', hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
